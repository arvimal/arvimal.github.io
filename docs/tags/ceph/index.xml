<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ceph - Tag - The Child is Father of the Man</title>
        <link>https://arvimal.github.io/tags/ceph/</link>
        <description>ceph - Tag - The Child is Father of the Man</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 30 Jun 2016 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://arvimal.github.io/tags/ceph/" rel="self" type="application/rss+xml" /><item>
    <title>Sharding the Ceph RADOS Gateway bucket index</title>
    <link>https://arvimal.github.io/posts/2016/06/2016-06-30-sharding-the-ceph-rados-gateway-bucket-index/</link>
    <pubDate>Thu, 30 Jun 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/06/2016-06-30-sharding-the-ceph-rados-gateway-bucket-index/</guid>
    <description><![CDATA[_S_harding is the process of breaking down data onto multiple locations so as to increase parallelism, as well as distribute load. This is a common feature used in databases. Read more on this at Wikipedia.
The concept of sharding is used in Ceph, for splitting the bucket index in a RADOS Gateway.
RGW or RADOS Gateway keeps an index for all the objects in its buckets for faster and easier lookup.]]></description>
</item><item>
    <title>Ceph OSD heartbeats</title>
    <link>https://arvimal.github.io/posts/2016/05/2016-05-09-ceph-osd-heartbeats/</link>
    <pubDate>Mon, 09 May 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/05/2016-05-09-ceph-osd-heartbeats/</guid>
    <description><![CDATA[Ceph OSD daemons need to ensure that the neighbouring OSDs are functioning properly so that the cluster remains in a healthy state.
For this, each Ceph OSD process (ceph-osd) sends a heartbeat signal to the neighbouring OSDs. By default, the heartbeat signal is sent every 6 seconds [1], which is configurable of course.
If the heartbeat check from one OSD doesn&rsquo;t hear from the other within the set value for `osd_heartbeat_grace` [2], which is set to 20 seconds by default, the OSD that sends the heartbeat check reports the other OSD (the one that didn&rsquo;t respond within 20 seconds) as down, to the MONs.]]></description>
</item><item>
    <title>Ceph Rados Block Device (RBD) and TRIM</title>
    <link>https://arvimal.github.io/posts/2015/10/2015-10-07-objects-remain-in-a-ceph-pool-used-for-rbd-even-if-the-files-are-deleted-from-the-mount-point/</link>
    <pubDate>Wed, 07 Oct 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/10/2015-10-07-objects-remain-in-a-ceph-pool-used-for-rbd-even-if-the-files-are-deleted-from-the-mount-point/</guid>
    <description><![CDATA[I recently came across a scenario where the objects in a RADOS pool used for an RBD block device doesn’t get removed, even if the files created through the mount point were removed.
I had an RBD image from an RHCS1.3 cluster mapped to a RHEL7.1 client machine, with an XFS filesystem created on it, and mounted locally. Created a 5GB file, and I could see the objects being created in the rbd pool in the ceph cluster.]]></description>
</item><item>
    <title>OSD information in a scriptable format</title>
    <link>https://arvimal.github.io/posts/2015/09/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</link>
    <pubDate>Fri, 18 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/09/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</guid>
    <description><![CDATA[In case you are trying to get the OSD ID and the corresponding node IP address mappings in a script-able format, use the following command:
 # ceph osd find  This will print the OSD number, the IP address, the host name, and the default root in the CRUSH map, as a python dictionary.
 # ceph osd find 2 { &ldquo;osd&rdquo;: 2, &ldquo;ip&rdquo;: &ldquo;192.168.122.112:6800\/5311&rdquo;, &ldquo;crush_location&rdquo;: { &ldquo;host&rdquo;: &ldquo;node4&rdquo;, &ldquo;root&rdquo;: &ldquo;default&rdquo;}}]]></description>
</item><item>
    <title>Monitor maps, how to edit them?</title>
    <link>https://arvimal.github.io/posts/2015/09/2015-09-01-how-to-extract-view-change-and-inject-a-monitor-map-in-a-ceph-cluster/</link>
    <pubDate>Tue, 01 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/09/2015-09-01-how-to-extract-view-change-and-inject-a-monitor-map-in-a-ceph-cluster/</guid>
    <description><![CDATA[The MON map is used by the monitors in a Ceph cluster, where they keep track of various attributes relevant to the working of the cluster.
Similar to the CRUSH map, a monitor map can be pulled out of the cluster, inspected, changed, and injected back to the monitors, manually. A frequent use-case is when the IP address of a monitor changes and the monitors cannot agree on a quorum.]]></description>
</item><item>
    <title>Calculate a PG id from the hex values in Ceph OSD debug logs</title>
    <link>https://arvimal.github.io/posts/2015/08/2015-08-30-calculate-a-pg-id-from-the-ceph-osd-debug-logs/</link>
    <pubDate>Sun, 30 Aug 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/08/2015-08-30-calculate-a-pg-id-from-the-ceph-osd-debug-logs/</guid>
    <description><![CDATA[Recently, I had an incident where the OSDs were crashing at the time of startup. Obviously, the next step was to enable debug logs for the OSDs and understand where they were crashing.
Enabled OSD debug logs dynamically by injecting it with:
 # ceph tell osd.* injectargs &ndash;debug-osd 20 &ndash;debug-ms 1
 NOTE: This command can be run from the MON nodes.
Once this was done, the OSDs were started manually (since it were crashing and not running) and watched out for the next crash.]]></description>
</item><item>
    <title>Mapping Placement Groups and Pools</title>
    <link>https://arvimal.github.io/posts/2015/08/2015-08-17-how-can-we-map-a-pg-to-a-pool/</link>
    <pubDate>Mon, 17 Aug 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/08/2015-08-17-how-can-we-map-a-pg-to-a-pool/</guid>
    <description><![CDATA[Understanding the mapping of Pools and Placement Groups can be very useful while troubleshooting Ceph problems.
A direct method is to dump information on the PGs via :
 # ceph pg dump
 This command should output something like the following:
 pg_stat objects mip degr unf bytes log disklog state 5.7a 0 0 0 0 0 0 0 active+clean
 The output will have more information, and I&rsquo;ve omitted it for the sake of explanation.]]></description>
</item><item>
    <title>Resetting Calamari password</title>
    <link>https://arvimal.github.io/posts/2015/07/2015-07-13-how-can-we-resetchange-the-calamari-interface-password/</link>
    <pubDate>Mon, 13 Jul 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/07/2015-07-13-how-can-we-resetchange-the-calamari-interface-password/</guid>
    <description><![CDATA[&lsquo;Calamari&rsquo; is the monitoring interface for a Ceph cluster.
The Calamari interface password can be reset/changed using the &lsquo;calamari-ctl&rsquo; command.
 # calamari-ctl change_password &ndash;password {password} {user-name}
 calamari-ctl can also be used to add a user, as well as disable, enable, and rename the user account. A &lsquo;&ndash;help&rsquo; should print out all the available ones.
 # calamari-ctl &ndash;help
 ]]></description>
</item><item>
    <title>Compacting a Ceph monitor store</title>
    <link>https://arvimal.github.io/posts/2015/07/2015-07-09-how-to-compact-a-ceph-monitor-store/</link>
    <pubDate>Thu, 09 Jul 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/07/2015-07-09-how-to-compact-a-ceph-monitor-store/</guid>
    <description><![CDATA[The Ceph monitor store growing to a big size is a common occurrence in a busy Ceph cluster.
If a &lsquo;ceph -s&rsquo; takes considerable time to return information, one of the possibility is the monitor database being large.
Other reasons included network lags between the client and the monitor, the monitor not responding properly due to the system load, firewall settings on the client or monitor etc..
The best way to deal with a large monitor database is to compact the monitor store.]]></description>
</item><item>
    <title>What is data scrubbing?</title>
    <link>https://arvimal.github.io/posts/2015/07/2015-07-08-what-is-data-scrubbing/</link>
    <pubDate>Wed, 08 Jul 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/07/2015-07-08-what-is-data-scrubbing/</guid>
    <description><![CDATA[Data Scrubbing is an error checking and correction method or routine check to ensure that the data on file systems are in pristine condition, and has no errors. Data integrity is of primary concern in today&rsquo;s conditions, given the humongous amounts of data being read and written daily.
A simple example for a scrubbing, is a file system check done on file systems with tools like &lsquo;e2fsck&rsquo; in EXT2/3/4, or &lsquo;xfs_repair&rsquo; in XFS.]]></description>
</item></channel>
</rss>
