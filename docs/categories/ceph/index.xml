<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ceph - Category - The Child is Father of the Man</title>
        <link>https://arvimal.github.io/categories/ceph/</link>
        <description>ceph - Category - The Child is Father of the Man</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 30 Jun 2016 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://arvimal.github.io/categories/ceph/" rel="self" type="application/rss+xml" /><item>
    <title>Sharding the Ceph RADOS Gateway bucket index</title>
    <link>https://arvimal.github.io/posts/2016/06/2016-06-30-sharding-the-ceph-rados-gateway-bucket-index/</link>
    <pubDate>Thu, 30 Jun 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/06/2016-06-30-sharding-the-ceph-rados-gateway-bucket-index/</guid>
    <description><![CDATA[_S_harding is the process of breaking down data onto multiple locations so as to increase parallelism, as well as distribute load. This is a common feature used in databases. Read more on this at Wikipedia.
The concept of sharding is used in Ceph, for splitting the bucket index in a RADOS Gateway.
RGW or RADOS Gateway keeps an index for all the objects in its buckets for faster and easier lookup.]]></description>
</item><item>
    <title>Ceph OSD heartbeats</title>
    <link>https://arvimal.github.io/posts/2016/05/2016-05-09-ceph-osd-heartbeats/</link>
    <pubDate>Mon, 09 May 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/05/2016-05-09-ceph-osd-heartbeats/</guid>
    <description><![CDATA[Ceph OSD daemons need to ensure that the neighbouring OSDs are functioning properly so that the cluster remains in a healthy state.
For this, each Ceph OSD process (ceph-osd) sends a heartbeat signal to the neighbouring OSDs. By default, the heartbeat signal is sent every 6 seconds [1], which is configurable of course.
If the heartbeat check from one OSD doesn&rsquo;t hear from the other within the set value for `osd_heartbeat_grace` [2], which is set to 20 seconds by default, the OSD that sends the heartbeat check reports the other OSD (the one that didn&rsquo;t respond within 20 seconds) as down, to the MONs.]]></description>
</item><item>
    <title>`ceph-check` - A Ceph installation checker</title>
    <link>https://arvimal.github.io/posts/2016/05/2016-05-08-ceph-check-a-ceph-installation-checker/</link>
    <pubDate>Sun, 08 May 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/05/2016-05-08-ceph-check-a-ceph-installation-checker/</guid>
    <description><![CDATA[Many a user wants to know if a Ceph cluster installation has been done to a specific suggested guideline.
Technologies like RAID is better avoided in Ceph due to an additional layer, which Ceph already takes care of.
I&rsquo;ve started writing a tool which can be run from the Admin node, and it aims to check various such points.
The code can be seen at https://github.com/arvimal/ceph_check
The work is slow, really slow, due to my daily work, procrastination, and what not, even though I intend to finish this fast.]]></description>
</item><item>
    <title>How to get a Ceph MON/OSD map at a specific epoch?</title>
    <link>https://arvimal.github.io/posts/2016/05/2016-05-08-how-to-get-a-ceph-monosd-map-at-a-specific-epoch/</link>
    <pubDate>Sun, 08 May 2016 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2016/05/2016-05-08-how-to-get-a-ceph-monosd-map-at-a-specific-epoch/</guid>
    <description><![CDATA[To get a MON map or an OSD map of a specific epoch, use:
 # ceph osd getmap # ceph mon getmap  The map can be forwarded to a file as following:
 # ceph osd getmap -o /tmp/ceph_osd_getmap.bin
 This would be in a binary format, and hence will need to be dumped to a human-readable form.
 # osdmaptool &ndash;print /tmp/ceph-osd-getmap.bin
 This will print the current OSD map, similar to the output of &lsquo;ceph osd dump&rsquo;.]]></description>
</item><item>
    <title>List RBD images, snapshots, and clones in Ceph pools</title>
    <link>https://arvimal.github.io/posts/2015/10/2015-10-15-list-all-ceph-pools-with-rbd-images-and-snapshots/</link>
    <pubDate>Thu, 15 Oct 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/10/2015-10-15-list-all-ceph-pools-with-rbd-images-and-snapshots/</guid>
    <description><![CDATA[This is a crude bash one-liner I did to get the details of all the RBD images, as well as the information on snapshots and clones created from them.
[code language=&ldquo;bash&rdquo;] # for pool in `rados lspools`; do echo &ldquo;POOL :&rdquo; $pool; rbd ls -l $pool; echo &ldquo;&mdash;&ndash;&quot;; done [/code]
This will print an output similar to the following:
[code language=&ldquo;bash&rdquo;] POOL : rbd NAME SIZE PARENT FMT PROT LOCK test_img 10240M 1 test_img2 1024M 2 test_img2@snap2 1024M 2 yes &mdash;&ndash; POOL : .]]></description>
</item><item>
    <title>Ceph and unfound objects</title>
    <link>https://arvimal.github.io/posts/2015/10/2015-10-07-why-do-objects-get-marked-as-unfound-in-a-a-ceph-cluster/</link>
    <pubDate>Wed, 07 Oct 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/10/2015-10-07-why-do-objects-get-marked-as-unfound-in-a-a-ceph-cluster/</guid>
    <description><![CDATA[In certain cases, a Ceph cluster may move away from an HEALTHY state due to “unfound” objects.
A “ceph -s” should show if you have any unfound objects. So, what are unfound objects? How does an object become “unfound”? This article tries to explain why/how “unfound” objects come into existence.
Let’s look into the life cycle of a write to a pool.
 The client contacts a Ceph monitor and fetches the CRUSH map, which includes:  MON map OSD map PG map CRUSH map MDS map    Once the client has the maps, the Ceph client-side algorithm breaks the data being written into objects (the object size depends on the client side configuration).]]></description>
</item><item>
    <title>Ceph Rados Block Device (RBD) and TRIM</title>
    <link>https://arvimal.github.io/posts/2015/10/2015-10-07-objects-remain-in-a-ceph-pool-used-for-rbd-even-if-the-files-are-deleted-from-the-mount-point/</link>
    <pubDate>Wed, 07 Oct 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/10/2015-10-07-objects-remain-in-a-ceph-pool-used-for-rbd-even-if-the-files-are-deleted-from-the-mount-point/</guid>
    <description><![CDATA[I recently came across a scenario where the objects in a RADOS pool used for an RBD block device doesn’t get removed, even if the files created through the mount point were removed.
I had an RBD image from an RHCS1.3 cluster mapped to a RHEL7.1 client machine, with an XFS filesystem created on it, and mounted locally. Created a 5GB file, and I could see the objects being created in the rbd pool in the ceph cluster.]]></description>
</item><item>
    <title>Custom CRUSH rulesets and pools</title>
    <link>https://arvimal.github.io/posts/2015/09/2015-09-23-how-to-find-the-crush-ruleset-on-which-a-pool-was-created/</link>
    <pubDate>Wed, 23 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/09/2015-09-23-how-to-find-the-crush-ruleset-on-which-a-pool-was-created/</guid>
    <description><![CDATA[Ceph supports custom rulesets via CRUSH, which can be used to sort hardware based on various features such as speed and other factors, set custom weights, and do a lot of other useful things.
Pools, or the buckets were the data is written to, can be created on the custom rulesets, hence positioning the pools on specific hardware as per the administrator&rsquo;s need.
A large Ceph cluster may have lots of pools and rulesets specific for multiple use-cases.]]></description>
</item><item>
    <title>OSD information in a scriptable format</title>
    <link>https://arvimal.github.io/posts/2015/09/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</link>
    <pubDate>Fri, 18 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/09/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</guid>
    <description><![CDATA[In case you are trying to get the OSD ID and the corresponding node IP address mappings in a script-able format, use the following command:
 # ceph osd find  This will print the OSD number, the IP address, the host name, and the default root in the CRUSH map, as a python dictionary.
 # ceph osd find 2 { &ldquo;osd&rdquo;: 2, &ldquo;ip&rdquo;: &ldquo;192.168.122.112:6800\/5311&rdquo;, &ldquo;crush_location&rdquo;: { &ldquo;host&rdquo;: &ldquo;node4&rdquo;, &ldquo;root&rdquo;: &ldquo;default&rdquo;}}]]></description>
</item><item>
    <title>Monitor maps, how to edit them?</title>
    <link>https://arvimal.github.io/posts/2015/09/2015-09-01-how-to-extract-view-change-and-inject-a-monitor-map-in-a-ceph-cluster/</link>
    <pubDate>Tue, 01 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://arvimal.github.io/posts/2015/09/2015-09-01-how-to-extract-view-change-and-inject-a-monitor-map-in-a-ceph-cluster/</guid>
    <description><![CDATA[The MON map is used by the monitors in a Ceph cluster, where they keep track of various attributes relevant to the working of the cluster.
Similar to the CRUSH map, a monitor map can be pulled out of the cluster, inspected, changed, and injected back to the monitors, manually. A frequent use-case is when the IP address of a monitor changes and the monitors cannot agree on a quorum.]]></description>
</item></channel>
</rss>
