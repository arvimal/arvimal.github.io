How to use the 'ceph_objectstore_tool' utility?
###############################################
:date: 2015-08-30 20:48
:author: arvimal
:slug: 363
:status: draft

At this point in time we do not believe it would be wise to change the crush map

in the cluster. Since the two pools you mention are sharing the same crush rule

with pool 12 (the one we are working on), adjusting the crush rule will change

the way they find the right pg/osds for an object.

>Also are there any pgs in these pools that require fixing.

This is unknown at this time however it is possible. We can not predict what

corrupted pgs these two OSDs have in their store.

In order to proceed we would like you to download the following rpm to a

working directory.

http://ceph.com/rpm-giant/el7/x86_64/ceph-test-0.87.2-0.el7.centos.x86_64.rpm

Once you have it please run the following commands to extract the

ceph_objectstore_tool.

# mkdir tmp

# cd tmp

# rpm2cpio ../ceph-test-0.87.2-0.el7.centos.x86_64.rpm|cpio -div

# mv -i ./usr/bin/ceph_objectstore_tool ..

# cd ..

We have identified the following pgs which are causing the OSDs to crash.

OSD.12 pg 12.5d7

OSD.57 pg 12.393

We would like to do this one OSD at a time and starting with OSD.57.

Please verify you have enough space in your working directory to export the pg.

You should be able to verify this with Â the following command.

# du -sh /lib/ceph/osd/ceph-57/current/12.393_head

Once you verify this proceed with the following command which will export this

pg from the disk to a file, 12.393.export for backup before we proceed.

# ./ceph_objectstore_tool --op export --pgid 12.393 --data-path /lib/ceph/osd/ceph-57 --journal-path /lib/ceph/osd/ceph-57/journal --file 12.393.export

If that is successful you can move onto remove the pg with the following

command.

# ./ceph_objectstore_tool --op remove --pgid 12.393 --data-path /lib/ceph/osd/ceph-57 --journal-path /lib/ceph/osd/ceph-57/journal

Once that is done try to start the OSD again and if it crashes again please

upload a debug log so we can identify which pg is involved. Once we get OSD.57

back up we can check the status of the cluster and move on as necessary.

Please note that there may be multiple more corrupted pgs in this store and

we are unable to predict how many.
