`ceph df` and disk space
########################
:date: 2016-06-16 17:35
:author: arvimal
:slug: 1389
:status: draft

| https://access.redhat.com/solutions/2273951 (Important)
| https://access.redhat.com/solutions/1598473

| My findings:
| -----------
| The \`GLOBAL\` section of \`ceph df\` shows metrics of all the OSDs which are UP and IN, and this does not take the replica count into picture.

An example:

| ~~~
| # ceph df
| GLOBAL:
| SIZE AVAIL RAW USED %RAW USED
| 27581M 27364M 217M 0.79
| POOLS:
| NAME ID USED %USED MAX AVAIL OBJECTS
| rbd 0 0 0 6050M 0
| images 1 0 0 6050M 0
| volumes 2 0 0 6050M 0
| vms 3 0 0 6050M 0
| ~~~

The output of \`ceph osd df\` gives us more information. Check the \`TOTAL\` size printed and compare it to \`SIZE\` in the GLOBAL section of the first output. (27581k is a typo, it should be in M. Will check that part)

| ~~~
| # ceph osd df
| ID WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR
| 0 0.00998 1.00000 3060M 41036k 3020M 1.31 1.57
| 1 0.00999 1.00000 6132M 40240k 6093M 0.64 0.77
| 3 0.00998 1.00000 3060M 37688k 3024M 1.20 1.45
| 5 0.00999 1.00000 6132M 39816k 6094M 0.63 0.76
| 2 0.00998 1.00000 3060M 36904k 3024M 1.18 1.42
| 4 0.00999 1.00000 6132M 39320k 6094M 0.63 0.75
| TOTAL 27581k 229M 27352M 0.83
| MIN/MAX VAR: 0.75/1.57 STDDEV: 0.32
| ~~~

To test this, push an OSD out of the cluster. The SIZE field will display a different value, ie.. the total size minus the size of the OSD pushed out of the cluster. (27581 - 3026 = 24555 ~ 24520)

| ~~~
| # ceph osd out osd.3 ; ceph df
| marked out osd.3.
| GLOBAL:
| SIZE AVAIL RAW USED %RAW USED
| 24520M 24334M 186M 0.76
| POOLS:
| NAME ID USED %USED MAX AVAIL OBJECTS
| rbd 0 0 0 6048M 0
| images 1 0 0 6048M 0
| volumes 2 0 0 6048M 0
| vms 3 0 0 6048M 0
| ~~~

Pulling the OSD in will increase the SIZE back to the previous value. Just do a 'ceph osd in osd.3 ; sleep 5; ceph df'.

I personally think that Horizon shouldn't be reporting the value under \`GLOBAL\` section as available since this is the sum of all the OSDs in the cluster. There can be cases where certain OSD nodes have more disks than others, and of different sizes as well.

Ultimately, the writes are going into pools which are bound to rulesets. A ruleset is a set of rules which can include or exclude OSDs depending on various factors. With rulesets, the administrator can create pools to use specific rulesets and thus bind it to a set of OSDs.

Since the writes are going to specific pools, and pools can be bounded to specific OSDs, I think it'd be good to show the pool size in Horizon to which the writes are going to, rather than the entire disk space given in the \`GLOBAL\` section.

Suggestion:

The \`MAX AVAIL\` space would be a better choice since it takes care of the replica size as well and is the actual available space per pool. Since this is the space available for a single replica, I think this should be the actual size displayed in Horizon.

\`MAX AVAIL\` is calculated as following (documented in https://access.redhat.com/solutions/2273951)

\`MAX AVAIL\` = min(osd.avail for osd in OSD_up) \* len(osd.avail for osd in OSD_up) / pool.size()

\* min(osd.avail for osd in OSD_up) : The Minimum space available in the OSDs which are UP and IN, for that particular pool (defined by a ruleset)

\* len(osd.avail for osd in OSD_up) : The number of OSDs UP and IN

\* pool.size() : The replication size of the pool in question.

In the above example, all the pools have the same \`MAX AVAIL\` size since they're all using the default ruleset. Any pool can consume the space, and as per the consumption, the \`MAX AVAIL\` value reduces.

The above 'ceph df' shows a \`MAX AVAIL\` of 6048M per pool. The min space available per OSD here is 3020M, the number of OSD disks are 6, and the replica count is 3.

Hence \`MAX AVAIL\` is ((3020 \* 6) / 3)) = 6040M which is approximately equal to the space per pool shown in \`ceph df`.

We're taking the minimum space available as 3020M since that's the minimum consistently available space per each OSD due to one OSD per each node smaller than the other OSD.

If we had all the OSDs of the same size, the minimum available space would be different. We can test this out by pushing out all the smaller sized OSDs out of the cluster.

| ~~~
| [root@overcloud-cephstorage-0 ~]# ceph osd out osd.0
| marked out osd.0.
| [root@overcloud-cephstorage-0 ~]# ceph osd out osd.3
| marked out osd.3.
| [root@overcloud-cephstorage-0 ~]# ceph osd out osd.2
| marked out osd.2.

| # ceph osd df
| ID WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR
| 0 0.00998 0 0 0 0 0 0
| 1 0.00999 1.00000 6132M 36564k 6097M 0.58 0.92
| 3 0.00998 0 0 0 0 0 0
| 5 0.00999 1.00000 6132M 41252k 6092M 0.66 1.04
| 2 0.00998 0 0 0 0 0 0
| 4 0.00999 1.00000 6132M 40772k 6093M 0.65 1.03
| TOTAL 18398k 115M 18283M 0.63
| MIN/MAX VAR: 0/1.04 STDDEV: 0.03

| [root@overcloud-cephstorage-0 ~]# ceph df
| GLOBAL:
| SIZE AVAIL RAW USED %RAW USED
| 18398M 18283M 115M 0.63
| POOLS:
| NAME ID USED %USED MAX AVAIL OBJECTS
| rbd 0 0 0 12176M 0
| images 1 0 0 12176M 0
| volumes 2 0 0 12176M 0
| vms 3 0 0 12176M 0
| ~~~

Calculating the \`MAX AVAIL\` from the above output:

| Minimum guaranteed space per OSD = 6092M
| Number of OSDs which are UP = 6 (The three OSDs we pushed out are still UP and are part of the CRUSH map)
| Number of replicas = 3

(6092 \* 6) / 3 = 12184 ~ \`MAX AVAIL`.

Perhaps there is a reason why it was decided to make Horizon show values from the GLOBAL section of \`ceph df`. But to more clearly depict the status, shouldn't it be showing the pool space that the component is writing to?

Vimal
