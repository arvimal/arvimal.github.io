<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>osd - Tag - The Child is Father of the Man</title>
        <link>https://arvimal.github.io/tags/osd/</link>
        <description>osd - Tag - The Child is Father of the Man</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>arvimal81@gmail.com (Vimal A.R)</managingEditor>
            <webMaster>arvimal81@gmail.com (Vimal A.R)</webMaster><lastBuildDate>Fri, 18 Sep 2015 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://arvimal.github.io/tags/osd/" rel="self" type="application/rss+xml" /><item>
    <title>OSD information in a scriptable format</title>
    <link>https://arvimal.github.io/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</link>
    <pubDate>Fri, 18 Sep 2015 00:00:00 &#43;0000</pubDate>
    <author>Vimal A.R</author>
    <guid>https://arvimal.github.io/2015-09-18-how-to-get-a-listing-of-the-osd-nodes-in-an-easily-scriptable-format/</guid>
    <description><![CDATA[In case you are trying to get the OSD ID and the corresponding node IP address mappings in a script-able format, use the following command:
# ceph osd find This will print the OSD number, the IP address, the host name, and the default root in the CRUSH map, as a python dictionary.
# ceph osd find 2 { &ldquo;osd&rdquo;: 2, &ldquo;ip&rdquo;: &ldquo;192.168.122.112:6800\/5311&rdquo;, &ldquo;crush_location&rdquo;: { &ldquo;host&rdquo;: &ldquo;node4&rdquo;,Â &ldquo;root&rdquo;: &ldquo;default&rdquo;}}
The output is in json format, which has a key:value format.]]></description>
</item>
<item>
    <title>What is data scrubbing?</title>
    <link>https://arvimal.github.io/what-is-data-scrubbing/</link>
    <pubDate>Wed, 08 Jul 2015 00:00:00 &#43;0000</pubDate>
    <author>Vimal A.R</author>
    <guid>https://arvimal.github.io/what-is-data-scrubbing/</guid>
    <description><![CDATA[Data Scrubbing is an error checking and correction method or routine check to ensure that the data on file systems are in pristine condition, and has no errors. Data integrity is of primary concern in today&rsquo;s conditions, given the humongous amounts of data being read and written daily.
A simple example for a scrubbing, is a file system check done on file systems with tools like &rsquo;e2fsck&rsquo; in EXT2/3/4, or &lsquo;xfs_repair&rsquo; in XFS.]]></description>
</item>
<item>
    <title>&#39;noout&#39; flag in Ceph</title>
    <link>https://arvimal.github.io/osds-noout-status/</link>
    <pubDate>Wed, 27 May 2015 00:00:00 &#43;0000</pubDate>
    <author>Vimal A.R</author>
    <guid>https://arvimal.github.io/osds-noout-status/</guid>
    <description><![CDATA[You may have seen the &lsquo;noout&rsquo; flag set in the output of &lsquo;ceph -s&rsquo;. What does this actually mean?
This is a global flag for the cluster, which means that if an OSD is out, the said OSD is not marked out of the cluster and data balancing shouldn&rsquo;t start to maintain the replica count. By default, the monitors mark the OSDs out of the acting set if it is not reachable for 300 seconds, ie.]]></description>
</item>
<item>
    <title>How to change the filling ratio for a Ceph OSD?</title>
    <link>https://arvimal.github.io/how-to-change-ceph-osd-filling-ratio/</link>
    <pubDate>Thu, 07 May 2015 00:00:00 &#43;0000</pubDate>
    <author>Vimal A.R</author>
    <guid>https://arvimal.github.io/how-to-change-ceph-osd-filling-ratio/</guid>
    <description><![CDATA[There could be many scenarios where you&rsquo;d need to change the percentage of space usage on a Ceph OSD. One such use case would be when your OSD space is about to hit the hard limit, and is constantly sending you warnings.
For some reason or other, you may need to extend the threshold limit for some time. In such a case, you don&rsquo;t need to change/add the configuration in ceph.]]></description>
</item>
<item>
    <title>How to remove a host from a Ceph cluster?</title>
    <link>https://arvimal.github.io/remove-a-host-from-ceph-cluster/</link>
    <pubDate>Thu, 07 May 2015 00:00:00 &#43;0000</pubDate>
    <author>Vimal A.R</author>
    <guid>https://arvimal.github.io/remove-a-host-from-ceph-cluster/</guid>
    <description><![CDATA[I&rsquo;m still studying Ceph, and recently faced a scenario in which one of my Ceph nodes went down due to hardware failure. Even though my data was safe due to the replication factor, I was not able to remove the node from the cluster.
I could remove the OSDs on the node, but I didn&rsquo;t find a way to remove the node being listed in &lsquo;ceph osd tree&rsquo;. I ended up editing the CRUSH map by hand, to remove the host, and uploaded it back.]]></description>
</item>
</channel>
</rss>
