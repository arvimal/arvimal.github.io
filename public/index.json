[{"categories":["personal"],"content":"Testing LoveIt Hugo theme Trying to revamp the blog with a beautiful theme - LoveIt. ","date":"10-08-2022","objectID":"/testing-loveit/:1:0","tags":["testing"],"title":"Testing LoveIt","uri":"/testing-loveit/"},{"categories":["programming","python"],"content":"Introduction A callable is an object in Python that can be called / executed when called with parantheses ( ). Classes and functions are callable. Callables can be a class, a function, or an instance of a class. In simple terms, a class/function/instance/builtin is callable if it gets executed when called with parantheses (). Example 1: In [1]: help() Welcome to Python 3.6's help utility! -- content omitted -- -------- In [2]: int() Out[2]: 0 In [3]: callable(int) Out [3]: True In [4]: callable(help) Out [4]: True In [5]: def hello(): print(\"Howdy!!\") In [6]: hello() Out [6]: Howdy!! In [7]: callable(hello) Out [7]: True In Example 1, we can see that builtins like help(), a pre-defined type such as int(), and a custom function hello() are all callable. These can be executed while being called with parantheses. ","date":"09-08-2017","objectID":"/callables-in-python/:1:0","tags":["callable","python","python-objects"],"title":"Callables in Python","uri":"/callables-in-python/"},{"categories":["programming","python"],"content":"The call() method The callable() builtin helps to determine if an object is callable or not. Internally, it translates to the magic method __call__(). In short: my_object(*args) translates to my_object.__call__(*args) All classes and functions are callable, as well as instances of classes with the __call__ magic method. An instance of a class/function is usually not callable (even though the class/function itself is), unless the class carries a __call__ magic method. ie. An instance is callable only if the class it is instantiated from contains the __call__ magic method. The inbuilt documentation on callable states: In [1]: print(callable.__doc__) Return whether the object is callable (i.e., some kind of function). Note that classes are callable, as are instances of classes with a call() method. [/code] Example 2: In [5]: def hello(): ...: print(\"Howdy!!\") In [6]: hello() Out [6]: Howdy!! In [7]: hello.__call__() Out [7]: Howdy!! In [8]: callable(hello) Out [8]: True Example 2 shows that a function when called with the parantheses (including any required arguments) is equivalent to calling the __call__() magic method. ie.. calling a function/class with parantheses translates to calling the __call__() magic method. NOTE: Read more on Magic methods in Python Example 3: Non-callable Instances [code language=“bash”] In [1]: type(1) Out[1]: int In [2]: callable(1) Out[2]: False In [3]: x = 1 In [4]: type(x) Out[4]: int In [5]: callable(int) Out[5]: True In [6]: callable(x) Out[6]: False [/code] Example 3 above shows that even though the int class is callable, the instances created from the int class are not. Remember that instances will only be callable if the class from which it was instantiated contains a __call__ method. Inspecting the methods of class int reveals that it does not have a __call__ method. NOTE: You can view the methods of the int class using help(int) or dir(int). Example 4: Another example with Classes [code language=“bash”] In [52]: class tell: …: def call(self): …: pass In [53]: telling = tell() In [54]: callable(tell) Out[54]: True In [55]: callable(telling) Out[55]: True In [56]: class test: …: pass In [57]: testing = test() In [58]: callable(test) Out[58]: True In [59]: callable(testing) Out[59]: False [/code] Since all classes are by default callable, both the classes tell and test in Example 4 are callable. But the instances of these classes necessarily need not be so. Since the class tell has the magic method __call__, the instance telling is callable. But the instance testing instantiated from the class test is not since the class does not have the magic method. Another set of examples. Example 5: Non-callable instance of a class [code language=“bash”] In [1]: class new: …: def foo(self): …: print(“Hello”) In [2]: n = new() In [3]: n() —————— TypeError Traceback (most recent call last) in module() —-\u003e 1 n() TypeError: ’new’ object is not callable [/code] Example 6: Callable instance of the same class [code language=“bash”] In [4]: class new: …: def call(self): …: print(“I’m callable!”) In [5]: n = new() In [6]: n Out[6]: main.new at 0x7f7a614b1f98 In [7]: n() I’m callable! [/code] Example 5 and Example 6 shows how a class is itself callable, but unless it carries a __call__() method, the instances spawned out of it are not so. ","date":"09-08-2017","objectID":"/callables-in-python/:2:0","tags":["callable","python","python-objects"],"title":"Callables in Python","uri":"/callables-in-python/"},{"categories":["programming","python"],"content":"References: http://docs.python.org/3/library/functions.html#callable http://eli.thegreenplace.net/2012/03/23/python-internals-how-callables-work/ https://docs.python.org/3/reference/datamodel.html#object.__call__ ","date":"09-08-2017","objectID":"/callables-in-python/:2:1","tags":["callable","python","python-objects"],"title":"Callables in Python","uri":"/callables-in-python/"},{"categories":["linux","programming"],"content":"Introduction _L_oadable Kernel Modules (LKM) are object code that can be loaded into memory, often used for supporting hardware or enable specific features. Kernel modules enable the core kernel to be minimal and have features to be loaded as required. A kernel module is a normal file usually suffixed with .ko denoting it’s a kernel object file. It contains compiled code from one or more source files, gets linked to the kernel when loaded, and runs in kernel space. It can dynamically adds functionality to a running kernel, without requiring a reboot. Linux kernel modules are written in C, and is compiled for a specific kernel version. This is the ideal practice since kernel data structures may change across versions, and using a module compiled for a specific version may break for another. Since kernel modules can be loaded and unloaded at will, it is pretty easy to unload an older version and load a newer one. This helps immensely in testing out new features since it is easy to change the source code, re-compile, unload the older version, load the newer version, and test the functionality. ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:1:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Structure Modules are expected to be under /lib/modules/$(uname -r)/ within directories specified according to use case. [root@centos7 3.10.0-514.26.2.el7.x86_64] # ls -l total 2940 lrwxrwxrwx. 1 root root 43 Jul 8 05:10 build -\u003e /usr/src/kernels/3.10.0-514.26.2.el7.x86_64 drwxr-xr-x. 2 root root 6 Jul 4 11:17 extra drwxr-xr-x. 12 root root 128 Jul 8 05:10 kernel -rw-r--r--. 1 root root 762886 Jul 8 05:11 modules.alias -rw-r--r--. 1 root root 735054 Jul 8 05:11 modules.alias.bin -rw-r--r--. 1 root root 1326 Jul 4 11:17 modules.block -rw-r--r--. 1 root root 6227 Jul 4 11:15 modules.builtin -rw-r--r--. 1 root root 8035 Jul 8 05:11 modules.builtin.bin -rw-r--r--. 1 root root 240071 Jul 8 05:11 modules.dep -rw-r--r--. 1 root root 343333 Jul 8 05:11 modules.dep.bin -rw-r--r--. 1 root root 361 Jul 8 05:11 modules.devname -rw-r--r--. 1 root root 132 Jul 4 11:17 modules.drm -rw-r--r--. 1 root root 110 Jul 4 11:17 modules.modesetting -rw-r--r--. 1 root root 1580 Jul 4 11:17 modules.networking -rw-r--r--. 1 root root 90643 Jul 4 11:15 modules.order -rw-r--r--. 1 root root 89 Jul 8 05:11 modules.softdep -rw-r--r--. 1 root root 350918 Jul 8 05:11 modules.symbols -rw-r--r--. 1 root root 432831 Jul 8 05:11 modules.symbols.bin lrwxrwxrwx. 1 root root 5 Jul 8 05:10 source -\u003e build drwxr-xr-x. 2 root root 6 Jul 4 11:17 updates drwxr-xr-x. 2 root root 95 Jul 8 05:10 vdso drwxr-xr-x. 2 root root 6 Jul 4 11:17 weak-updates As we can see, there are several files that deals with the inter-dependencies of modules, which is used by modprobe to understand which modules to load before the one being actually requested to load. For example: modules.block lists the modules for block devices modules.networking lists the ones for network devices. modules.builtin lists the path of modules included in the kernel. modules.devname lists the ones that would be loaded automatically if a particular device is created. The kernel folder contains modules divided according to their use cases. [root@centos7 3.10.0-514.26.2.el7.x86_64]# ls -l kernel/ total 16 drwxr-xr-x. 3 root root 17 Jul 8 05:10 arch drwxr-xr-x. 3 root root 4096 Jul 8 05:10 crypto drwxr-xr-x. 67 root root 4096 Jul 8 05:10 drivers drwxr-xr-x. 26 root root 4096 Jul 8 05:10 fs drwxr-xr-x. 3 root root 19 Jul 8 05:10 kernel drwxr-xr-x. 5 root root 222 Jul 8 05:10 lib drwxr-xr-x. 2 root root 32 Jul 8 05:10 mm drwxr-xr-x. 33 root root 4096 Jul 8 05:10 net drwxr-xr-x. 11 root root 156 Jul 8 05:10 sound drwxr-xr-x. 3 root root 17 Jul 8 05:10 virt Each directory within kernel contains modules depending on the area they’re used for. For example, kernel/fs/ contains filesystem drivers. [root@centos7 3.10.0-514.26.2.el7.x86_64]# ls -l kernel/fs total 48 -rw-r--r--. 1 root root 21853 Jul 4 11:51 binfmt_misc.ko drwxr-xr-x. 2 root root 22 Jul 8 05:10 btrfs drwxr-xr-x. 2 root root 27 Jul 8 05:10 cachefiles drwxr-xr-x. 2 root root 21 Jul 8 05:10 ceph drwxr-xr-x. 2 root root 21 Jul 8 05:10 cifs drwxr-xr-x. 2 root root 23 Jul 8 05:10 cramfs drwxr-xr-x. 2 root root 20 Jul 8 05:10 dlm drwxr-xr-x. 2 root root 23 Jul 8 05:10 exofs drwxr-xr-x. 2 root root 21 Jul 8 05:10 ext4 drwxr-xr-x. 2 root root 51 Jul 8 05:10 fat drwxr-xr-x. 2 root root 24 Jul 8 05:10 fscache rwxr-xr-x. 2 root root 36 Jul 8 05:10 fuse drwxr-xr-x. 2 root root 21 Jul 8 05:10 gfs2 drwxr-xr-x. 2 root root 22 Jul 8 05:10 isofs drwxr-xr-x. 2 root root 21 Jul 8 05:10 jbd2 drwxr-xr-x. 2 root root 22 Jul 8 05:10 lockd -rw-r--r--. 1 root root 19597 Jul 4 11:51 mbcache.ko drwxr-xr-x. 6 root root 128 Jul 8 05:10 nfs drwxr-xr-x. 2 root root 40 Jul 8 05:10 nfs_common drwxr-xr-x. 2 root root 21 Jul 8 05:10 nfsd drwxr-xr-x. 2 root root 4096 Jul 8 05:10 nls drwxr-xr-x. 2 root root 24 Jul 8 05:10 overlayfs drwxr-xr-x. 2 root root 24 Jul 8 05:10 pstore drwxr-xr-x. 2 root root 25 Jul 8 05:10 squashfs drwxr-xr-x. 2 root root 20 Jul 8 05:10 udf drwxr-xr-x. 2 root root 20 Jul 8 05:10 xfs ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:2:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"depmod, and related commands Modules can export the features it carry, called symbols which can be used by other modules. If module A depends on a symbol exported by module B, module B should be loaded first followed by module A. depmod creates a list of symbol dependencies each module has, so that modprobe can go ahead and load the modules serving the symbols, prior loading the actual module. depmod works by: Creating a list of symbols each module exports. Creating a list of symbol dependencies each module has. Dumping the list of symbols each module exports, to lib/modules/$(uname -r)/modules.symbols.bin and /lib/modules/$(uname -r)/modules.symbols Dumping the module dependency information to /lib/modules/$(uname -r)/modules.dep.bin and /lib/modules/$(uname -r)/modules.dep. Creating /lib/modules/$(uname -r)/modules.devnames which contains the device file information (device type, major:minor number) that gets created at boot for this module to function properly. NOTE: modprobe refers /lib/modules/$(uname -r)/modules.dep.bin to understand the dependencies each module require. A human-readable version of this file is maintained at /lib/modules/$(uname -r)/modules.dep but modprobe does not refer this. The binary file modules.symbols.bin carry the symbols exported (if any) by each module, one symbol per line. A human-readable version of the same is kept at modules.symbols. A sneak peek into modules.symbols and modules.dep: ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:3:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"modules.symbols [root@centos7 3.10.0-514.26.2.el7.x86_64]# head modules.symbols # Aliases for symbols, used by symbol_request(). alias symbol:cfg80211_report_obss_beacon cfg80211 alias symbol:drm_dp_link_train_channel_eq_delay drm_kms_helper alias symbol:__twofish_setkey twofish_common alias symbol:mlx4_db_free mlx4_core alias symbol:nf_send_unreach nf_reject_ipv4 alias symbol:sdhci_remove_host sdhci alias symbol:videobuf_dma_init_kernel videobuf_dma_sg alias symbol:ar9003_paprd_is_done ath9k_hw alias symbol:cxgbi_ep_disconnect libcxgbi ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:3:1","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"modules.dep [root@centos7 3.10.0-514.26.2.el7.x86_64]# head modules.dep kernel/arch/x86/kernel/cpu/mcheck/mce-inject.ko: kernel/arch/x86/kernel/test_nx.ko: kernel/arch/x86/kernel/iosf_mbi.ko: kernel/arch/x86/crypto/ablk_helper.ko: kernel/crypto/cryptd.ko kernel/arch/x86/crypto/glue_helper.ko: kernel/arch/x86/crypto/camellia-x86_64.ko: kernel/crypto/xts.ko kernel/crypto/lrw.ko kernel/crypto/gf128mul.ko kernel/arch/x86/crypto/glue_helper.ko kernel/arch/x86/crypto/blowfish-x86_64.ko: kernel/crypto/blowfish_common.ko kernel/arch/x86/crypto/twofish-x86_64.ko: kernel/crypto/twofish_common.ko kernel/arch/x86/crypto/twofish-x86_64-3way.ko: kernel/arch/x86/crypto/twofish-x86_64.ko kernel/crypto/twofish_common.ko kernel/crypto/xts.ko kernel/crypto/lrw.ko kernel/crypto/gf128mul.ko kernel/arch/x86/crypto/glue_helper.ko kernel/arch/x86/crypto/salsa20-x86_64.ko: lsmod is a parser that reads through /proc/modules and presents it in an easy-to-read format. Note how lsmod parse throug the content of /proc/modules below: [root@centos7 3.10.0-514.26.2.el7.x86_64]# head /proc/modules test 12498 0 - Live 0xffffffffa0492000 (POE) binfmt_misc 17468 1 - Live 0xffffffffa048c000 uhid 17369 0 - Live 0xffffffffa0486000 ipt_MASQUERADE 12678 2 - Live 0xffffffffa0481000 nf_nat_masquerade_ipv4 13412 1 ipt_MASQUERADE, Live 0xffffffffa0451000 xt_addrtype 12676 2 - Live 0xffffffffa044c000 br_netfilter 22209 0 - Live 0xffffffffa0468000 dm_thin_pool 65565 1 - Live 0xffffffffa046f000 dm_persistent_data 67216 1 dm_thin_pool, Live 0xffffffffa0456000 dm_bio_prison 15907 1 dm_thin_pool, Live 0xffffffffa043f000 [root@centos7 3.10.0-514.26.2.el7.x86_64]# lsmod | head Module Size Used by test 12498 0 binfmt_misc 17468 1 uhid 17369 0 ipt_MASQUERADE 12678 2 nf_nat_masquerade_ipv4 13412 1 ipt_MASQUERADE xt_addrtype 12676 2 br_netfilter 22209 0 dm_thin_pool 65565 1 dm_persistent_data 67216 1 dm_thin_pool NOTE: The first field lists the module name. The second field lists the size of the module in memory. The third field lists the number of times the module is in use. `0` means the module is not used despite it being loaded. The fourth field lists the modules which uses this module as their dependency. ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:3:2","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Creating a dummy module The steps for creating a kernel module includes: Writing the module file. Writing the Makefile for the module. Compile the module file using make , which will refer the Makefile to build it. The module file and its corresponding Makefile are put in a separate directory so as to keep the kernel module directory clean. Once the module code and the Makefile are ready, the make command is used to build the module, $(PWD) being the directory with the module code and Makefile. # make -C /lib/modules/$(uname -r)/build M=$PWD modules The make command above does the following: Change to the path mentioned after -C, ie.. to the location where the kernel Makefile is present. (/lib/modules/$(uname -r)/build/) Use the kernel Makefile’s macro M which denotes the location from which the code should be compiled, ie.. in this case, the PWD where the module code/Makefile is present. Use the target modules which tells make to build the module. make is trying to build a module in the current working directory, using the kernel Makefile at /lib/modules/$(uname -r)/build/Makefile If we have a module file named test.c and its corresponding Makefile in $(PWD), the make command would follow the steps below: make calls the modules target and refers to the kernel Makefile. The kernel Makefile looks for the module Makefile in $PWD. The kernel Makefile read the module’s Makefile and gets a list of the objects assigned to the macro obj-m. The make command builds modules for each object assigned to the macro obj-m. ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:4:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Writing a simple module The following is a very simple module, which prints a message while loading, and another one while unloading. int test_module(void) { printk(\"Loading the test module!\\\\n\"); return 0; } void unload_test(void) { printk(\"Unloading the test module!\\\\n\"); } module_init(test_module) module_exit(unload_test) This has two functions, test_module() and unload_test() which simply prints a text banner upon loading and unloading respectively. module_init() is used to load the module, and can call whatever functions that needs to initialize the module. We load our test_module() function into module_init() so that it gets initialized when the module is loaded. module_exit() is called whenever the module has to be unloaded, and it can take in whatever functions are required to do a proper cleanup (if required) prior the module being unloaded. We load our unload_test() function in module_exit(). ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:5:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Writing a Makefile Since the kernel Makefile will look in for the obj-m macro in the module Makefile with the object filename as its argument, add the following in the Makefile: obj-m := test.o make will create an object file test.o from test.c, and then create a kernel object file test.ko. ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:6:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Compiling the module with make Let’s compile the module [root@centos7 test]# pwd /lib/modules/3.10.0-514.26.2.el7.x86_64/test [root@centos7 test]# ls Makefile test.c [root@centos7 test]# make -C /lib/modules/$(uname -r)/build M=$PWD modules make: Entering directory `/usr/src/kernels/3.10.0-514.26.2.el7.x86_64' CC \\[M\\] /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.o Building modules, stage 2. MODPOST 1 modules CC /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.mod.o LD \\[M\\] /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.ko make: Leaving directory \\`/usr/src/kernels/3.10.0-514.26.2.el7.x86_64' Listing the contents show lot of new files, including the module code, the Makefile, the object file test.o created from test.c, the kernel object file test.ko. test.mod.c contains code which should be the one ultimately being built to test.ko, but that should be for another post since much more is yet to be read/learned on what’s happening there. [root@centos7 test]# ls -l total 292 -rw-r--r--. 1 root root 16 Jul 27 11:52 Makefile -rw-r--r--. 1 root root 60 Jul 27 11:57 modules.order -rw-r--r--. 1 root root 0 Jul 27 11:57 Module.symvers -rw-r--r--. 1 root root 281 Jul 27 11:53 test.c -rw-r--r--. 1 root root 137768 Jul 27 11:57 test.ko -rw-r--r--. 1 root root 787 Jul 27 11:57 test.mod.c -rw-r--r--. 1 root root 52912 Jul 27 11:57 test.mod.o -rw-r--r--. 1 root root 87776 Jul 27 11:57 test.o ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:7:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["linux","programming"],"content":"Loading/Unloading the module Loading and unloading the module should print the messages passed via printk in dmesg. [root@centos7 test]# insmod ./test.ko [root@centos7 test]# lsmod | grep test test 12498 0 [root@centos7 test]# rmmod test Checking dmesg shows the informational messages in the module code: [root@centos7 test]# dmesg | tail [35889.187282] test: loading out-of-tree module taints kernel. [35889.187288] test: module license 'unspecified' taints kernel. [35889.187290] Disabling lock debugging due to kernel taint [35889.187338] test: module verification failed: signature and/or required key missing - tainting kernel [35889.187548] Loading the test module! [35899.216954] Unloading the test module! Note the messages about the module test tainting the kernel. Read more on how a module can taint the kernel, at https://www.kernel.org/doc/html/latest/admin-guide/tainted-kernels.html. More on customizing the Makefile in another post. ","date":"27-07-2017","objectID":"/writing-a-minimalistic-kernel-module/:8:0","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1","uri":"/writing-a-minimalistic-kernel-module/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"_R_ecursion is a technique by which a function calls itself until a condition is met. ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:0:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Introduction Loops or repetitive execution based on certain conditions are inevitable in programs. Usual loops include if, while and for loops. Recursion is an entirely different way to deal with such situations, and in many cases, easier. Recursion is a when a function calls itself in each iteration till a condition is met. Ideally, the data set in each iteration gets smaller until it reach the required condition, after which the recursive function exists. A typical example of recursion is a factorial function. ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:1:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"How does Recursion work? A recursive function ideally contains a Base case and a Recursive case. A Recursive case is when the function calls itself, until the Base case is met. Each level of iteration in the Recursive case moves the control to the next level. Once a specific level finishes execution, the control is passed back to the previous level of execution. A Recursive function can go several layers deep until the Base condition is met. In short, a Recursive case is a loop in which the function calls itself. The Base case is required so that the function doesn’t continue running in the Recursive loop forever. Once the Base case is met, the control moves out of the Recursive case, executes the conditions in the Base case (if any), and exits. As mentioned in the Introduction, a factorial function can be seen as an example of recursion. ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:2:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"NOTE: The Base case for a factorial function is when n == 1 Consider n!: n! can be written as: n x (n - 1) x (n - 2) x (n - 3) x …. x 1 n! can also be represented as: [code language=“bash”] n! = n * (n - 1)! —\u003e [Step 1] (n - 1)! = (n - 1) * (n - 2)! —\u003e [Step 2] (n - 2)! = (n - 2) * (n - 3)! —\u003e [Step 3] . .. … (n - (n - 1)) = 1 —\u003e [Base case] [/code] Each level/step is a product of a value and all the levels below it. Hence, Step 1 will end up moving to Step 2 to get the factorial of elements below it, then to Step 3 and so on. ie.. the control of execution move as: [Step 1] -\u003e [Step 2] -\u003e [Step 3] -\u003e ….. [Step n] In a much easier-to-grasp example, a 5! would be: [code language=“bash”] 5! = 5 * 4! —\u003e [Step 1] 4! = 4 * 3! —\u003e [Step 2] 3! = 3 * 2! —\u003e [Step 3] 2! = 2 * 1! —\u003e [Step 4] 1! = 1 —\u003e [Step 5] / [Base case] [/code] The order of execution will be : [Step 1] -\u003e [Step 2] -\u003e [Step 3] -\u003e [Step 4] -\u003e [Step 5] As we know, in Recursion, each layer pause itself and pass the control to the next level. Once it reach the end or the Base case, it returns the result back to the previous level one by one until it reaches where it started off. In this example, once the control of execution reaches Step 5 / Base case , the control is returned back to its previous level Step 4 . This level returns the result back to Step 3 which completes its execution and returns to Step 2 , so on and so forth until it reach Step 1 . The return control flow would be as: [Base case / Step 5] -\u003e [Step 4] -\u003e [Step 3] -\u003e [Step 2] -\u003e [Step 1] -\u003e Result. This can be summed up using an awesome pictorial representation, from the book Grokking Algorithms by Adit. Please check out the References section for the link for more information about this awesome book. Figure 1: Recursion, Recursive case and Base case (Copyright Manning Publications, drawn by adit.io) ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:2:1","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":" Code ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:3:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Example 1: A factorial function in a while loop [code language=“python”] def fact(n): factorial = 1 while n \u003e 1: factorial = factorial * n n = n - 1 return factorial print(“Factorial of {0} is {1}\".format(10, fact(10))) print(“Factorial of {0} is {1}\".format(20, fact(20))) [/code] The same function above, in a recursive loop [code language=“python”] def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) print(“Factorial of {0} is {1}\".format(10, factorial(10))) print(“Factorial of {0} is {1}\".format(20, factorial(20))) [/code] ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:3:1","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Example 2: A function to sum numbers in a normal for loop. [code language=“python”] def my_sum(my_list): num = 0 for i in my_list: num += i return num print(my_sum([10, 23, 14, 12, 11, 94, 20])) [/code] The same function to add numbers, in a recursive loop [code language=“python”] def my_sum(my_list): if my_list == []: return 0 else: return my_list[0] + my_sum(my_list[1:]) print(my_sum([10, 23, 14, 12, 11, 94, 20])) [/code] ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:3:2","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Code explanation Both Example 1 and Example 2 are represented as an iterative function as well as a recursive function. The iterative function calls the next() function on the iterator sum.__iter__() magic method iterate over the entire data set. The recursive function calls itself to reach a base case and return the result. ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:4:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Observations: While a recursive function does not necessarily give you an edge on performance, it is much easier to understand and the code is cleaner. Recursion has a disadvantage though, for large data sets. Each loop is put on a call stack until it reaches a Base case. Once the Base case is met, the call stack is rewound back to reach where it started, executing each of the previous levels on the way. The examples above showed a sum function and a factorial function. In large data sets, this can lead to a large call stack which in turns take a lot of memory. ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:5:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"References: Grokking Algorithms Data Structures and Algorithms in Python ","date":"27-06-2017","objectID":"/recursion-algorithm-study/:6:0","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study","uri":"/recursion-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Selection Sort is a sorting algorithm used to sort a data set either in incremental or decremental order. It goes through the entire elements one by one and hence it’s not a very efficient algorithm to work on large data sets. ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:0:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"1. How does Selection sort work? Selection sort starts with an unsorted data set. With each iteration, it builds up a sub dataset with the sorted data. By the end of the sorting process, the sub dataset contains the entire elements in a sorted order. Iterate through the data set one element at a time. Find the biggest element in the data set (Append it to another if needed) Reduce the sample space by the biggest element just found. The new data set becomes n - 1 compared to the previous iteration. Start over the iteration again, on the reduced sample space. Continue till we have a sorted data set, either incremental or decremental ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:1:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"2. How does the data sample change in each iteration? Consider the data set [10, 4, 9, 3, 6, 19, 8] Data set - [10, 4, 9, 3, 6, 19, 8] After Iteration 1 - [10, 4, 9, 3, 6, 8] - [19] After Iteration 2 - [4, 9, 3, 6, 8] - [10, 19] After Iteration 3 - [4, 3, 6, 8] - [9, 10, 19] After Iteration 4 - [4, 3, 6] - [8, 9, 10, 19] After Iteration 5 - [4, 3] - [6, 8, 9, 10, 19] After Iteration 6 - [3] - [4, 6, 8, 9, 10, 19] After Iteration 7 - [3, 4, 6, 8, 9, 10, 19] Let’s check what the Selection Sort algorithm has to go through in each iteration. Iter 1 - [10, 4, 9, 3, 6, 8] Iter 2 - [4, 9, 3, 6, 8] Iter 3 - [4, 3, 6, 8] Iter 4 - [4, 3, 6] Iter 5 - [4, 3] Iter 6 - [3] Sorted Dataset - [3, 4, 6, 8, 9, 10, 19] ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:2:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"3. Performance / Time Complexity Selection Sort has to go through all the elements in the data set, no matter what. Hence, the Worst case, Best case and Average Time Complexity would be O(n^2). Since Selection Sort takes in n elements while starting, and goes through the data set n times (each step reducing the data set size by 1 member), the iterations would be: n + [ (n - 1) + (n - 2) + (n - 3) + (n - 4) + ... + 2 + 1 ] We are more interested in the worse-case scenario. In a very large data set, an n - 1, n - 2 etc.. won’t make a difference. Hence we can re-write the above iterations as: n + [n + n + n + n ..... n] Or also as: n * n = (n^2) ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:3:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"4. Code def find_smallest(my_list): smallest = my_list[0] smallest_index = 0 for i in range(1, len(my_list)): if my_list[i] \u003c smallest: smallest = my_list[i] smallest_index = i return smallest_index def selection_sort(my_list): new_list = [] for i in range(len(my_list)): smallest = find_smallest(my_list) new_list.append(my_list.pop(smallest)) return new_list[code] ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:4:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"5. Observations Selection Sort is an algorithm to sort a data set, but it is not particularly fast. It takes n iterations in each step to find the biggest element in that iteration. The next iteration has to run on a data set of n - 1 elements compared to the previous iteration. For n elements in a sample space, Selection Sort takes n x n iterations to sort the data set. ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:5:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"6. References https://en.wikipedia.org/wiki/Selection_sort http://bigocheatsheet.com https://github.com/egonschiele/grokking_algorithms ","date":"11-02-2017","objectID":"/selection-sort-algorithm-study/:6:0","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study","uri":"/selection-sort-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Introduction **B**inary Search is a search method used to find an object in a data set. This is much faster compared to the Linear Search algorithm we saw in a previous post. This algorithm works on the Divide and Conquer principle. Binary Search gets its speed by essentially dividing the list/array in half in each iteration, thus reducing the dataset size for the next iteration. Imagine searching for an element in a rather large dataset. Searching for an element one by one using Linear Search would take n iterations. In a worst case scenario, if the element being searched is not present in the dataset or is at the end of the dataset, the time taken to find the object/element would be proportional to the size of the dataset. The element of interest is returned if it is present in the dataset, else a NULL/None value is. Note: Binary search will only work effectively on a Sorted collection. The code implementation will need minor changes depending on how the dataset is sorted, ie.. either in an increasing order or in a decreasing order. ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:1:0","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Performance ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:2:0","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"1. Worst-case performance: log(n) As discussed in the post on, Linear Search a worst-case analysis is done with the upper bound of the running time of the algorithm. ie.. the case when the maximum number of operations are needed/executed to find/search the element of interest in the dataset. Of course, the worst-case scenario for any search algorithms is when the element of interest is not present in the dataset. The maximum number of searches has to be done in such a case, and it still ends up with no fruitful result. A similar but less worse case is when the element is found in the final (or somewhere near the last) iteration. Due to the divide-and-conquer method, the maximum number of iterations needed for a dataset of n elements is, log(n) where the log base is 2. Hence, for a data set of 10240 elements, Binary Search takes a maximum of 13 iterations. [code language=“python”] In [1]: import math In [2]: math.log(10240, 2) Out[2]: 13.321928094887364 [/code] For a data set of 50,000 elements, Binary Search takes 16 iterations in the worst case scenario while a Linear Search may take 50,000 iterations in a similar case. [code language=“python”] In [1]: import math In [2]: math.log(50000, 2) Out[2]: 15.609640474436812 [/code] ie.. the Worst case for Binary search takes log(n) iterations to find the element. ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:2:1","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"2. Best-case performance: O(1) The best case scenario is when the element of interest is found in the first iteration itself. Hence the best-case would be where the search finishes in one iteration. ie.. The best-case scenario would be O(1). ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:2:2","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"How does Binary Search work? Imagine a sorted dataset of 100 numbers and we’re searching for 98 is in the list. A simple search would start from index 0 , moves to the element at index 1, progresses element by element until the one in interest is found. Since we’re searching for 98, it’ll take n iterations depending on the number of elements between the first element in the dataset and 98. Binary Search uses the following method, provided the dataset is sorted. Find the length of the data set. Find the lowest (index 0), highest (index n), and the middle index of the dataset. Find the subsequent elements residing in the first, last, and middle index. Check if the element of interest is the middle element. If not, check if the element-of-interest is higher or lower than the middle element. If it is higher, assuming the dataset is sorted in an increasing order, move the lower index to one above the middle index. if it is lower, move the highest index to one below the middle index. Check if the element of interest is the middle element in the new/shorter dataset. Continue till the element of interest is found. [caption id=“attachment_2310” align=“alignnone” width=“1280”] Binary Search - Source: Wikipedia[/caption] The figure above shows how Binary Search works on a dataset of 16 elements, to find the element 7. Index 0 , Index 16, and the middle index are noted. Subsequent values/elements at these indices are found out as well. Check if the element of interest 7 is equal to, lower, or higher than the middle element 14 at index 8. Since it’s lower and the dataset is sorted in an increasing order, the search moves to the left of the middle index, ie.. from index 0 to index 7. -— The lower index is now 0, the higher index is now 7, and the middle index is now 3, the element in the middle index is 6. Check if the element of interest 7 is lower or higher than the middle element 6 at index 3. Since it’s higher and the dataset is sorted in an increasing order, the search moves to the right of the middle index, ie.. from index 4 to index 7. -— So on and so forth.. till we arrive at the element of interest, ie.. 7. As noted earlier, the data set is divided into half in each iteration. A numeric representation on how Binary search progress can be seen as: 100 elements -\u003e 50 elements -\u003e 25 elements -\u003e 12 elements -\u003e 6 elements - 3 elements -\u003e 1 element ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:3:0","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Code ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:0","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Example 1 : (Data set sorted in Increasing order) [code language=“python”] def binary_search(my_list, item): low_position = 0 high_position = len(my_list) - 1 while low_position = high_position: mid_position = (low_position + high_position) // 2 mid_element = my_list[mid_position] if mid_element == item: print(\"\\nYour search item {0} is at index {1}\".format( item, mid_position)) return mid_element elif mid_element \u003c= item: high_position = mid_position - 1 else: low_position = mid_position + 1 return None if __name__ == “__main__”: my_list = [1, 2, 3, 4, 5, 6] binary_search(my_list, 3) [/code] ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:1","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Example 2 : (Same as above, with statements on how the search progresses) [code language=“python”] def binary_search(my_list, item): # Find and set the low and high positions of the data set # Note that these are not the values, but just positions. low_position = 0 high_position = len(my_list) - 1 # Calculate the Complexity import math complexity = math.ceil(math.log(len(my_list), 2)) # Print some info on the dataset print(\"\\nDataset size : {0} elements\".format(len(my_list))) print(“Element of interest : {0}\".format(item)) print(“Maximum number of iterations to find {0} : {1}\\n”.format( item, complexity)) while low_position \u003c= high_position: # Find the middle position from the low and high positions mid_position = (low_position + high_position) // 2 # Find the element residing in the middle position of the data set. mid_element = my_list[mid_position] print(“Element at min index : {0}\".format(my_list[low_position])) print(“Element at max index : {1}\".format(high_position, my_list[high_position])) print(“Element at mid index {0} : {1}\".format(mid_position, mid_element)) if mid_element == item: print(”\\nYour search item {0} is at index {1}\".format( item, mid_position)) return mid_element elif mid_element \u003e item: high_position = mid_position - 1 print(\"{0} in the left subset, omitting the right subset\\n”.format(item)) else: low_position = mid_position + 1 print(\"{0} in the right subset, omitting the left subset\\n”.format(item)) print(“Element of interest not in dataset\\n”) return None if __name__ == “__main__”: my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] binary_search(my_list, 13) [/code] ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:2","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Observations: Binary Search needs a Sorted dataset to work, either increasing or decreasing. It finds the element of interest in logarithmic time, hence is also known as, Logarithmic Search. Binary Search searches through a dataset of n elements in log(n) iterations, in the worst case scenario. ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:3","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"NOTE: All the examples used in this blog are available at https://github.com/arvimal/DataStructures-and-Algorithms-in-Python, with more detailed notes. ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:4","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"References: https://en.wikipedia.org/wiki/Binary_search_algorithm http://quiz.geeksforgeeks.org/binary-search/ https://www.hackerearth.com/practice/algorithms/searching/binary-search/tutorial/ http://research.cs.queensu.ca/home/cisc121/2006s/webnotes/search.html ","date":"16-01-2017","objectID":"/binary-search-algorithm-study/:4:5","tags":null,"title":"Binary Search - Algorithm Study","uri":"/binary-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"autoauto- [Introduction](#introduction)auto- [Performance](#performance)auto - [1\\. Worst-case performance: O(n)](#1\\-worst-case-performance-on)auto - [2\\. Best-case performance: O(1)](#2\\-best-case-performance-o1)auto - [3\\. Average performance: O(n/2)](#3\\-average-performance-on2)auto - [Observations:](#observations)auto- [How does Linear Search work?](#how-does-linear-search-work)auto- [Code](#code)auto - [Reference:](#reference)autoauto ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:0:0","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Introduction _L_inear Search is an way to search a data set for an element of interest. It is one of the many search algorithms available and is also the most direct and simple of the lot. Linear search looks for the element of interest in a dataset starting from the first element and moves on to the consecutive elements till it finds the one we’re interested in. Due to this behaviour, it’s not the fastest search algorithm around. In the worst case, when the element of interest is the last (or near-last) in the data set, linear-search has to sift through till the end. Hence, in a worst-case scenario, the larger the data set is, the more the iterations it take to find the element of interest. Hence, the performance of Linear search takes a toll as the data set grows. Linear search works on sorted and unsorted data sets equally, since it has to go through the elements one by one and so doesn’t mind if the data is ordered or not. ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:1:0","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Performance ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:2:0","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"1. Worst-case performance: O(n) A worst-case analysis is done with the upper bound of the running time of the algorithm. ie.. the case when the maximum number of operations are executed. The worst-case scenario for a linear search happens when the element-of-interest is not present in the dataset. A near worst-case scenario is when the element-of-interest is the last element of the dataset. In the first case, the search has to go through each element only to find that the element is not present in the dataset. In the second scenario, the search has to be done till the last element, which still takes n iterations. In the worst-case, the performance is O(n), where n is the number of elements in the dataset. ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:2:1","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"2. Best-case performance: O(1) In the best-case, where the element-of-interest is the first element in the dataset, only one search/lookup is needed. Hence the performance is denoted as O(1), for n elements. ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:2:2","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"3. Average performance: O(n/2) On an average, the performance can be denoted as O(n/2). ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:2:3","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Observations: Linear Search iterates through every element in the dataset until it finds the match. In Linear Search, the number of iterations grows linearly if the data set grows in size. This algorithm is called Linear Search due to this linear increase in the complexity depending on the dataset. The best case scenario is when the first iteration finds the element. The Worst case is when the element of interest is not present in the dataset. A very near worse case is when the element of interest is the last one in the dataset. ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:2:4","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"How does Linear Search work? Linear search progresses as following: 1. Takes in a dataset as well as an element of interest. 2. Checks if the first element is the element of interest. 3. If yes, returns the element. 4. If not, move to the next element in the dataset. 5. Iterate till the dataset is exhausted. 6. Return None if the element of interest is not present in the dataset. ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:3:0","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Code [code language=“python”] def linear_search(my_list, item): “““Linear search””” low_position = 0 high_position = len(my_list) - 1 while low_position \u003c high_position: if my_list[low_position] == item: print(“Your search item {0} is at position {1}\".format( item, low_position)) return low_position else: low_position += 1 if __name__ == “__main__”: my_list = [1, 2, 3, 4, 5, 6] linear_search(my_list, 5) [/code] ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:4:0","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["data-structures-and-algorithms","programming","python"],"content":"Reference: http://quiz.geeksforgeeks.org/linear-search/ http://research.cs.queensu.ca/home/cisc121/2006s/webnotes/search.html ","date":"15-01-2017","objectID":"/linear-search-algorithm-study/:4:1","tags":null,"title":"Linear Search - Algorithm Study","uri":"/linear-search-algorithm-study/"},{"categories":["programming","python"],"content":"A method defined within a class can either be an Accessor or a Mutator method. An Accessor method returns the information about the object, but do not change the state or the object. A Mutator method, also called an Update method, can change the state of the object. Consider the following example: [code language=“python”] In [10]: a = [1,2,3,4,5] In [11]: a.count(1) Out[11]: 1 In [12]: a.index(2) Out[12]: 1 In [13]: a Out[13]: [1, 2, 3, 4, 5] In [14]: a.append(6) In [15]: a Out[15]: [1, 2, 3, 4, 5, 6] [/code] The methods a.count() and a.index() are both Accessor methods since it doesn’t alter the object a in any sense, but only pulls the relevant information. But a.append() is a mutator method, since it effectively changes the object (list a) to a new one. In short, knowing the behavior of a method is helpful to understand how it alters the objects it acts upon. ","date":"18-12-2016","objectID":"/accessor-and-mutator-methods/:0:0","tags":["object-oriented-programming","python"],"title":"Accessor and Mutator methods - Python","uri":"/accessor-and-mutator-methods/"},{"categories":["programming","python"],"content":"_E_verything in Python is an object, what does that mean? This post tries to discuss some very basic concepts. What does the following assignment do? [code language=“python”] a = 1 [/code] Of course, anyone dabbled in code knows this. The statement above creates a container `a` and stores the value `1` in it. But it seem that’s not exactly what’s happening, at least from Python’s view-point. When a = 1 is entered or executed by the python interpreter, the following happens in the backend, seemingly unknown to the user. The Python interpreter evaluates the literal 1 and tries to understand what data type can be assigned for it. There are several in-built data types such as str, float, bool, list, dict, set etc.. Builtin types are classes implemented in the python core. For a full list of types and explanation, read the python help at python-\u003e help()-\u003e topics -\u003e TYPES Read the help sections for builtin types, eg.. help(int), help(list) etc.. The interpreter finds the appropriate builtin type for the literal. Since the literal 1 fits the type int, the interpreter creates an instance from class int() in memory. This instance is called an object since it’s just a blob with some metadata. This object has a memory address, a value, a name in one or more namespace, some metadata etc.. type(a) helps in understanding the instance type. In short, an assignment statement simply creates an instance in memory from a pre-defined class. The interpreter reads the LHS (Left hand side) of the statement a = 1, and creates the name a in the current namespace. The name in the namespace is a reference to the object in memory. Through this reference, we can access the data portion as well as the attributes of that object. A single object can have multiple names (references). The name a created in the current namespace is linked to the corresponding object in memory. When a name that’s already defined is entered at the python prompt, the interpreter reads the namespace, finds the name (reference), goes to the memory location it’s referring to, and pull the value of the object, and prints it on-screen. Every object has the following features: A single value, available in its data section. [code language=“python”] In [1]: a = 1 In [2]: a Out[2]: 1 [/code] A single type, since the object is an instance of a pre-defined type class such as int , float etc.. [code language=“python”] In [3]: type(a) Out[3]: int [/code] Attributes either inherited from the parent type class or defined by the user. [code language=“python”] In [10]: dir(a) Out[10]: [’__abs__’, ‘__add__’, ‘__and__’, ‘__bool__’, ‘__ceil__’, ‘__class__’, ‘__delattr__’, ‘__dir__’, ‘__divmod__’, ‘__doc__’, ‘__eq__’, ‘__float__’, …[content omitted] ‘__setattr__’, ‘__sizeof__’, ‘__str__’, ‘__sub__’, ‘__subclasshook__’, ‘__truediv__’, ‘__trunc__’, ‘__xor__’, ‘bit_length’, ‘conjugate’, ‘denominator’, ‘from_bytes’, ‘imag’, ’numerator’, ‘real’, ’to_bytes’] [/code] One or more base classes. All new-stlye classes in Python ultimately inherits from the object class. [code language=“python”] In [4]: type(a) Out[4]: int In [5]: int.mro() Out[5]: [int, object] [/code] NOTE: a is an instance of the int class, and int inturn inherits from the object class. Read more on Method Resolution Order. A unique ID representing the object. [code language=“python”] In [6]: id(a) Out[6]: 140090033476640 [/code] Zero, One, or more names. Use dir() to check the current namespace. Use dir(\u003cobject-name\u003e) to refer the indirect namespace. Several other builtins are available in the default namespace without defining them specifically, possible due to the inclusion of the builtin module available under the reference __builtin__ in the current namespace. For a full list of the pre-defined variables, refer dir(__builtins__), help(__builtin__) or help(builtins) after an import builtins. A few questions and observations: Q1. How can an assignment have zero names in the namespace? Ans: An assignment such as a = 1 creates an object in me","date":"20-10-2016","objectID":"/python-objects/:0:0","tags":["namespace","objects","programming","python-namespace","python-objects"],"title":"Python, Objects, and some more..","uri":"/python-objects/"},{"categories":["techno"],"content":"A recent discussion at work brought up the question “What can be the length of a file name in EXT4”. Or in other words, what would be the maximum character length of the name for a file in EXT4? Wikipedia states that it’s 255 Bytes, but how does that come to be? Is it 255 Bytes or 255 characters? In the kernel source for the 2.6 kernel series (the question was for a RHEL6/EXT4 combination), in fs/ext4/ext4.h, we’d be able to see the following: [code language=“c”] #define EXT4_NAME_LEN 255 struct ext4_dir_entry { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __le16 name_len; /* Name length */ char name[EXT4_NAME_LEN]; /* File name */ }; /* * The new version of the directory entry. Since EXT4 structures are * stored in intel byte order, and the name_len field could never be * bigger than 255 chars, it’s safe to reclaim the extra byte for the * file_type field. */ struct ext4_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[EXT4_NAME_LEN]; /* File name */ }; [/code] This shows that there are two versions of the directory entry structure, ie.. ext4_dir_entry and ext4_dir_entry_2 A directory entry structure carries the file/folder name and the corresponding inode number under every directory. Both structs use an element named name_len to denote the length of the file/folder name. If the EXT filesystem feature filetype is not set, the directory entry structure falls to the first method ext4_dir_entry, else it’s the second, ie.. ext4_dir_entry_2. By default, the file system feature filetype is set, hence the directory entry structure is ext4_dir_entry_2 . As seen above, in this case, the name_len field is set to 8 bits. __u8 represents an unsigned 8-bit integer in C, and can store values from 0 to 255. ie.. 2^8 = 255 (0 t0 255 == 256) ext4_dir_entry has a name_len of __le16, but it seems that the file-name length can only go to a max of 256. ","date":"21-07-2016","objectID":"/max-file-name-in-ext4/:0:0","tags":["dentry","directory-entry-structure","ext4","ext4_dir_entry","ext4_dir_entry_2","file-systems","filesystem","name_len"],"title":"Max file-name length in an EXT4 file system.","uri":"/max-file-name-in-ext4/"},{"categories":["techno"],"content":"Observations: The maximum name length is 255 characters on Linux machines. The actual name length of a file/folder is stored in name_len in each directory entry, under its parent folder. So if the file name length is 5 characters, 5 would be the value set for name_len for that particular file. ie.. the actual length. A character will consume a byte of storage, so the number of characters in a file name will map to the respective number bytes. If so, a file with a name_len of 5 will be using 5 bytes of memory to store the name. Hence, name_len denotes the number of characters that a file can have. Since U8 is 8-bits, name_len can store a file name with upto 255 chars. Now the actual memory being consumed for storing these characters is not denoted by name_len. Since the size of a character translates to a byte, the maximum size wrt memory that a file name can have is 255 Bytes. ","date":"21-07-2016","objectID":"/max-file-name-in-ext4/:0:1","tags":["dentry","directory-entry-structure","ext4","ext4_dir_entry","ext4_dir_entry_2","file-systems","filesystem","name_len"],"title":"Max file-name length in an EXT4 file system.","uri":"/max-file-name-in-ext4/"},{"categories":["techno"],"content":"NOTE: The initial dir entry structure ext4_dir_entry had __le16 for name_len, it was later re-sized to __u8 in ext4_dir_entry_2 , by culling 8 bits from the existing 16 bits of name_len. The remaining free space culled from name_len was assigned to store the file type, in ext4_dir_entry_2. It was named file_type with size __u8. file_type helps to identity the file types such as regular files, sockets, character devices, block devices etc.. ","date":"21-07-2016","objectID":"/max-file-name-in-ext4/:0:2","tags":["dentry","directory-entry-structure","ext4","ext4_dir_entry","ext4_dir_entry_2","file-systems","filesystem","name_len"],"title":"Max file-name length in an EXT4 file system.","uri":"/max-file-name-in-ext4/"},{"categories":["techno"],"content":"References: RHEL6 kernel-2.6.32-573.el6 EXT4 header file (ext4.h) EXT4 Wiki - Disk layout http://unix.stackexchange.com/questions/32795/what-is-the-maximum-allowed-filename-and-folder-size-with-ecryptfs ","date":"21-07-2016","objectID":"/max-file-name-in-ext4/:0:3","tags":["dentry","directory-entry-structure","ext4","ext4_dir_entry","ext4_dir_entry_2","file-systems","filesystem","name_len"],"title":"Max file-name length in an EXT4 file system.","uri":"/max-file-name-in-ext4/"},{"categories":["programming","python"],"content":"_s_uper() is a feature through which inherited methods can be accessed, which has been overridden in a class. It can also help with the MRO lookup order in case of multiple inheritance. This may not be obvious first, but a few examples should help to drive the point home. Inheritance and method overloading was discussed in a previous post, where we saw how inherited methods can be overloaded or enhanced in the child classes. In many scenarios, it’s needed to overload an inherited method, but also call the actual method defined in the Parent class. Let’s start off with a simple example based on Inheritance, and build from there. Example 0: [code language=“python”] class MyClass(object): def func(self): print(“I’m being called from the Parent class!”) class ChildClass(MyClass): pass my_instance_1 = ChildClass() my_instance_1.func() [/code] This outputs: [code language=“python”] In [18]: %run /tmp/super-1.py I’m being called from the Parent class [/code] In Example 0, we have two classes, MyClass and ChildClass. The latter inherits from the former, and the parent class MyClass has a method named func defined. Since ChildClass inherits from MyClass, the child class has access to the methods defined in the parent class. An instance is created my_instance_2, for ChildClass. Calling my_instance_1.func() will print the statement from the Parent class, due to the inheritance. Building up on the first example: Example 1: [code language=“python”] class MyClass(object): def func(self): print(“I’m being called from the Parent class”) class ChildClass(MyClass): def func(self): print(“I’m being called from the Child class”) my_instance_1 = MyClass() my_instance_2 = ChildClass() my_instance_1.func() my_instance_2.func() [/code] This outputs: [code language=“python”] In [19]: %run /tmp/super-1.py I’m being called from the Parent class I’m being called from the Child class [/code] This example has a slight difference, both the child class as well as the parent class have the same method defined, ie.. func. In this scenario, the parent class’ method is overridden by the child class method. ie.. if we call the func() method from the instance of ChildClass, it need not go a fetch the method from its Parent class, since it’s already defined locally. NOTE: This is due to the Method Resolution Order, discussed in an earlier post. But what if there is a scenario that warranties the need for specifically calling methods defined in the Parent class, from the instance of a child class? ie.. How to call the methods defined in the Parent class, through the instance of the Child class, even if the Parent class method is overloaded in the Child class? In such a case, the inbuilt function super() can be used. Let’s add to the previous example. Example 2: [code language=“python”] class MyClass(object): def func(self): print(“I’m being called from the Parent class”) class ChildClass(MyClass): def func(self): print(“I’m actually being called from the Child class”) print(“But…”) # Calling the `func()` method from the Parent class. super(ChildClass, self).func() my_instance_2 = ChildClass() my_instance_2.func() [/code] This outputs: [code language=“python”] In [21]: %run /tmp/super-1.py I’m actually being called from the Child class But… I’m being called from the Parent class [/code] ","date":"01-07-2016","objectID":"/inheritance-and-super-oop/:0:0","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming","uri":"/inheritance-and-super-oop/"},{"categories":["programming","python"],"content":"How is the code structured? We have two classes MyClass and ChildClass. The latter is inheriting from the former. Both classes have a method named func The child class ChildClass is instantiated as my_instance_2 The func method is called from the instance. ","date":"01-07-2016","objectID":"/inheritance-and-super-oop/:0:1","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming","uri":"/inheritance-and-super-oop/"},{"categories":["programming","python"],"content":"How does the code work? When the func method is called, the interpreter searches it using the Method Resolution Order, and find the method defined in the class ChildClass. Since it finds the method in the child class, it executes it, and prints the string “I’m actually being called from the Child class”, as well “But…” The next statement is super which calls the method func defined in the parent class of ChildClass Since the control is now passed onto the func method in the Parent class via super, the corresponding print() statement is printed to stdout. Example 2 can also be re-written as : [code language=“python”] class MyClass(object): def func(self): print(“I’m being called from the Parent class”) class ChildClass(MyClass): def func(self): print(“I’m actually being called from the Child class”) print(“But…”) # Calling the `func()` method from the Parent class. # super(ChildClass, self).func() MyClass.func(self) # Call the method directly via Parent class my_instance_2 = ChildClass() my_instance_2.func() [/code] NOTE: The example above uses the Parent class directly to access it’s method. Even though it works, it is not the best way to do it since the code is tied to the Parent class name. If the Parent class name changes, the child/sub class code has to be changed as well. Let’s see another example for super() . This is from our previous article on Inheritance and method overloading. Example 3: [code language=“python”] import abc class MyClass(object): __metaclass__ = abc.ABCMeta def my_set_val(self, value): self.value = value def my_get_val(self): return self.value @abc.abstractmethod def print_doc(self): return class MyChildClass(MyClass): def my_set_val(self, value): if not isinstance(value, int): value = 0 super(MyChildClass, self).my_set_val(self) def print_doc(self): print(“Documentation for MyChild Class”) my_instance = MyChildClass() my_instance.my_set_val(100) print(my_instance.my_get_val()) print(my_instance.print_doc()) [/code] The code is already discussed here. The my_set_val method is defined in both the child class as well as the parent class. We overload the my_set_val method defined in the parent class, in the child class. But after enhancing/overloading it, we call the my_set_val method specifically from the Parent class using super() and thus enhance it. ","date":"01-07-2016","objectID":"/inheritance-and-super-oop/:0:2","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming","uri":"/inheritance-and-super-oop/"},{"categories":["programming","python"],"content":"Takeaway: super() helps to specifically call the Parent class method which has been overridden in the child class, from the child class. The super() in-built function can be used to call/refer the Parent class without explicitly naming them. This helps in situations where the Parent class name may change. Hence, super() helps in avoiding strong ties with class names and increases maintainability. super() helps the most when there are multiple inheritance happening, and the MRO ends up being complex. In case you need to call a method from a specific parent class, use super(). There are multiple ways to call a method from a Parent class. . super(, self). super(). ","date":"01-07-2016","objectID":"/inheritance-and-super-oop/:0:3","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming","uri":"/inheritance-and-super-oop/"},{"categories":["programming","python"],"content":"References: https://docs.python.org/2/library/functions.html#super https://rhettinger.wordpress.com/2011/05/26/super-considered-super/ https://stackoverflow.com/questions/222877/how-to-use-super-in-python ","date":"01-07-2016","objectID":"/inheritance-and-super-oop/:0:4","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming","uri":"/inheritance-and-super-oop/"},{"categories":null,"content":"_E_xt3 and Ext4 recently have been the most commonly used file system on Linux machines. What does uninit_bg actually do? Read https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout section `Meta Block Groups and Lazy Block Group Initialization`. https://ext4.wiki.kernel.org/index.php/Frequently_Asked_Questions https://www.thomas-krenn.com/en/wiki/Ext4_Filesystem https://access.redhat.com/labs/psb/versions/kernel-3.10.0-327.18.2.el7/Documentation/filesystems/ext4.txt https://access.redhat.com/labs/psb/versions/kernel-3.10.0-327.18.2.el7/fs/ext4/ext4.h ","date":"30-06-2016","objectID":"/uninit-bg-and-lazy-block-group-allocation/:0:0","tags":["ext3","ext4","file-systems","lazy-block-group-allocation","uninit_bg"],"title":"`uninit_bg` and lazy block group allocation in EXT3/4","uri":"/uninit-bg-and-lazy-block-group-allocation/"},{"categories":["ceph"],"content":"_S_harding is the process of breaking down data onto multiple locations so as to increase parallelism, as well as distribute load. This is a common feature used in databases. Read more on this at Wikipedia. The concept of sharding is used in Ceph, for splitting the bucket index in a RADOS Gateway. RGW or RADOS Gateway keeps an index for all the objects in its buckets for faster and easier lookup. For each RGW bucket created in a pool, the corresponding index is created in the XX.index pool. For example, for each of the buckets created in .rgw pool, the bucket index is created in .rgw.buckets.index pool. For each bucket, the index is stored in a single RADOS object. When the number of objects increases, the size of the RADOS object increases as well. Two problems arise due to the increased index size. RADOS does not work good with large objects since it’s not designed as such. Operations such as recovery, scrubbing etc.. work on a single object. If the object size increases, OSDs may start hitting timeouts because reading a large object may take a long time. This is one of the reason that all RADOS client interfaces such as RBD, RGW, CephFS use a standard 4MB object size. Since the index is stored in a single RADOS object, only a single operation can be done on it at any given time. When the number of objects increases, the index stored in the RADOS object grows. Since a single index is handling a large number of objects, and there is a chance the number of operations also increase, parallelism is not possible which can end up being a bottleneck. Multiple operations will need to wait in a queue since a single operation is possible at a time. In order to work around these problems, the bucket index is sharded into multiple parts. Each shard is kept on a separate RADOS object within the index pool. Sharding is configured with the tunable bucket_index_max_shards . By default, this tunable is set to 0 which means that there are no shards. ","date":"30-06-2016","objectID":"/sharding-the-ceph-rgw-bucket-index/:0:0","tags":["ceph","rados","rados-gateway","rgw","rgw-index","sharding"],"title":"Sharding the Ceph RADOS Gateway bucket index","uri":"/sharding-the-ceph-rgw-bucket-index/"},{"categories":["ceph"],"content":"How to check if Sharding is set? List the buckets [code language=“bash”] # radosgw-admin metadata bucket list [ “my-new-bucket” ] [/code] Get information on the bucket in question[code language=“bash”] # radosgw-admin metadata get bucket:my-new-bucket { “key”: “bucket:my-new-bucket”, “ver”: { “tag”: “_bGZAVUgayKVwGNgNvI0328G”, “ver”: 1 }, “mtime”: 1458940225, “data”: { “bucket”: { “name”: “my-new-bucket”, “pool”: “.rgw.buckets”, “data_extra_pool”: “.rgw.buckets.extra”, “index_pool”: “.rgw.buckets.index”, “marker”: “default.2670570.1”, “bucket_id”: “default.2670570.1” }, “owner”: “rgw_user”, “creation_time”: 1458940225, “linked”: “true”, “has_bucket_info”: “false” } } [/code] Use the bucket ID to get more information, including the number of shards. [code language=“bash”] radosgw-admin metadata get bucket.instance:my-new-bucket:default.2670570.1 { “key”: “bucket.instance:my-new-bucket:default.2670570.1”, “ver”: { “tag”: “_xILkVKbfQD7reDFSOB4a5VU”, “ver”: 1 }, “mtime”: 1458940225, “data”: { “bucket_info”: { “bucket”: { “name”: “my-new-bucket”, “pool”: “.rgw.buckets”, “data_extra_pool”: “.rgw.buckets.extra”, “index_pool”: “.rgw.buckets.index”, “marker”: “default.2670570.1”, “bucket_id”: “default.2670570.1” }, “creation_time”: 1458940225, “owner”: “rgw_user”, “flags”: 0, “region”: “default”, “placement_rule”: “default-placement”, “has_instance_obj”: “true”, “quota”: { “enabled”: false, “max_size_kb”: -1, “max_objects”: -1 }, “num_shards”: 0, “bi_shard_hash_type”: 0 }, “attrs”: [ { “key”: “user.rgw.acl”, “val”: “AgKPAAAAAgIaAAAACAAAAHJnd191c2VyCgAAAEZpcnN0IFVzZXIDA2kAAAABAQAAAAgAAAByZ3dfdXNlcg8AAAABAAAACAAAAHJnd191c2VyAwM6AAAAAgIEAAAAAAAAAAgAAAByZ3dfdXNlcgAAAAAAAAAAAgIEAAAADwAAAAoAAABGaXJzdCBVc2VyAAAAAAAAAAA=” }, { “key”: “user.rgw.idtag”, “val”: \"\" }, { “key”: “user.rgw.manifest”, “val”: \"\" } ] } } [/code] Note that `num_shards` is set to 0, which means that sharding is not enabled. ","date":"30-06-2016","objectID":"/sharding-the-ceph-rgw-bucket-index/:0:1","tags":["ceph","rados","rados-gateway","rgw","rgw-index","sharding"],"title":"Sharding the Ceph RADOS Gateway bucket index","uri":"/sharding-the-ceph-rgw-bucket-index/"},{"categories":["ceph"],"content":"How to configure Sharding? To configure sharding, we need to first dump the region info. NOTE: By default, RGW has a region named default even if regions are not configured. [code language=“bash”] # radosgw-admin region get \u003e /tmp/region.txt # cat /tmp/region.txt { “name”: “default”, “api_name”: “”, “is_master”: “true”, “endpoints”: [], “hostnames”: [], “master_zone”: “”, “zones”: [ { “name”: “default”, “endpoints”: [], “log_meta”: “false”, “log_data”: “false”, “bucket_index_max_shards”: 0 } ], “placement_targets”: [ { “name”: “default-placement”, “tags”: [] } ], “default_placement”: “default-placement” } [/code] Edit the file /tmp/region.txt, change the value for `bucket_index_max_shards` to the needed shard value (we’re setting it to 8 in this example), and inject it back to the region. [code language=“bash”] # radosgw-admin region set \u003c /tmp/region.txt { “name”: “default”, “api_name”: “”, “is_master”: “true”, “endpoints”: [], “hostnames”: [], “master_zone”: “”, “zones”: [ { “name”: “default”, “endpoints”: [], “log_meta”: “false”, “log_data”: “false”, “bucket_index_max_shards”: 8 } ], “placement_targets”: [ { “name”: “default-placement”, “tags”: [] } ], “default_placement”: “default-placement” } [/code] Reference: Red Hat Ceph Storage 1.3 Rados Gateway documentation https://en.wikipedia.org/wiki/Shard_(database_architecture) ","date":"30-06-2016","objectID":"/sharding-the-ceph-rgw-bucket-index/:0:2","tags":["ceph","rados","rados-gateway","rgw","rgw-index","sharding"],"title":"Sharding the Ceph RADOS Gateway bucket index","uri":"/sharding-the-ceph-rgw-bucket-index/"},{"categories":["programming","python"],"content":"_I_nheritance is a usual theme in Object Oriented Programming. Because of Inheritance, the functions/methods defined in parent classes can be called in Child classes which enables code reuse, and several other features. In this article, we try to understand some of those features that come up with Inheritance. We’ve discussed Abstract Methods in an earlier post, which is a feature part of Inheritance, and can be applied on child classes that inherits from a Parent class. E the methods which are inherited can also be seen as another feature or possibility in Inheritance. In many cases, it’s required to override or specialize the methods inherited from the Parent class. This is of course possible, and is called as ‘Method Overloading’. Consider the two classes and its methods defined below: Example 0: [code language=“python”] import abc class MyClass(object): __metaclass__ = abc.ABCMeta def __init__(self): pass def my_set_method(self, value): self.value = value def my_get_method(self): return self.value @abc.abstractmethod def printdoc(self): return class MyChildClass(MyClass): def my_set_method(self, value): if not isinstance(value, int): value = 0 super(MyChildClass, self).my_set_method(self) def printdoc(self): print(\"\\nDocumentation for MyChildClass()\") instance_1 = MyChildClass() instance_1.my_set_method(10) print(instance_1.my_get_method()) instance_1.printdoc() [/code] We have two classes, the parent class being MyClass and the child class being MyChildClass. MyClass has three methods defined. my_set_method() my_get_method() printdoc() The printdoc() method is an Abstract method, and hence should be implemented in the Child class as a mandatory method. The child class MyChildClass inherits from MyClass and has access to all it’s methods. Normally, we can just go ahead and use the methods defined in MyClass , in MyChildClass. But there can be situations when we want to improve or build upon the methods inherited. As said earlier, this is called Method Overloading. MyChildClass extends the parent’s my_set_method() function by it’s own implementation. In this example, it does an additional check to understand if the input value is an int or not, and then calls the my_set_method() of it’s parent class using super. Hence, this method in the child class extends the functionality prior calling method in the parent. A post on super is set for a later time. Even though this is a trivial example, it helps to understand how the features inherited from other classes can be extended or improved upon via method overloading. The my_get_method() is not overridden in the child class but still called from the instance, as instance_1.my_get_method(). We’re using it as it is available via Inheritance. Since it’s defined in the parent class, it works in the child class’ instance when called, even if not overridden. The printdoc() method is an abstract method and hence is mandatory to be implemented in the child class, and can be overridden with what we choose to do. Inheritance is possible from python builtins, and can be overridden as well. Let’s check out another example: Example 1: [code language=“python”] class MyList(list): def __getitem__(self, index): if index == 0: raise IndexError if index \u003e 0: index -= 1 return list.__getitem__(self, index) def __setitem__(self, index, value): if index == 0: raise IndexError if index \u003e 0: index -= 1 list.__setitem__(self, index, value) x = MyList([‘a’, ‘b’, ‘c’]) print(x) print(\"-\" * 10) x.append(’d’) print(x) print(\"-\" * 10) x.__setitem__(4, ’e’) print(x) print(\"-\" * 10) print(x[1]) print(x.__getitem__(1)) print(\"-\" * 10) print(x[4]) print(x.__getitem__(4)) [/code] This outputs: [code language=“python”] [‘a’, ‘b’, ‘c’] ———- [‘a’, ‘b’, ‘c’, ’d’] ———- [‘a’, ‘b’, ‘c’, ’e’] ———- a a ———- e e [/code] ","date":"29-06-2016","objectID":"/inheritance-and-method-overloading-oop/:0:0","tags":["abstract-methods","abstractmethod","builtins","inheritance","method-overloading"],"title":"Inheritance and Method overloading - Object Oriented Programming","uri":"/inheritance-and-method-overloading-oop/"},{"categories":["programming","python"],"content":"How does the code work? The class MyList() inherits from the builtin list. Because of the inheritance, we can use list’s available magic methods such as __getitem__() , __setitem__() etc.. NOTE: In order to see the available methods in list, use dir(list). We create two functions/methods named `__getitem__()` and `__setitem__()` to override the inherited methods. Within these functions/methods, we set our own conditions. Wie later call the builtin methods directly within these functions, using list.__getitem__() list.__setitem__() We create an instance named x from MyList(). We understand that x[1] and x.__getitem__(1) are same. x[4, 'e'] and x.__setitem__(4, 'e') are same. x.append(f) is same as x.__setitem__(\u003cn\u003e, f) where is the element to the extreme right which the python interpreter iterates and find on its own. Hence, in Inheritance, child classes can: Inherit from parent classes and use those methods. Parent classes can either be user-defined classes or buitins like list , dict etc.. Override (or Overload) an inherited method. Extend an inherited method in its own way. Implement an Abstract method the parent class requires. ","date":"29-06-2016","objectID":"/inheritance-and-method-overloading-oop/:0:1","tags":["abstract-methods","abstractmethod","builtins","inheritance","method-overloading"],"title":"Inheritance and Method overloading - Object Oriented Programming","uri":"/inheritance-and-method-overloading-oop/"},{"categories":["programming","python"],"content":"Reference: Python beyond the basics - Object Oriented Programming ","date":"29-06-2016","objectID":"/inheritance-and-method-overloading-oop/:0:2","tags":["abstract-methods","abstractmethod","builtins","inheritance","method-overloading"],"title":"Inheritance and Method overloading - Object Oriented Programming","uri":"/inheritance-and-method-overloading-oop/"},{"categories":["programming","python"],"content":"_A_bstract classes, in short, are classes that are supposed to be inherited or subclassed, rather than instantiated. Through Abstract Classes, we can enforce a blueprint on the subclasses that inherit the Abstract Class. This means that Abstract classes can be used to define a set of methods that must be implemented by it subclasses. Abstract classes are used when working on large projects where classes have to be inherited, and need to strictly follow certain blueprints. Python supports Abstract Classes via the module abc from version 2.6. Using the abc module, its pretty straight forward to implement an Abstract Class. Example 0: [code language=“python”] import abc class My_ABC_Class(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def set_val(self, val): return @abc.abstractmethod def get_val(self): return # Abstract Base Class defined above ^^^ # Custom class inheriting from the above Abstract Base Class, below class MyClass(My_ABC_Class): def set_val(self, input): self.val = input def get_val(self): print(\"\\nCalling the get_val() method\") print(“I’m part of the Abstract Methods defined in My_ABC_Class()”) return self.val def hello(self): print(\"\\nCalling the hello() method\") print(“I’m *not* part of the Abstract Methods defined in My_ABC_Class()”) my_class = MyClass() my_class.set_val(10) print(my_class.get_val()) my_class.hello() [/code] In the code above, set_val() and get_val() are both abstract methods defined in the Abstract Class My_ABC_Class(). Hence it should be implemented in the child class inheriting from My_ABC_Class(). In the child class MyClass() , we have to strictly define the abstract classes defined in the Parent class. But the child class is free to implement other methods of their own. The hello() method is one such. This will print : [code language=“bash”] # python abstractclasses-1.py Calling the get_val() method I’m part of the Abstract Methods defined in My_ABC_Class() 10 Calling the hello() method I’m *not* part of the Abstract Methods defined in My_ABC_Class() [/code] The code gets executed properly even if the hello() method is not an abstract method. Let’s check what happens if we don’t implement a method marked as an abstract method, in the child class. Example 1: [code language=“python”] import abc class My_ABC_Class(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def set_val(self, val): return @abc.abstractmethod def get_val(self): return # Abstract Base Class defined above ^^^ # Custom class inheriting from the above Abstract Base Class, below class MyClass(My_ABC_Class): def set_val(self, input): self.val = input def hello(self): print(\"\\nCalling the hello() method\") print(“I’m *not* part of the Abstract Methods defined in My_ABC_Class()”) my_class = MyClass() my_class.set_val(10) print(my_class.get_val()) my_class.hello() [/code] Example 1 is the same as Example 0 except we don’t have the get_val() method defined in the child class. This means that we’re breaking the rule of abstraction. Let’s see what happens: [code language=“bash”] # python abstractclasses-2.py Traceback (most recent call last): File “abstractclasses-2.py”, line 50, in my_class = MyClass() TypeError: Can’t instantiate abstract class MyClass with abstract methods get_val [/code] The traceback clearly states that the child class MyClass() cannot be instantiated since it does not implement the Abstract methods defined in it’s Parent class. We mentioned that an Abstract class is supposed to be inherited rather than instantiated. What happens if we try instantiating an Abstract class? Let’s use the same example, this time we’re instantiating the Abstract class though. Example 2: [code language=“python”] import abc class My_ABC_Class(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def set_val(self, val): return @abc.abstractmethod def get_val(self): return # Abstract Base Class defined above ^^^ # Custom class inheriting from the above Abstract Base Class, below class MyClass(My_ABC_Class): def set_v","date":"14-06-2016","objectID":"/abc-oop-python/:0:0","tags":["abstract-base-class","abstract-methods","abstractmethod","object-oriented-programming","programming","python"],"title":"Abstract Base Classes/Methods - Object Oriented Programming","uri":"/abc-oop-python/"},{"categories":["programming","python"],"content":"Takeaway: An Abstract Class is supposed to be inherited, not instantiated. The Abstraction nomenclature is applied on the methods within a Class. The abstraction is enforced on methods which are marked with the decorator @abstractmethod or @abc.abstractmethod, depending on how you imported the module, from abc import abstractmethod or import abc. It is not mandatory to have all the methods defined as abstract methods, in an Abstract Class. Subclasses/Child classes are enforced to define the methods which are marked with @abstractmethod in the Parent class. Subclasses are free to create methods of their own, other than the abstract methods enforced by the Parent class. ","date":"14-06-2016","objectID":"/abc-oop-python/:0:1","tags":["abstract-base-class","abstract-methods","abstractmethod","object-oriented-programming","programming","python"],"title":"Abstract Base Classes/Methods - Object Oriented Programming","uri":"/abc-oop-python/"},{"categories":["programming","python"],"content":"Reference: https://pymotw.com/2/abc/ Python beyond the basics - Object Oriented Programming ","date":"14-06-2016","objectID":"/abc-oop-python/:0:2","tags":["abstract-base-class","abstract-methods","abstractmethod","object-oriented-programming","programming","python"],"title":"Abstract Base Classes/Methods - Object Oriented Programming","uri":"/abc-oop-python/"},{"categories":["programming","python"],"content":"_T_his article was long overdue and should have been published before many of the articles in this blog. Better late than never. self in Python is usually used in an Object Oriented nomenclature, to denote the instance/object created from a Class. In short, self is the instance itself. Let’s check the following example: [code language=“python”] class MyClass(object): def __init__(self, name): self.name = name print(“Initiating the instance!”) def hello(self): print(self.name) myclass = MyClass(“Dan Inosanto”) # Calling the `hello` method via the Instance `myclass` myclass.hello() # Calling the `hello` method vai the class. MyClass.hello(myclass) [/code] The code snippet above is trivial and stupid, but I think it gets the idea across. We have a class named MyClass() which takes a name value as an argument. It also prints the string “Initiating the instance”. The name value is something that has to be passed while creating an instance. The function hello() just prints the name value that is passed while instantiating the class MyClass(). We instantiate the class MyClass() as myclass and pass the string Dan Inosanto as an argument. Read about the great Inosanto here. Next, we call the hello() method through the instance. ie.. [code language=“python”] myclass.hello() [/code] This should print the name we passed while instantiating MyClass() as myclass , which should be pretty obvious. The second and last instruction is doing the same thing, but in a different way. [code language=“python”] MyClass.hello(myclass) [/code] Here, we call the class MyClass() directly as well as it’s method hello(). Let’s check out what both prints: [code language=“bash”] # python /tmp/test.py Initiating the instance! Dan Inosanto Dan Inosanto [/code] As we can see, both prints the same output. This means that : myclass.hello(self) == MyClass.hello(myclass) In general, we can say that: .(self) == .() ie.. The keyword self actually represents the instance being instantiated from the Class. Hence self can be seen as Syntactic sugar. ","date":"12-06-2016","objectID":"/self-in-python/:0:0","tags":null,"title":"`self` in Python - Object Oriented Programming","uri":"/self-in-python/"},{"categories":["programming","python"],"content":"_F_unctions defined under a class are also called methods. Most of the methods are accessed through an instance of the class. There are three types of methods: Instance methods Static methods Class methods Both Static methods and Class methods can be called using the @staticmethod and @classmethod syntactic sugar respectively. ","date":"12-06-2016","objectID":"/instance-class-static-method-oop/:0:0","tags":null,"title":"Instance, Class, and Static methods - Object Oriented Programming","uri":"/instance-class-static-method-oop/"},{"categories":["programming","python"],"content":"Instance methods _I_nstance methods are also called Bound methods since the instance is bound to the class via self. Read a simple explanation on self here. Almost all methods are Instance methods since they are accessed through instances. For example: [code language=“python”] class MyClass(object): def set_val(self, val): self.value = val def get_val(self): print(self.value) return self.value a = MyClass() b = MyClass() a.set_val(10) b.set_val(100) a.get_val() b.get_val() [/code] The above code snippet shows manipulating the two methods set_val() and get_val() . These are done through the instances a and b. Hence these methods are called Instance methods. NOTE: Instance methods have self as their first argument. self is the instance itself. All methods defined under a class are usually called via the instance instantiated from the class. But there are methods which can work without instantiating an instance. Class methods and Static methods don’t require an instance, and hence don’t need self as their first argument. ","date":"12-06-2016","objectID":"/instance-class-static-method-oop/:0:1","tags":null,"title":"Instance, Class, and Static methods - Object Oriented Programming","uri":"/instance-class-static-method-oop/"},{"categories":["programming","python"],"content":"Static methods Static methods are functions/methods which doesn’t need a binding to a class or an instance. Static methods, as well as Class methods, don’t require an instance to be called. Static methods doesn’t need self or cls as the first argument since it’s not bound to an instance or class. Static methods are normal functions, but within a class. Static methods are defined with the keyword @staticmethod above the function/method. Static methods are usually used to work on Class Attributes. ============================= A note on class attributes Attributes set explicitly under a class (not under a function) are called Class Attributes. For example: [code language=“python”] class MyClass(object): value = 10 def my_func(self): pass [/code] In the code snippet above, value = 10 is an attribute defined under the class MyClass() and not under any functions/methods. Hence, it’s called a Class attribute. ============================= Let’s check out an example on static methods and class attributes: [code language=“python”] class MyClass(object): # A class attribute count = 0 def __init__(self, name): print(“An instance is created!”) self.name = name MyClass.count += 1 # Our class method @staticmethod def status(): print(“The total number of instances are “, MyClass.count) print(MyClass.count) my_func_1 = MyClass(“MyClass 1”) my_func_2 = MyClass(“MyClass 2”) my_func_3 = MyClass(“MyClass 3”) MyClass.status() print(MyClass.count) [/code] This prints the following: [code language=“bash”] # python statismethod.py 0 An instance is created! An instance is created! An instance is created! The total number of instances are 3 3 [/code] How does the code work? The example above has a class MyClass() with a class attribute count = 0. An __init__ magic method accepts a name variable. The __init__ method also increments the count in the count counter at each instantiation. We define a staticmethod status() which just prints the number of the instances being created. The work done in this method is not necessarily associated with the class or any functions, hence its defined as a staticmethod. We print the initial value of the counter count via the class, as MyClass.count. This will print 0since the counter is called before any instances are created. We create three instances from the class MyClass We can check the number of instances created through the status() method and the count counter. Another example: [code language=“python”] class Car(object): def sound(): print(“vroom!”) [/code] The code above shows a method which is common to all the Car instances, and is not limited to a specific instance of Car. Hence, this can be called as a staticmethod since it’s not necessarily bound to a Class or Instance to be called. [code language=“python”] class Car(object): @staticmethod def sound(): print(“vroom!”) [/code] ","date":"12-06-2016","objectID":"/instance-class-static-method-oop/:0:2","tags":null,"title":"Instance, Class, and Static methods - Object Oriented Programming","uri":"/instance-class-static-method-oop/"},{"categories":["programming","python"],"content":"Class methods We can define functions/methods specific to classes. These are called Class methods. The speciality of a class methods is that an instance is not required to access a class method. It can be called directly via the Class name. Class methods are used when it’s not necessary to instantiate a class to access a method. NOTE: A method can be set as a Class method using the decorator @classmethod. Example: [code language=“python”] class MyClass(object): value = 10 @classmethod def my_func(cls): print(“Hello”) [/code] NOTE: Class methods have cls as their first argument, instead of self. Example: [code language=“python”] class MyClass(object): count = 0 def __init__(self, val): self.val = val MyClass.count += 1 def set_val(self, newval): self.val = newval def get_val(self): return self.val @classmethod def get_count(cls): return cls.count object_1 = MyClass(10) print(”\\nValue of object : %s” % object_1.get_val()) print(MyClass.get_count()) object_2 = MyClass(20) print(\"\\nValue of object : %s\" % object_2.get_val()) print(MyClass.get_count()) object_3 = MyClass(40) print(\"\\nValue of object : %s\" % object_3.get_val()) print(MyClass.get_count()) [/code] Here, we use a get_count() function to get the number of times the counter was incremented. The counter is incremented each time an instance is created. Since the counter is not really tied with the instance but only counts the number of instance, we set it as a classmethod, and calls it each time using MyClass.get_count()when an instance is created. The output looks as following: [code language=“bash”] # python classmethod.py Value of object : 10 1 Value of object : 20 2 Value of object : 40 3 [/code] Courtsey: This was written as part of studying class and static methods. Several articles/videos have helped including but not limited to the following: https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/ Python beyond the basics - Object Oriented Programming - O’Reilly Learning Paths ","date":"12-06-2016","objectID":"/instance-class-static-method-oop/:0:3","tags":null,"title":"Instance, Class, and Static methods - Object Oriented Programming","uri":"/instance-class-static-method-oop/"},{"categories":["programming","python"],"content":"Magic methods _M_agic methods are special methods which can be defined (or already designed and available) to act on objects. Magic methods start and end with underscores \"__\", and are not implicitly called by the user even though they can be. Most magic methods are used as syntactic sugar by binding it to more clear/easy_to_understand keywords. Python is mostly objects and method calls done on objects. Many available functions in Python are actually tied to magic methods_._ Let’s checkout a few examples. Example 0: [code language=“python”] In [1]: my_var = “Hello!” In [2]: print(my_var) Hello! In [3]: my_var.__repr__() Out[3]: “‘Hello!’” [/code] As we can see, the __repr__() magic method can be called to print the object, ie.. it is bound to the print() keyword. This is true for many other builtin keywords/operators as well. Example 1: [code language=“python”] In [22]: my_var = “Hello, \" In [23]: my_var1 = “How are you?” In [24]: my_var + my_var1 Out[24]: ‘Hello, How are you?’ In [25]: my_var.__add__(my_var1) Out[25]: ‘Hello, How are you?’ [/code] Here, Python interprets the + sign as a mapping to the magic method __add__(), and calls it on the L-value (Left hand object value) my_var, with the R-value (Right hand object value) as the argument. When a builtin function is called on an object, in many cases it is mapped to the magic method. Example 2: [code language=“python”] In [69]: my_list_1 = [‘a’, ‘b’, ‘c’, ’d’] In [70]: ‘a’ in my_list_1 Out[70]: True In [71]: my_list_1.__contains__(“a”) Out[71]: True [/code] The in builtin is mapped to the __contains__()method. The methods available for an object should mostly be dependent on the type of the object. Example 3: [code language=“python” wraplines=“true”] In [59]: my_num = 1 In [60]: type(my_num) Out[60]: int In [61]: my_num.__doc__ Out[61]: Out[61]: “int(x=0) -\u003e int or long\\nint(x, base=10) -\u003e int or long\\n\\nConvert a number or string to an integer, or return 0 if no arguments\\nare given. ….»\u003e In [62]: help(my_num) class int(object) | int(x=0) -\u003e int or long | int(x, base=10) -\u003e int or long | | Convert a number or string to an integer, or return 0 if no arguments | are given. If x is floating point, the conversion truncates towards zero. | If x is outside the integer range, the function returns a long instead. [/code] From the tests above, we can understand that the help() function is actually mapped to the object.__doc__ magic method. It’s the same doc string that __doc__ and help() uses. NOTE: Due to the syntax conversion (+ to __add__(),and other conversions), operators like + , in, etc.. are also called Syntactic sugar. ","date":"02-06-2016","objectID":"/magic-methods-in-python/:0:1","tags":["programming","python"],"title":"Magic methods and Syntactic sugar in Python","uri":"/magic-methods-in-python/"},{"categories":["programming","python"],"content":"What is Syntactic sugar? _A_ccording to Wikipedia, Syntact sugar is: In computer science, syntactic sugar is syntax within a programming language that is designed to make things easier to read or to express. It makes the language “sweeter” for human use: things can be expressed more clearly, more concisely, or in an alternative style that some may prefer. Hence, magic methods can be said to be Syntactic sugar. But it’s not just magic methods that are mapped to syntactic sugar methods, but higher order features such as Decorators are as well. Example 4: [code language=“python”] def my_decorator(my_function): def inner_decorator(): print(“This happened before!”) my_function() print(“This happens after “) print(“This happened at the end!”) return inner_decorator def my_decorated(): print(“This happened!”) var = my_decorator(my_decorated) if __name__ == ‘__main__’: var() [/code] The example above borrows from one of the examples in the post on Decorators. Here, my_decorator() is a decorator and is used to decorate my_decorated(). But rather than calling the decorator function my_decorator() with the argument my_decorated(), the above code can be syntactically sugar-coated as below: [code language=“python”] def my_decorator(my_function): def inner_decorator(): print(“This happened before!”) my_function() print(“This happens after “) print(“This happened at the end!”) return inner_decorator @my_decorator def my_decorated(): print(“This happened!”) if __name__ == ‘__main__’: my_decorated() [/code] Observing both code snippets, the decorator is syntactically sugar coated and called as: @my_decorator instead of instantiating the decorator with the function to be decorated as an argument, ie.. var = my_decorator(my_decorated) ","date":"02-06-2016","objectID":"/magic-methods-in-python/:0:2","tags":["programming","python"],"title":"Magic methods and Syntactic sugar in Python","uri":"/magic-methods-in-python/"},{"categories":["programming","python"],"content":"A few syntax resolution methods: ’name’ in my_list -\u003e my_list.__contains__(’name’) len(my_list) -\u003e my_list.__len__() print(my_list) -\u003e my_list.__repr__() my_list == “value” -\u003e my_list.__eq__(“value”) my_list[5] -\u003e my_list.__getitem__(5) my_list[5:10] -\u003e my_list.__getslice__(5, 10) NOTE: This article is written from the notes created while learning magic methods. The following articles (along with several others) were referred as part of the process. A Guide to Python’s Magic Methods, by Rafe Kettler Special method names, The Official Python 3 documentation ","date":"02-06-2016","objectID":"/magic-methods-in-python/:0:3","tags":["programming","python"],"title":"Magic methods and Syntactic sugar in Python","uri":"/magic-methods-in-python/"},{"categories":["programming","python"],"content":"_D_ecorators are wrapper functions (or classes) that wrap and modify another function (or class), and change it’s behavior as required. Decorators help to modify your code without actually modifying the working function/class itself. There are several inbuilt Decorators in Python, such as @classmethod and @staticmethod. Examples on these are due for another post. Decorators are called to act upon a function or class, by mentioning the Decorator name just above the function/class. Decorators are written such as it returns a function, rather than output something. Example 0: [code language=‘python’] @my_decorator def my_func(): print(“Hello”) my_func() [/code] In the above code snippet, when my_func() is called, the python interpreter calls the decorator function my_decorator, executes it, and then passes the result to my_func(). The example above doesn’t do anything worth, but the following example should help to get a better idea. NOTE: The examples below are taken from the excellent talks done by Jillian Munson (in PyGotham 2014) and Mike Burns for ThoughtBot. The URLs are at [1] and [2]. All credit goes to them. Example 1: [code language=“python”] def my_decorator(my_function): def inner_decorator(): print(“This happened before!”) my_function() print(“This happens after “) print(“This happened at the end!”) return inner_decorator @my_decorator def my_decorated(): print(“This happened!”) if __name__ == ‘__main__’: my_decorated() [/code] ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:0","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"Components: A function named my_decorated(). A decorator function named my_decorator(). The decorator function my_decorator() has a function within itself named inner_decorator(). The decorator function my_decorator(), returns the inner function inner_decorator(). Every function should return a value, if not it defaults to None. my_decorator() decorator should return the inner_decorator() inner function, else the decorator cannot be used with the my_decorated() function. To understand this, test with ‘return None’ for the decorator function my_decorator(). The inner function inner_decorator() is the one that actually decorates (modifies) the function my_decorated(). The decorator function is called on the function my_decorated() using the format @my_decorator. The decorator function takes an argument, which can be named whatever the developer decides. When the decorator function is executed, the argument is replaced with the function name on which the decorator is executed. In our case, it would be my_decorated() ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:1","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"How does the code work? The function my_decorated() is called. The interpreter sees that the decorator @my_decorator is called wrt this function. The interpreter searches for a function named my_decorator()and executes it. Since the decorator function returns the inner function inner_decorator(), the python interpreter executes the inner function. It goes through each steps, reaches my_function() , and gets it executed. Once that function is executed, it goes back and continues with the execution of the decorator my_decorator(). ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:2","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"Output: [code language=“bash”] # python decorators-1.py This happened before! # Called from the decorator This happened! # Called from the function This happens after # Called from the decorator This happened at the end! # Called from the decorator [/code] Example 2: [code language=“python”] def double(my_func): def inner_func(a, b): return 2 * my_func(a, b) return inner_func @double def adder(a, b): return a + b @double def subtractor(a, b): return a - b print(adder(10, 20)) print(subtractor(6, 1)) [/code] ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:3","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"Components: Two functions named adder() and subtractor(). A decorator function named double(). The decorator has an inner function named inner_func() which does the actual intended work. The decorator returns the value of the inner function inner_func() Both the adder() and subtractor()functions are decorated with the decorator double() ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:4","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"How does the code work? We call the adder() and subtractor() functions with a print(), since the said functions don’t print by default (due to the return statement). The python interpreter sees the decorator @double and calls it. Since the decorator returns the inner function inner_func(), the interpreter executes it. The decorator takes an argument my_func, which is always the function on which the decorator is applied, ie.. in our case my_case == adder()and my_case == subtractor(). The inner function within the decorator takes arguments, which are the arguments passed to the functions that are being decorated. ie.. Any arguments passed to adder() and subtractor()are passed to inner_func(). The statement return 2 * my_func(a, b) returns the value of : 2 x adder(10, 20) 2 x subtractor(6, 1) ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:5","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["programming","python"],"content":"Output: [code language=“bash”] # python decorators-2.py 60 10 [/code] Inbuilt decorators such as @staticmethod and @classmethod will be discussed in an upcoming post. NOTE: To see how decorators are syntactically sugar coated, read Magic methods and Syntactic sugar in Python ","date":"30-05-2016","objectID":"/decorators-object-oriented-programming/:0:6","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming","uri":"/decorators-object-oriented-programming/"},{"categories":["ceph"],"content":"Ceph OSD daemons need to ensure that the neighbouring OSDs are functioning properly so that the cluster remains in a healthy state. For this, each Ceph OSD process (ceph-osd) sends a heartbeat signal to the neighbouring OSDs. By default, the heartbeat signal is sent every 6 seconds [1], which is configurable of course. If the heartbeat check from one OSD doesn’t hear from the other within the set value for `osd_heartbeat_grace` [2], which is set to 20 seconds by default, the OSD that sends the heartbeat check reports the other OSD (the one that didn’t respond within 20 seconds) as down, to the MONs. Once an OSD reports three times that the non-responding OSD is indeed `down`, the MON acknowledges it and mark the OSD as down. The Monitor will update the Cluster map and send it over to the participating nodes in the cluster. When an OSD can’t reach another OSD for a heartbeat, it reports the following in the OSD logs: osd.510 1497 heartbeat_check: no reply from osd.11 since back 2016-04-28 20:49:42.088802 In Ceph Jewel, the MONs require a minimum of two ceph OSDs report a specific OSD as down from two nodes which are in different CRUSH subtrees, in order to actually mark the OSD as down. These are controlled by the following tunables : From ‘common/config_opts.h’: [1] OPTION(mon_osd_min_down_reporters, OPT_INT, 2) // number of OSDs from different subtrees who need to report a down OSD for it to count [2] OPTION(mon_osd_reporter_subtree_level , OPT_STR, “host”) // in which level of parent bucket the reporters are counted Image Courtsey : Red Hat Ceph Storage 1.3.2 Configuration guide ","date":"09-05-2016","objectID":"/ceph-osd-heartbeats/:0:0","tags":["ceph","monitors"],"title":"Ceph OSD heartbeats","uri":"/ceph-osd-heartbeats/"},{"categories":["ceph","programming","python"],"content":"Many a user wants to know if a Ceph cluster installation has been done to a specific suggested guideline. Technologies like RAID is better avoided in Ceph due to an additional layer, which Ceph already takes care of. I’ve started writing a tool which can be run from the Admin node, and it aims to check various such points. The code can be seen at https://github.com/arvimal/ceph_check The work is slow, really slow, due to my daily work, procrastination, and what not, even though I intend to finish this fast. ","date":"08-05-2016","objectID":"/ceph-check/:0:0","tags":null,"title":"`ceph-check` - A Ceph installation checker","uri":"/ceph-check/"},{"categories":["data-structures-and-algorithms","programming"],"content":"Efficiency or Complexity is how well you’re using your resources to get your code run. Efficiency can be calculated on the basis of how much time your code takes to run/execute. Understanding the efficiency of your code can help to reduce the complexity, thus improving the runtime efficiency further. Getting the same job done in less time and less system resources is always good. Once you find the efficiency of your program, you can start to find ways for: Reducing the complexity (or increase the efficiency) which will reduce the program run time, and hence free the computer resources in a proportional rate. Try to maintain a constant or reduced run time for a growing data set, which will help your program to fare well when the input size grows. In Computer Science, the `Big O` notation is used to indicate the effieciency or complexity of a program. It is denoted by ‘O(n)’, where ’n’ is a mathematical function to denote the input. This is Some examples: O(n) O(n³) O(n log(n)) O(√n) O(O(n) + 1) or O(1) Efficiency can be measures on the best, average, and worst cases. For example, consider finding a specific alphabet from a list of all the alphabets jumbled up. The worst case is your program running through all 26 iterations and finding the alphabet as the last value. The best case is when your program finds it in the first iteration itself. The average is when your program finds the alphabet somewhere around 12-15 iterations (ie.. half of the worst case scenario). Study of Data structures and Algorithms aim to study program complexities, and to reduce it as much as possible. Algorithms are a series of well-defined steps you follow to solve a problem, whereas Data Structures are specific structures by which how you layout your data. Application of well-known algorithms in a program can help in reducing the run time. More on Time complexity and the Big O notation can be read at: https://en.wikipedia.org/wiki/Time_complexity https://en.wikipedia.org/wiki/Big_O_notation ","date":"08-05-2016","objectID":"/big-o-notation/:0:0","tags":["algorithms","big-o-notation","code-complexity","data-structures","on"],"title":"Code complexity - The Big O notation [O(n)]","uri":"/big-o-notation/"},{"categories":["data-structures-and-algorithms","programming"],"content":"Arrays are a commonly used data structure, and is one of the first a DS student looks into. It is created as a collection of memory addresses which are contiguous in memory. These memory locations store data of a specific type depending on the array’s type. Advantages: Arrays are easier to create since the size and type is mentioned at the creation time. Arrays have constant access/lookup time since the lookup is done by accessing the memory location as an offset from the base/first element. Hence the complexity will be O(1). Arrays are contiguous in memory, ie.. a 10 cell array can start at perhaps 0x0001 and end at 0x0010. Disadvantages: The size of an array has to be defined at the time of its creation. This make the size static, and hence cannot be resized later. An array can only accomodate a specific data type. The type of data an array can store has to be defined at creation time. Hence, if an array is set to store integers, it can only store integers in each memory location. Since the size of an array is set at the time of creation, allocating an array may fail depending on the size of the array and the available memory on the machine. Inserting an element into an array can be expensive depending on the location. To insert an element at a particular position, for example ’n’, the element already has to be moved to ’n + 1’, the element at ’n + 1’ to ’n + 2’ etc.. Hence, if the position to which the element is written to is at the starting of the array, the operation will be expensive. But if the position is at the starting, it won’t be. What is the lookup time in an array? The elements in an array are continuguous to each other. The address of an element is looked up as an `offset` of the primary or base element. Hence, the lookup of any element in the array is constant and can be denoted by O(1). Arrays in Python Python doesn’t have a direct implementation of an Array. The one that closely resembles an array in python is a `list`. The major differences between an array and a list are: The size of lists are not static. It can be grown or shrinked using the `append` and `remove` methods. Arrays are static in size. lists can accomodate multiple data types, while arrays cannot. [code language=“python”] In [1]: mylist = [] In [2]: type(mylist) Out[2]: list In [3]: mylist.append(“string”) In [4]: mylist.append(1000) In [5]: mylist Out[5]: [‘string’, 1000] [/code] Time complexity of Arrays Indexing - O(1) Insertion/Deletion at beginning - O(n) (If the array has space to shift the elements) Insertion/Deletion at the end - O(1) (If the array has space at the end) Deletion at the end - O(1) (Since it doesn’t have to move any elements and reorder) Insertion at the middle - O(n) (Since it requires to move the elements to the right and reorder) Deletion at the middle - O(n) (Since it requires to delete the element and move the ones from the right to the left) The ‘array’ module Python comes with a module named ‘array’ which emulates the behavior of arrays. [code language=“python”] In [24]: import array In [25]: myarray = array.array(‘i’, [1,2,3,4]) In [26]: myarray Out[26]: array(‘i’, [1, 2, 3, 4]) [/code] ","date":"08-05-2016","objectID":"/data-structures-arrays/:0:0","tags":null,"title":"Data Structures - Arrays","uri":"/data-structures-arrays/"},{"categories":["ceph"],"content":"To get a MON map or an OSD map of a specific epoch, use: # ceph osd getmap # ceph mon getmap The map can be forwarded to a file as following: # ceph osd getmap -o /tmp/ceph_osd_getmap.bin This would be in a binary format, and hence will need to be dumped to a human-readable form. # osdmaptool –print /tmp/ceph-osd-getmap.bin This will print the current OSD map, similar to the output of ‘ceph osd dump’. Where this command shines is when you can fetch maps from previous epochs, and pull information on specific placement groups in those epochs. For example, I’ve had all the OSDs on one of my node down some time back (in a previous epoch). The ability to query a previous epoch gives the administrator the power to understand how exactly the cluster was at a specific time period. ","date":"08-05-2016","objectID":"/ceph-mon-osd-map-at-epoch/:0:0","tags":null,"title":"How to get a Ceph MON/OSD map at a specific epoch?","uri":"/ceph-mon-osd-map-at-epoch/"},{"categories":["ceph"],"content":"This is a crude bash one-liner I did to get the details of all the RBD images, as well as the information on snapshots and clones created from them. [code language=“bash”] # for pool in `rados lspools`; do echo “POOL :” $pool; rbd ls -l $pool; echo “—–”; done [/code] This will print an output similar to the following: [code language=“bash”] POOL : rbd NAME SIZE PARENT FMT PROT LOCK test_img 10240M 1 test_img2 1024M 2 test_img2@snap2 1024M 2 yes —– POOL : .rgw.root —– POOL : .rgw.control —– POOL : .rgw —– POOL : .rgw.gc —– POOL : .users.uid —– POOL : .users —– POOL : .users.swift —– POOL : .users.email —– POOL : .rgw.buckets.index —– POOL : images NAME SIZE PARENT FMT PROT LOCK clone1 1024M rbd/test_img2@snap2 2 —– [/code] ","date":"15-10-2015","objectID":"/list-ceph-pools-with-snapshots-and-rbd/:0:0","tags":null,"title":"List RBD images, snapshots, and clones in Ceph pools","uri":"/list-ceph-pools-with-snapshots-and-rbd/"},{"categories":["programming","python"],"content":"The usual way to iterate over a range of numbers or a list in python, is to use range(). Example 0: [code language=“python”] colors = [“yellow”, “red”, “blue”, “white”, “black”] for i in range(len(colors)): print(i, colors[i]) [/code] This should output: [code language=“bash”] (0, ‘yellow’) (1, ‘red’) (2, ‘blue’) (3, ‘white’) (4, ‘black’) [/code] print(), by default, returns a tuple. If we want to print it in a more presentable way, we’ll need to find the indice at which each value is, and print that as well. Re-write the code a bit, to achieve the desired output: [code language=“python”] colors = [“yellow”, “red”, “blue”, “white”, “black”] for i in range(len(colors)): color = colors[i] print(\"%d: %s\" % (i, color)) [/code] This should print: [code language=“bash”] 0: yellow 1: red 2: blue 3: white 4: black [/code] We can see that the above output starts with ‘0’ since python starts counting from ‘0’. To change that to ‘1’, we’ll need to tweak the print() statement. [code language=“python”] colors = [“yellow”, “red”, “blue”, “white”, “black”] for i in range(len(colors)): color = colors[i] print(\"%d: %s\" % (i + 1, color)) [/code] This should print: [code language=“bash”] 1: yellow 2: red 3: blue 4: white 5: black [/code] Even though the above code snippet isn’t that complex, a much better way exists to do this. This is where the builtin function enumerate() comes in. enumerate() returns a tuple when passed an object which supports iteration, for example, a list. It also supports a second argument named ‘start’ which default to 0, and can be changed depending on where to start the order. We’ll check what ‘start’ is towards the end of this article. [code language=“python”] colors = [“yellow”, “red”, “blue”, “white”, “black”] print(list(enumerate(colors))) [/code] This returns a list of a tuples. [code language=“bash”] [(0, ‘yellow’), (1, ‘red’), (2, ‘blue’), (3, ‘white’), (4, ‘black’)] [/code] To get to what we desire, modify it as: [code language=“python”] for i, color in enumerate(colors): print(’%d: %s’ % (i, color)) [/code] This outputs: [code language=“bash”] 0: yellow 1: red 2: blue 3: white 4: black [/code] Remember that we talked about that enumerate() takes a second value named ‘start’ which defaults to ‘0’? Let’s check how that’ll help here. The above output starts with ‘0’. ‘start’ can help to change that. [code language=“python”] for i, color in enumerate(colors, start=1): print(’%d: %s’ % (i, color)) [/code] This should change the output as: [code language=“bash”] 1: yellow 2: red 3: blue 4: white 5: black [/code] ","date":"12-10-2015","objectID":"/range-and-enumerate/:0:0","tags":["enumerate","programming","python","range"],"title":"range() and enumerate()","uri":"/range-and-enumerate/"},{"categories":["ceph"],"content":"In certain cases, a Ceph cluster may move away from an HEALTHY state due to “unfound” objects. A “ceph -s” should show if you have any unfound objects. So, what are unfound objects? How does an object become “unfound”? This article tries to explain why/how “unfound” objects come into existence. Let’s look into the life cycle of a write to a pool. The client contacts a Ceph monitor and fetches the CRUSH map, which includes: MON map OSD map PG map CRUSH map MDS map Once the client has the maps, the Ceph client-side algorithm breaks the data being written into objects (the object size depends on the client side configuration). Clients such as RBD and RGW uses a 4MB object size, but RADOS doesn’t actually have such a limitation. Each pool has a set of Placement Groups (PG) assigned to it at the time of creation, and the client always writes to a pool. Since the client has the maps which talks about the entire cluster, it knows the placement groups within the pool which it is writing to, and the OSDs assigned for each placement group. The client talks to the OSDs directly without going over any other path, such as a monitor. The PG map will have the ACTING and UP OSD sets for each PG. To understand the ACTING set and UP set for the PGs, as well as a plethora of other information, use : [code language=“bash”] # ceph pg dump [/code] The ACTING set is the current active set of OSDs that stores the replica sets for that particular PG. The UP set is the set of OSDs that are currently up and running. Usually, the ACTING set and UP set are the same. When an OSD in the ACTING set is not reachable, other OSDs wait for 5 minutes (which is configurable) for it to come back online (this is checked with a hearbeat). The said OSD is removed out of the UP set when it is not accessible. If it doesn’t come back online within the configured period, the said OSD is marked out of the ACTING set, as well as the UP set. When it comes back, it is added back to the ACTING/UP set and a peering happens where the data is synced back. Let’s discuss the scenario where an “unfound” object came come into existence. Imagine a pool with a two replica configuration. A write that goes into the pool is split into objects and stored in the OSDs which are in the ACTIVE set of a PG. One OSD in the ACTING set goes down. The write is done on the second OSD which is UP and ACTING. The first OSD which went down, came back up. The peering process started between the first OSD (that came back), and the second OSD (that serviced the write). Peering refers to the process of arriving at an understanding on the object states between the OSDs in an ACTING set, and sync up the metadata/data between them. Both the OSDs reach an understanding on which objects needs to be synced. The second OSD that had the objects ready to be synced, went down before the sync process starts or is in midway. In this situation, the first OSD knows about the objects that was written to the second OSD, but cannot probe it. The first OSD will try to probe possible locations for copies, provided there are more replicas. If the OSD is able to find other locations, the data will be synced up. But in case there are no other copies, and the OSD with the only copy is not coming up anytime soon (perhaps a disk crash, file system corruption etc..) the only way is to either mark the object as “lost”, or revert it back to the previous version. Reverting to a previous version may not be possible for a new object, and in such cases the only way would be to mark it as “lost” or copy from a backup. 1. For a new object without a previous version: [code language=“bash”] # ceph pg {pg.num} mark_unfound_lost delete [/code] 2. For an object which is likely to have a previous version: [code language=“bash”] # ceph pg {pg.num} mark_unfound_lost revert [/code] NOTE: The upstream Ceph documentation has an excellent write-up about “unfound” objects here. I suggest reading the documentation prior taking any sort of action in a ca","date":"07-10-2015","objectID":"/unfound-objects-in-ceph-cluster/:0:0","tags":null,"title":"Ceph and unfound objects","uri":"/unfound-objects-in-ceph-cluster/"},{"categories":["ceph"],"content":"I recently came across a scenario where the objects in a RADOS pool used for an RBD block device doesn’t get removed, even if the files created through the mount point were removed. I had an RBD image from an RHCS1.3 cluster mapped to a RHEL7.1 client machine, with an XFS filesystem created on it, and mounted locally. Created a 5GB file, and I could see the objects being created in the rbd pool in the ceph cluster. 1.RBD block device information [code language=“bash”] # rbd info rbd_img rbd image ‘rbd_img’: size 10240 MB in 2560 objects order 22 (4096 kB objects) block_name_prefix: rb.0.1fcbe.2ae8944a format: 1 [/code] An XFS file system was created on this block device, and mounted at /test. 2.Write a file onto the RBD mapped mount point. Used ‘dd’ to write a 5GB file. [code language=“bash”] # dd if=/dev/zero of=/mnt/rbd_image.img bs=1G count=5 5+0 records in 5+0 records out 5368709120 bytes (5.4 GB) copied, 8.28731 s, 648 MB/s [/code] 3.Check the objects in the backend RBD pool [code language=“bash”] # rados -p rbd ls | wc -l \u003c Total number of objects in the ‘rbd’ pool\u003e [/code] 4.Delete the file from the mount point. [code language=“bash”] # rm /test/rbd_image.img -f # ls /test/ –NO FILES LISTED– [/code] 5.List the objects in the RBD pool [code language=“bash”] # rados -p rbd ls | wc -l \u003c Total number of objects in the ‘rbd’ pool \u003e [/code] The number of objects doesn’t go down as we expect, after the file deletion. It remains the same, wrt to step 3. Why does this happen? This is due to the fact that traditional file systems do not delete the underlying data blocks even if the files are deleted. The process of writing a file onto a file system involves several steps like finding free blocks and allocating them for the new file, creating an entry in the directory entry structure of the parent folder, setting the name and inode number in the directory entry structure, setting pointers from the inode to the data blocks allocated for the file etc.. When data is written to the file, the data blocks are used to store the data. Additional information such as the file size, access times etc.. are updated in the inode after the writes. Deleting a file involves removing the pointers from the inode to the corresponding data blocks, and also clearing the name\u003c-\u003einode mapping from the directory entry structure of the parent folder. But, the underlying data blocks are not cleared off, since that is a high I/O intensive operation. So, the data remains on the disk, even if the file is not present. A new write will make the allocator take these blocks for the new data, since they are marked as not-in-use. This applies for the files created on an RBD device as well. The files created on top of the RBD-mapped mount point will ultimately be mapped to objects in the RADOS cluster. When the file is deleted from the mount point, since the entry is removed, it doesn’t show up in the mount point. But, since the file system doesn’t clear off the underlying block device, the objects remain in the RADOS pool. These would be normally over-written when a new file is created via the mount point. But this has a catch though. Since the pool contains the objects even if the files are deleted, it consumes space in the rados pool (even if they’ll be overwritten). An administrator won’t be able to get a clear understanding on the space usage, if the pool is used heavily, and multiple writes are coming in. In order to clear up the underlying blocks, or actually remove them, we can rely on the TRIM support most modern disks offer. Read more about TRIM at Wikipedia. TRIM is a set of commands supported by HDD/SSDs which allow the operating systems to let the disk know about the locations which are not currently being used. Upon receiving a confirmation from the file system layer, the disk can go ahead and mark the blocks as not used. For the TRIM commands to work, the disks and the file system has to have the support. All the modern file systems have built-in suppo","date":"07-10-2015","objectID":"/rbd-object-remain-in-pool-after-deletion/:0:0","tags":["ceph","discard","fstrim","objects","rados","rados-block-device","rbd","trim"],"title":"Ceph Rados Block Device (RBD) and TRIM","uri":"/rbd-object-remain-in-pool-after-deletion/"},{"categories":["ceph"],"content":"Ceph supports custom rulesets via CRUSH, which can be used to sort hardware based on various features such as speed and other factors, set custom weights, and do a lot of other useful things. Pools, or the buckets were the data is written to, can be created on the custom rulesets, hence positioning the pools on specific hardware as per the administrator’s need. A large Ceph cluster may have lots of pools and rulesets specific for multiple use-cases. There may be times when we’d like to understand the pool to ruleset mapping. The default CRUSH ruleset is named ‘replicated_ruleset’. The available CRUSH rulesets can be listed with: $ ceph osd crush rule ls On a fresh cluster, or one without any custom rulesets, you’d find the following being printed to stdout. # ceph osd crush rule ls [ “replicated_ruleset” ] I’ve got a couple more on my cluster, and this is how it looks: # ceph osd crush rule ls [ “replicated_ruleset”, “replicated_ssd”, “erasure-code”] Since this article looks into the mapping of pools to CRUSH rulesets, it’d be good to add in how to list the pools, as a refresher. # ceph osd lspools On my Ceph cluster, it turned out to be: # ceph osd lspools 0 data,1 metadata,2 rbd,21 .rgw,22 .rgw.root,23 .rgw.control,24 .rgw.gc,25 .users.uid,26 .users,27 .users.swift,28 test_pool, Since you have the pool name you’re interested in, let’s see how to map it to the ruleset. The command syntax is: # ceph osd pool get \u003cpool_name\u003e crush_ruleset I was interested to understand the ruleset on which the pool ‘test_pool’ was created. The command to list this was: # ceph osd pool get test_pool crush_ruleset crush_ruleset: 1 Please note that the rulesets are numbered from ‘0’, and hence ‘1’ would map to the CRUSH ruleset ‘replicated_ssd’. We’ll try to understand how a custom ruleset is created, in another article. ","date":"23-09-2015","objectID":"/find-crush-ruleset-of-a-pool/:0:0","tags":null,"title":"Custom CRUSH rulesets and pools","uri":"/find-crush-ruleset-of-a-pool/"},{"categories":["ceph"],"content":"In case you are trying to get the OSD ID and the corresponding node IP address mappings in a script-able format, use the following command: # ceph osd find This will print the OSD number, the IP address, the host name, and the default root in the CRUSH map, as a python dictionary. # ceph osd find 2 { “osd”: 2, “ip”: “192.168.122.112:6800\\/5311”, “crush_location”: { “host”: “node4”, “root”: “default”}} The output is in json format, which has a key:value format. This can be parsed using awk/sed, or any programming languages that support json. All recent ones do. For a listing of all the OSDs and related information, get the number of OSDs in the cluster, and then use that number to probe the OSDs. # for i in `seq 0 $(ceph osd stat | awk {‘print $3’})`; do ceph osd find $i; echo; done This should output: { “osd”: 0, “ip”: “192.168.122.244:6805\\/2579”, “crush_location”: { “host”: “node3”, “root”: “ssd”}} { “osd”: 1, “ip”: “192.168.122.244:6800\\/955”, “crush_location”: { “host”: “node3”, “root”: “ssd”}} { “osd”: 2, “ip”: “192.168.122.112:6800\\/5311”, “crush_location”: { “host”: “node4”, “root”: “default”}} { “osd”: 3, “ip”: “192.168.122.112:6805\\/5626”, “crush_location”: { “host”: “node4”, “root”: “default”}} { “osd”: 4, “ip”: “192.168.122.82:6800\\/4194”, “crush_location”: { “host”: “node5”, “root”: “default”}} { “osd”: 5, “ip”: “192.168.122.82:6805\\/4521”, “crush_location”: { “host”: “node5”, “root”: “default”}} { “osd”: 6, “ip”: “192.168.122.73:6801\\/5614”, “crush_location”: { “host”: “node2”, “root”: “ssd”}} { “osd”: 7, “ip”: “192.168.122.73:6800\\/1719”, “crush_location”: { “host”: “node2”, “root”: “ssd”}} { “osd”: 8, “ip”: “192.168.122.10:6805\\/5842”, “crush_location”: { “host”: “node6”, “root”: “default”}} { “osd”: 9, “ip”: “192.168.122.10:6800\\/4356”, “crush_location”: { “host”: “node6”, “root”: “default”}} { “osd”: 10, “ip”: “192.168.122.109:6800\\/4517”, “crush_location”: { “host”: “node7”, “root”: “default”}} { “osd”: 11, “ip”: “192.168.122.109:6805\\/4821”, “crush_location”: { “host”: “node7”, “root”: “default”}} ","date":"18-09-2015","objectID":"/listing-osd-nodes-scriptable/:0:0","tags":["ceph","osd"],"title":"OSD information in a scriptable format","uri":"/listing-osd-nodes-scriptable/"},{"categories":["ceph"],"content":"The MON map is used by the monitors in a Ceph cluster, where they keep track of various attributes relevant to the working of the cluster. Similar to the CRUSH map, a monitor map can be pulled out of the cluster, inspected, changed, and injected back to the monitors, manually. A frequent use-case is when the IP address of a monitor changes and the monitors cannot agree on a quorum. Monitors use the monitor map (monmap) to get the details of other monitors. So just changing the monitor address in ‘ceph.conf’ and pushing the configuration to all the nodes won’t help to propagate the changes. In most cases, starting the monitor with a wrong monitor map would make the monitors commit suicide, since they would find conflicting information about themself in the mon map due to the IP address change. There are two methods to fix this problem, the first being adding enough new monitors, let them form a quorum, and remove the faulty monitors. This doesn’t need any explanation. The second and more crude way, is to edit the monitor map directly, set the new IP address, and upload the monmap back to the monitors. This article discusses the second method, ie.. how to edit the monmap, and re-inject it back. This can be done using the ‘monmap’ tool. 1. As the first step, login to one of the monitors, and get the monitor map: # ceph mon getmap -o /tmp/monitor_map.bin 2. Inspect what the monitor map contains: # monmaptool –print /tmp/monitor_map.bin An example from my cluster : # monmaptool –print monmap monmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000 0: 192.168.122.73:6789/0 mon.node2 3. Remove the node which has the wrong IP address, referring it’s hostname # monmaptool –rm node2 /tmp/monitor_map.bin 4. Inspect the monitor map to see if the monitor is indeed removed. # monmaptool –print /tmp/monitor_map.bin monmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000 5. Add a new monitor (or the existing monitor with it’s new IP) # monmaptool –add node3 192.168.122.76:6789 /tmp/monitor_map.bin monmaptool: monmap file monmap monmaptool: writing epoch 1 to monmap (1 monitors) 6. Check the monitor map to confirm the changes # monmaptool –print monmap monmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000 0: 192.168.122.76:6789/0 mon.node3 7. Make sure the mon processes are not running on the monitor nodes # service ceph stop mon 8. Upload the changes # ceph-mon -i monitor_node –inject-monmap /tmp/mon_map.bin 9. Start the mon process on each monitor # service ceph start mon 10. Check if the cluster has taken in the changes. # ceph -s ","date":"01-09-2015","objectID":"/view-change-inject-monitor-map/:0:0","tags":["ceph","monitors","monmaptool"],"title":"Monitor maps, how to edit them?","uri":"/view-change-inject-monitor-map/"},{"categories":["ceph","programming","python"],"content":"Recently, I had an incident where the OSDs were crashing at the time of startup. Obviously, the next step was to enable debug logs for the OSDs and understand where they were crashing. Enabled OSD debug logs dynamically by injecting it with: # ceph tell osd.* injectargs –debug-osd 20 –debug-ms 1 NOTE: This command can be run from the MON nodes. Once this was done, the OSDs were started manually (since it were crashing and not running) and watched out for the next crash. It crashed with the following logs : *read_log 107487'1 (0'0) modify f6b07b93/rbd_data.hash/head//12 by client.version date, time *osd/PGLog.cc: In function ‘static bool PGLog::read_log(ObjectStore*, coll_t, hobject_t, const pg_info_t\u0026, std::mapeversion_t, hobject_t\u0026, PGLog::IndexedLog\u0026, pg_missing_t\u0026, std::ostringstream\u0026, std::setstd::basic_stringchar *)’ thread thread time date, time *osd/PGLog.cc: 809: FAILED assert(last_e.version.version e.version.version)ceph version version-details 1: (PGLog::read_log(ObjectStore*, coll_t, hobject_t, pg_info_t const\u0026, std::mapeversion_t, hobject_t, std::lesseversion_t, std::allocatorstd::paireversion_t const,hobject_t , PGLog::IndexedLog\u0026, pg_missing_t\u0026, std::basic_ostringstreamchar, std::char_traitschar, std::allocatorchar, std::setstd::string, std::lessstd:string, std::allocatorstd::string *)+0x13ee) [0x6efcae] 2: (PG::read_state(ObjectStore*, ceph::buffer::list\u0026)+0x315) [0x7692f5] 3: (OSD::load_pgs()+0xfff) [0x639f8f] 4: (OSD::init()+0x7bd) [0x63c10d] 5: (main()+0x2613) [0x5ecd43] 6: (__libc_start_main()+0xf5) [0x7fdc338f9af5] 7: /usr/bin/ceph-osd() [0x5f0f69] The above is a log snippet at which the OSD process was crashing. The ceph-osd process was reading through the log areas of each PG in the OSD, and once it reached the problematic PG it crashed due to failing an assert condition. Checking the source at ‘osd/PGLog.cc’, we see that this error is logged from ‘PGLog::read_log’. void PGLog::read_log(ObjectStore *store, coll_t pg_coll, coll_t log_coll, ghobject_t log_oid, const pg_info_tinfo, mapeversion_t, hobject_tdivergent_priors, IndexedLoglog, pg_missing_tmissing, ostringstreamoss, setstring *log_keys_debug) { … if (!log.log.empty()) { pg_log_entry_t last_e(log.log.back()); assert(last_e.version.version e.version.version); == The assert condition at which read_log is failing for a particular PG assert(last_e.version.epoch = e.version.epoch); In order to make the OSD start, we needed to move this PG to a different location using the ‘ceph_objectstore_tool’ so that the ceph-osd can bypass the problematic PG. To understand the PG where it was crashing, we had to do some calculations based on the logs. The ‘read_log’ line in the debug logs contain a hex value after the string “modify” and that is the hash of the PG number. The last number in that series is the pool id (12 in our case). The following python code will help to calculate the PG id based on the arguments passed to it. This program accepts three arguments, the first being the hex value we talked about, the second being the pg_num of the pool, and the third one being the pool id. [code language=“python”] #!/usr/bin/env python # Calculate the PG ID from the object hash # vimal@redhat.com import sys def pg_id_calc(*args): if any([len(args) == 0, len(args) \u003e 3, len(args) \u003c 3]): help() else: hash_hex = args[0] pg_num = int(args[1]) pool_id = int(args[2]) hash_dec = int(hash_hex, 16) id_dec = hash_dec % pg_num id = hex(id_dec) pg_id = str(pool_id) + “.” + str(id)[2:] print(\"\\nThe PG ID is %s\\n\" % pg_id) def help(): print(“Usage:”) print(“This script expects the hash (in Hex), pg_num of the pool, and the pool id as arguments, in order”) print(\"\\nExample:\") print(\"./pg_id_calc.py 0x8e2fe5d7 2048 12\") sys.exit() if __name__ == ‘__main__’: pg_id_calc(*sys.argv[1:]) [/code] An example of the program in action: # python pg_id_calc.py 0xf6b07b93 2048 12 The PG ID is 12.393 Once we get the PG ID, we can proceed using ‘ceph_objectstore_tool’ to move the PG to a different ","date":"30-08-2015","objectID":"/calculate-pg-id-from-ceph-osd-debug-logs/:0:0","tags":["ceph","pg","python"],"title":"Calculate a PG id from the hex values in Ceph OSD debug logs","uri":"/calculate-pg-id-from-ceph-osd-debug-logs/"},{"categories":["ceph"],"content":"Understanding the mapping of Pools and Placement Groups can be very useful while troubleshooting Ceph problems. A direct method is to dump information on the PGs via : # ceph pg dump This command should output something like the following: pg_stat objects mip degr unf bytes log disklog state 5.7a 0 0 0 0 0 0 0 active+clean The output will have more information, and I’ve omitted it for the sake of explanation. The first field is the PG ID, which are two values separated by a single dot (.). The left side value is the POOL ID, while the right side value is the actual PG number. It means that a specific PG can only be present under a specific pool, ie.. no PGs can be shared across pools. But please note that OSDs can be shared across multiple PGs. To get the pools and associated numbers, use: # ceph osd lspools 0 data,1 metadata,2 rbd,5 ssdtest,6 ec_pool, So, the PG 5.7a belongs to the pool numbered ‘5’, ie.. ‘ssdtest’, and the PG number is ‘7a’. The output of ‘ceph pg dump’ also shows various important informations such as the Acting OSD set, the primary OSD, the last time the PG was reported, the state of the PG, the time at which a normal scrub as well as a deep-scrub was run etc.. ","date":"17-08-2015","objectID":"/map-pg-to-pool/:0:0","tags":["ceph","placement-groups","pool"],"title":"Mapping Placement Groups and Pools","uri":"/map-pg-to-pool/"},{"categories":null,"content":"In many cases, one would like to understand the journal disk a particular OSD is using. There are two methods to understand this: a) This is the most direct method, and should give you details on the OSD disks and the corresponding journal disks. [sourcecode language=“bash” gutter=“false”] # ceph-disk list [/sourcecode] This should output something like: [sourcecode language=“bash” gutter=“false”] # ceph-disk list /dev/sda : /dev/sda1 other, xfs, mounted on /boot /dev/sda2 other, LVM2_member /dev/sr0 other, unknown /dev/vda : /dev/vda1 ceph data, active, cluster ceph, osd.0, journal /dev/vda2 /dev/vda2 ceph journal, for /dev/vda1 /dev/vdb : /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdc1 /dev/vdc : /dev/vdc1 ceph journal, for /dev/vdb1 [/sourcecode] b) The second method is cruder, and involves listing the OSD mount point on the file system. [sourcecode language=“bash” gutter=“false”] # ls -l /var/lib/ceph/osd/ceph-0/ total 52 -rw-r–r–. 1 root root 191 Aug 3 18:02 activate.monmap -rw-r–r–. 1 root root 3 Aug 3 18:02 active -rw-r–r–. 1 root root 37 Aug 3 18:02 ceph_fsid drwxr-xr-x. 70 root root 4096 Aug 4 00:38 current -rw-r–r–. 1 root root 37 Aug 3 18:02 fsid lrwxrwxrwx. 1 root root 58 Aug 3 18:02 journal -\u003e /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 -rw-r–r–. 1 root root 37 Aug 3 18:02 journal_uuid -rw——-. 1 root root 56 Aug 3 18:02 keyring -rw-r–r–. 1 root root 21 Aug 3 18:02 magic -rw-r–r–. 1 root root 6 Aug 3 18:02 ready -rw-r–r–. 1 root root 4 Aug 3 18:02 store_version -rw-r–r–. 1 root root 42 Aug 3 18:02 superblock -rw-r–r–. 1 root root 0 Aug 5 13:09 sysvinit -rw-r–r–. 1 root root 2 Aug 3 18:02 whoami # ls -l /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 lrwxrwxrwx. 1 root root 10 Aug 5 13:08 /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 -\u003e ../../vda2 [/sourcecode] As you can see, the file ‘journal’ is a symlink to the journal disk. The first method is much easier, but its always better to know how things are layered out underneath. ","date":"05-08-2015","objectID":"/map-journal-disk-of-a-ceph-osd/:0:0","tags":null,"title":"How to identify the journal disk for a Ceph OSD?","uri":"/map-journal-disk-of-a-ceph-osd/"},{"categories":["ceph"],"content":"‘Calamari’ is the monitoring interface for a Ceph cluster. The Calamari interface password can be reset/changed using the ‘calamari-ctl’ command. # calamari-ctl change_password –password {password} {user-name} calamari-ctl can also be used to add a user, as well as disable, enable, and rename the user account. A ‘–help’ should print out all the available ones. # calamari-ctl –help ","date":"13-07-2015","objectID":"/reset-calamari-interface-password/:0:0","tags":["calamari","ceph"],"title":"Resetting Calamari password","uri":"/reset-calamari-interface-password/"},{"categories":["ceph"],"content":"The Ceph monitor store growing to a big size is a common occurrence in a busy Ceph cluster. If a ‘ceph -s’ takes considerable time to return information, one of the possibility is the monitor database being large. Other reasons included network lags between the client and the monitor, the monitor not responding properly due to the system load, firewall settings on the client or monitor etc.. The best way to deal with a large monitor database is to compact the monitor store. The monitor store is a leveldb store which stores key/value pairs. There are two ways to compact a levelDB store, either on the fly or at the monitor process startup. To compact the store dynamically, use : # ceph tell mon.[ID] compact To compact the levelDB store every time the monitor process starts, add the following in /etc/ceph/ceph.conf under the [mon] section: mon compact on start = true The second option would compact the levelDB store each and every time the monitor process starts. The monitor database is stored at /var/lib/ceph/mon//store.db/ as files with the extension ‘.sst’, which is the synonym for ‘Sorted String Table’ To read more on levelDB, please refer: https://en.wikipedia.org/wiki/LevelDB http://leveldb.googlecode.com/svn/trunk/doc/impl.html http://google-opensource.blogspot.in/2011/07/leveldb-fast-persistent-key-value-store.html ","date":"09-07-2015","objectID":"/compact-a-ceph-monitor-data-store/:0:0","tags":["ceph","leveldb","monitors"],"title":"Compacting a Ceph monitor store","uri":"/compact-a-ceph-monitor-data-store/"},{"categories":["ceph"],"content":"Data Scrubbing is an error checking and correction method or routine check to ensure that the data on file systems are in pristine condition, and has no errors. Data integrity is of primary concern in today’s conditions, given the humongous amounts of data being read and written daily. A simple example for a scrubbing, is a file system check done on file systems with tools like ’e2fsck’ in EXT2/3/4, or ‘xfs_repair’ in XFS. Ceph also includes a daily scrubbing as well as weekly scrubbing, which we will talk about in detail in another article. This feature is available on most hardware RAID controllers, backup tools, as well as softwares that emulate RAID such as MD-RAID. Btrfs is one of the file systems that can schedule a internal scrubbing automatically, to ensure that corruptions are detected and preventive measures taken automatically. Since Btrfs can maintain multiple copies of data, once it finds an error in the primary copy, it can check for a good copy (if mirroring is used) and replace it. We will be looking more into scrubbing, especially how it is implemented in Ceph, and the various tunables, in an upcoming post. ","date":"08-07-2015","objectID":"/what-is-data-scrubbing/:0:0","tags":["ceph","osd","scrubbing"],"title":"What is data scrubbing?","uri":"/what-is-data-scrubbing/"},{"categories":["ceph"],"content":"In a previous post, we saw how to dynamically change a tunable on a running Ceph cluster dynamically. Unfortunately, such a change is not permanent, and will revert back to the previous setting once ceph is restarted. Rather than using the command ‘ceph tell’, I recently came upon another way to change configuration values. We’ll try changing the tunable ‘mon_osd_full_ratio’ once again. 1. Get the current setting # ceph daemon osd.1 config get mon_osd_full_ratio { “mon_osd_full_ratio”: “0.75”} 2. Change the configuration value using ‘ceph daemon’. # ceph daemon osd.1 config set mon_osd_full_ratio 0.85 { “success”: “mon_osd_full_ratio = ‘0.85’ “} 3. Check if the change has been introduced. # ceph daemon osd.1 config get mon_osd_full_ratio { “mon_osd_full_ratio”: “0.85”} 4. Restart the ‘ceph’ service # service ceph restart 5. Check the status # ceph daemon osd.1 config get mon_osd_full_ratio { “mon_osd_full_ratio”: “0.75”} NOTE: Please note that the changes introduced with ‘ceph tell’ as well as ‘ceph daemon’ is not persistent across process restarts. ","date":"03-06-2015","objectID":"/dynamically-change-ceph-configuration/:0:0","tags":["ceph"],"title":"Another method to dynamically change a Ceph configuration","uri":"/dynamically-change-ceph-configuration/"},{"categories":["ceph"],"content":"You may have seen the ‘noout’ flag set in the output of ‘ceph -s’. What does this actually mean? This is a global flag for the cluster, which means that if an OSD is out, the said OSD is not marked out of the cluster and data balancing shouldn’t start to maintain the replica count. By default, the monitors mark the OSDs out of the acting set if it is not reachable for 300 seconds, ie.. 5 minutes. To know the default value set in your cluster, use: # ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_report_timeout When an OSD is marked as out, another OSD takes its place and data replication starts to that OSD depending on the number of replica counts each pool has. If this flag (noout) is set, the monitor will not mark the OSDs out from the acting set. The PGs will be reporting an inconsistent state, but the OSD will still be in the acting set. This can be helpful when we want to remove an OSD from the server, but don’t want the data objects to be replicated over to another OSD. To set the ‘noout’ flag, use: # ceph osd set noout Once everything you’ve planned has been done/finished, you can reset it back using: # ceph osd unset noout ","date":"27-05-2015","objectID":"/osds-noout-status/:0:0","tags":["ceph","noout","osd"],"title":"'noout' flag in Ceph","uri":"/osds-noout-status/"},{"categories":["ceph"],"content":"It is possible to change a particular configuration setting in a Ceph cluster dynamically, and I think it is a very neat and useful feature. Imagine the case where you want to change the replica count of a particular PG from 3 to 4. How would you change this without restarting the Ceph cluster itself? That is where the ‘ceph tell’ command comes in. As we saw in the previous post, you can get the list of configuration settings using the administrator socket, from either a monitor or an OSD node. To change a configuration use: [sourcecode language=“bash” gutter=“false”] # ceph tell mon.* injectargs ‘–{tunable value_to_be_set}’ [/sourcecode] For example, to change the timeout value after which an OSD is out and down, can be changed with: [sourcecode language=“bash” gutter=“false”] # ceph tell mon.* injectargs ‘–mon_osd_report_timeout 400’ [/sourcecode] By default, it is 300 seconds, ie.. 5 minute ","date":"27-05-2015","objectID":"/change-ceph-configurations-dynamically/:0:0","tags":["ceph","ceph-tell"],"title":"How to dynamically change a configuration value in a Ceph cluster?","uri":"/change-ceph-configurations-dynamically/"},{"categories":["ceph"],"content":"In many cases we would like to get the active configurations from a Ceph node, either a monitor or an OSD node. A neat feature, I must say, is to probe the administrative socket file to get a listing of all the active configurations, be it on the OSD node or the monitor node. This comes handy when we have changed a setting and wants to confirm if it had indeed changed or not. The admin socket file exists for both the monitors and the OSD nodes. The monitor node will have a single admin socket file, while the OSD nodes will have an admin socket for each of the OSDs present on the node. Listing of the admin socket on a monitor node [sourcecode language=“bash” gutter=“false”] # ls /var/run/ceph/ -l total 4 srwxr-xr-x. 1 root root 0 May 13 05:13 ceph-mon.hp-m300-2.asok -rw-r–r–. 1 root root 7 May 13 05:13 mon.hp-m300-2.pid [/sourcecode] Listing of the admin sockets on an OSD node [sourcecode language=“bash” gutter=“false”] # ls -l /var/run/ceph/ total 20 srwxr-xr-x. 1 root root 0 May 8 02:42 ceph-osd.0.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.2.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.3.asok srwxr-xr-x. 1 root root 0 May 8 02:42 ceph-osd.4.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.5.asok -rw-r–r–. 1 root root 8 May 8 02:42 osd.0.pid -rw-r–r–. 1 root root 8 May 26 11:18 osd.2.pid -rw-r–r–. 1 root root 8 May 26 11:18 osd.3.pid -rw-r–r–. 1 root root 8 May 8 02:42 osd.4.pid -rw-r–r–. 1 root root 8 May 26 11:18 osd.5.pid [/sourcecode] For example, consider that we have changed the ‘mon_osd_full_ratio’ value, and need to confirm that the cluster has picked up the change. We can get a listing of the active configured settings and grep out the setting we are interested in. [sourcecode language=“bash” gutter=“false”] # ceph daemon /var/run/ceph/ceph-mon.*.asok config show [/sourcecode] The above command prints out a listing of all the active configurations and their current values. We can easily grep out ‘mon_osd_full_ratio’ from this list. [sourcecode language=“bash” gutter=“false”] # ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_full_ratio [/sourcecode] On my test cluster, this printed out ‘0.75’ which is the default setting. The cluster should print out ’near full’ warnings once any OSD has reached 75% of its size. This can be checked by probing the OSD admin socket as well. NOTE: In case you are probing a particular OSD, please make sure to use the OSD admin socket on the node in which the OSD is. In order to locate the OSD and the node it is on, use : [sourcecode language=“bash” gutter=“false”] # ceph osd tree [/sourcecode] Example: We try probing the OSD admin socket on its node, for ‘mon_osd_full_ratio’ as we did on the monitor. It should return the same value. [sourcecode language=“bash” gutter=“false”] # ceph daemon /var/run/ceph/ceph-osd.5.asok config show | grep mon_osd_full_ratio [/sourcecode] NOTE: Another command exists which should print the same configuration settings, but only for OSDs. [sourcecode language=“bash” gutter=“false”] # ceph daemon osd.5 config show [/sourcecode] A drawback worth mentioning, this should be executed on the node on which the OSD is present. To find that the OSD to node mapping, use ‘ceph osd tree’. ","date":"27-05-2015","objectID":"/get-a-list-of-all-configurations-ceph-cluster/:0:0","tags":["admin-socket","ceph","config-show"],"title":"How to fetch the entire list of tunables along with the values for a Ceph cluster node?","uri":"/get-a-list-of-all-configurations-ceph-cluster/"},{"categories":["ceph"],"content":"There could be many scenarios where you’d need to change the percentage of space usage on a Ceph OSD. One such use case would be when your OSD space is about to hit the hard limit, and is constantly sending you warnings. For some reason or other, you may need to extend the threshold limit for some time. In such a case, you don’t need to change/add the configuration in ceph.conf and push it across. Rather you can do it while the cluster is online, via command mode. The ‘ceph tell’ is a very useful command in the sense the administrator don’t need to stop/start the OSDs, MONs etc.. after a configuration change. In our case, we are looking to set the ‘mon_osd_full_ratio’ to 98%. We can do it by using: [sourcecode language=“bash” gutter=“false”] # ceph tell mon.* injectargs “–mon_osd_full_ratio .98” [/sourcecode] In an earlier post (https://goo.gl/xjXOoI) we had seen how to get all the configurable options from a monitor. If I understand correct, almost all the configuration values can be changed online by injecting the values using ‘ceph tell’. ","date":"07-05-2015","objectID":"/how-to-change-ceph-osd-filling-ratio/:0:0","tags":["ceph","fill-ratio","osd"],"title":"How to change the filling ratio for a Ceph OSD?","uri":"/how-to-change-ceph-osd-filling-ratio/"},{"categories":["ceph"],"content":"I’m still studying Ceph, and recently faced a scenario in which one of my Ceph nodes went down due to hardware failure. Even though my data was safe due to the replication factor, I was not able to remove the node from the cluster. I could remove the OSDs on the node, but I didn’t find a way to remove the node being listed in ‘ceph osd tree’. I ended up editing the CRUSH map by hand, to remove the host, and uploaded it back. This worked as expected. Following are the steps I did to achieve this. a) This was the state just after the node went down: [sourcecode language=“bash” gutter=“false”] # ceph osd tree # id weight type name up/down reweight -10 .08997 root default -20 .01999 host hp-m300-5 00 .009995 osd.0 up 1 40 .009995 osd.4 up 1 -30 .009995 host hp-m300-9 10 .009995 osd.1 down 0 -40 .05998 host hp-m300-4 20 .04999 osd.2 up 1 30 .009995 osd.3 up 1 [/sourcecode] [sourcecode language=“bash” gutter=“false”] # ceph -w cluster 62a6a880-fb65-490c-bc98-d689b4d1a3cb health HEALTH_WARN 64 pgs degraded; 64 pgs stuck unclean; recovery 261/785 objects degraded (33.248%) monmap e1: 1 mons at {hp-m300-4=10.65.200.88:6789/0}, election epoch 1, quorum 0 hp-m300-4 osdmap e130: 5 osds: 4 up, 4 in pgmap v8465: 196 pgs, 4 pools, 1001 MB data, 262 objects 7672 MB used, 74192 MB / 81865 MB avail 261/785 objects degraded (33.248%) 64 active+degraded 132 active+clean [/sourcecode] I started with marking the OSDs on the node out, and removing them. Note that I don’t need to stop the OSD (osd.1) since the node carrying osd.1 is down and not accessible. b) If not, you would’ve to stop the OSD using: [sourcecode language=“bash” gutter=“false”] # sudo service osd stop osd.1 [/sourcecode] c) Mark the OSD out, this is not ideally needed in this case since the node is already out. [sourcecode language=“bash” gutter=“false”] # ceph osd out osd.1 [/sourcecode] d) Remove the OSD from the CRUSH map, so that it does not receive any data. You can also get the crushmap, de-compile it, remove the OSD, re-compile, and upload it back. Remove item id 1 with the name ‘osd.1’ from the CRUSH map. [sourcecode language=“bash” gutter=“false”] # ceph osd crush remove osd.1 [/sourcecode] e) Remove the OSD authentication key [sourcecode language=“bash” gutter=“false”] # ceph auth del osd.1 [/sourcecode] f) At this stage, I had to remove the OSD host from the listing but was not able to find a way to do so. The ‘ceph-deploy’ didn’t have any tools to do this, other than ‘purge’, and ‘uninstall’. Since the node was not f) accessible, these won’t work anyways. A ‘ceph-deploy purge’ failed with the following errors, which is expected since the node is not accessible. [sourcecode language=“bash” gutter=“false”] # ceph-deploy purge hp-m300-9 [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.22-rc1): /usr/bin/ceph-deploy purge hp-m300-9 [ceph_deploy.install][INFO ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm [ceph_deploy.install][INFO ] like: librbd1 and librados2 [ceph_deploy.install][DEBUG ] Purging from cluster ceph hosts hp-m300-9 [ceph_deploy.install][DEBUG ] Detecting platform for host hp-m300-9 … ssh: connect to host hp-m300-9 port 22: No route to host [ceph_deploy][ERROR ] RuntimeError: connecting to host: hp-m300-9 resulted in errors: HostNotFound hp-m300-9 [/sourcecode] I ended up fetching the CRUSH map, removing the OSD host from it, and uploading it back. g) Get the CRUSH map [sourcecode language=“bash” gutter=“false”] # ceph osd getcrushmap -o /tmp/crushmap [/sourcecode] h) De-compile the CRUSH map [sourcecode language=“bash” gutter=“false”] # crushtool -d /tmp/crushmap -o crush_map [/sourcecode] i) I had to remove the entries pertaining to the host-to-be-removed from the following sections: a) devices b) types c) And from the ‘root’ default section as well. j) Once I had the entries removed, I went ahead compiling the map, and inserted it back. [sour","date":"07-05-2015","objectID":"/remove-a-host-from-ceph-cluster/:0:0","tags":["ceph","crush","crush-map","osd"],"title":"How to remove a host from a Ceph cluster?","uri":"/remove-a-host-from-ceph-cluster/"},{"categories":["ceph"],"content":"It can be really helpful to have a single command to list all the configuration settings in a monitor node, in a Ceph cluster. This is possible by interacting directly with the monitor’s unix socket file. This can be found under /var/run/ceph/. By default, the admin socket for the monitor will be in the path /var/run/ceph/ceph-mon..asok. The default location can vary in case you have defined it to be a different one, at the time of the installation. To know the actual socket path, use the following command: [sourcecode language=“bash” gutter=“false”] # ceph-conf –name mon.$(hostname -s) –show-config-value admin_socket [/sourcecode] This should print the location of the admin socket. In most cases, it should be something like /var/run/ceph/ceph-mon.$(hostname -s).asok Once you have the monitor admin socket, use that location to show the various configuration settings with: [sourcecode language=“bash” gutter=“false”] # ceph daemon /var/run/ceph/ceph-mon.*.asok config show [/sourcecode] The output would be long, and won’t fit in a single screen. You can either pipe it to ’less’ or grep for a specific value in case you know what you are looking for. For example, if I need to look at the ratio at which the OSD would be considered full, I’ll be using: [sourcecode language=“bash” gutter=“false”] # ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_full_ratio [/sourcecode] ","date":"06-05-2015","objectID":"/list-ceph-cluster-configurations/:0:0","tags":["admin_socket","ceph","ceph-admin-socket","ceph-conf","monitors","mons"],"title":"How to list all the configuration settings in a Ceph cluster monitor?","uri":"/list-ceph-cluster-configurations/"},{"categories":["techno"],"content":"The ‘cachefilesd’ kernel module will create two directories at the location specified in /etc/cachefilesd.conf. By default it’s /var/cache/fscache/. [root@montypython ~]# lsmod |grep -i cache cachefiles 40871 1 fscache 62354 3 nfs,cachefiles,nfsv4 Those are /var/cache/fscache/cache and /var/cache/fscache/graveyard. The cache structure is maintained inside ‘/var/cache/fscache/cache/’, while anything that is retired or culled is moved to ‘graveyard’. The ‘cachefilesd’ daemon monitors ‘graveyard’ using ‘dnotify’ and will delete anything that is in there. We’ll try an example. Consider an NFS share mounted with fscache support. The share contains the following files, with some random text. # ls /vol1 files1.txt files2.txt files3.txt files4.txt a) Configure ‘cachefiles’ by editing ‘/etc/cachefilesd.conf’, and start the ‘cachefilesd’ daemon. # systemctl start cachefilesd b) Mount the NFS share on the client with the ‘fsc’ mount option, to enable ‘fscache’ support. # sudo mount localhost:/vol1 /vol1-backup/ -o fsc d) Access the data from the mount point, and fscache will create the backed caching index at the location specified in /etc/cachefilesd.conf. By default, its /var/cache/fscache/ e) Once the files are accessed on the client side, fscache builds an index as following: NOTE: The index structure is dependent on the netfs (NFS in our case). The netfs driver can structure the cache index as it seems fit. Explanation of the caching structure: # tree /var/cache/fscache/ /var/cache/fscache/cache/ └── @4a └── I03nfs ├── @22 │ └── Jo00000008400000000000000000000000400 │ └── @59 │ └── J110000000000000000w080000000000000000000000 │ ├── @53 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @5e │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @61 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @62 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @70 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @7c │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ └── @e8 │ └── EE0g00sgwB-90600000000ww0000000000000000 └── @42 └── Jc000000000000EggDj00 └── @0a a) The ‘cache’ directory under /var/cache/fscache/ is a special index and can be seen as the root of the entire cache index structure. b) Data objects (actual cached files) are represented as files if they have no children, or folders if they have. If represented as a directory, data objects will have a file inside named ‘data’ which holds the data. c) The ‘cachefiles’ kernel module represents : i) ‘index’ objects as ‘directories’, starting with either ‘I’ or ‘J’. ii) Data objects are represented with filenames, beginning with ‘D’ or ‘E’. iii) Special objects are similar to data objects, and start with ‘S’ or ‘T’. In general, any object would be represented as a folder, if that object has children. g) In the directory hierarchy, immediately between the parent object and its child object, are directories named with *hash values* of the immediate child object keys, starting with an ‘@’. The child objects are placed inside this directory.These child objects would be folders, if it has child objects, or files if its the cached data itself. This can go on till the end of the path and reaches the file where the cached data is. Representation of the object indexes (For NFS, in this case) INDEX INDEX INDEX DATA FILES ========= ========== ================================= ================ cache/@4a/I03nfs/@30/Ji000000000000000–fHg8hi8400 cache/@4a/I03nfs/@30/Ji000000000000000–fHg8hi8400/@75/Es0g000w…DB1ry cache/@4a/I03nfs/@30/Ji000000000000000–fHg8hi8400/@75/Es0g000w…N22ry cache/@4a/I03nfs/@30/Ji000000000000000–fHg8hi8400/@75/Es0g000w…FP1ry ","date":"12-11-2014","objectID":"/structure-of-cached-content-in-fscache/:0:0","tags":["cachefilesd","cachefs","fscache"],"title":"FSCache and the on-disk structure of the cached data","uri":"/structure-of-cached-content-in-fscache/"},{"categories":["techno"],"content":"FS-Cache and CacheFS. Are there any differences between these two? Initially, I thought both were same. But no, it’s not. CacheFS is the backend implementation which caches the data onto the disk and mainpulates it, while FS-Cache is an interface which talks to CacheFS. So why do we need two levels here? FS-Cache was introduced as an API or front-end for CacheFS, which can be used by any file system driver. The file system driver talks with the FS-Cache API which inturn talks with CacheFS in the back-end. Hence, FS-Cache acts as a common interface for the file system drivers without the need to understand the backend CacheFS complexities, and how its implemented. The only drawback is the additional code that needs to go into each file system driver which needs to use FS-Cache. ie.. Every file system driver that needs to talk with FS-Cache, has to be patched with the support to do so. Moreover, the cache structure differs slightly between file systems using it, and thus lacks a standard. This unfortunately, prevents FS-Cache from being used by every network filesystem out there. The data flow would be as: VFS \u003c-\u003e File system driver (NFS/CIFS etc..) \u003c-\u003e FS-Cache \u003c-\u003e CacheFS \u003c-\u003e Cached data CacheFS need not cache every file in its entirety, it can also cache files partially. This partial caching mechanism is possible since FS-Cache caches ‘pages’ rather than an entire file. Pages are smaller fixed-size segments of data, and these are cached depending on how much the files are read initially. FS-Cache does not require an open file to be loaded in the cache, prior being accessed. This is a nice feature as far as I understand, and the reasons are: a) Not every open file in the remote file system can be loaded into cache, due to size limits. In such a case, only certain parts (pages) may be loaded. And the rest of the file should be accessed normally over the network. b) The cache won’t necessarily be large enough to hold all the open files on the remote system. c) Even if the cache is not populated properly, the file should be accessible. ie.. the cache should be able to be bypassed totally. This hopefully clears the differences between FS-Cache and CacheFS. ","date":"14-09-2014","objectID":"/fscache-and-cachefs-differences/:0:0","tags":["cachefs","fs-cache"],"title":"FS-Cache and CacheFS, what are the differences?","uri":"/fscache-and-cachefs-differences/"},{"categories":["techno"],"content":"I would be working on enabling FS-Cache support in the FUSE kernel module, as part of my under graduate project. Niels De Vos, from Red Hat Engineering, would act as my mentor and guide throughout this project. He would also be presenting this idea in the ‘Linux Plumbers Conference’ being held in Germany, October 2014. More details on the the talk can be seen at http://www.linuxplumbersconf.org/2014/ocw/sessions/2247 This feature has got quite a few requests from the FOSS world, and I’m glad I could work on this. For now, I’m trying to get a hold on FS-Cache, how it works with other file systems, and trying to build FUSE with some customizations. Ultimately, it would be the FUSE module were the code additions would go, not FS-Cache. I’ll try to keep this blog updated, so that I have a journal to refer later. ","date":"14-09-2014","objectID":"/fscache-and-fuse/:0:0","tags":["cachefilesd","fs-cache","fuse"],"title":"FS-Cache and FUSE","uri":"/fscache-and-fuse/"},{"categories":["techno"],"content":"I’ve been trying to create a minimal docker image for RHEL versions, for one of my projects. The following were the steps I followed: a) Installed a RHEL6.5 server with ‘Minimal Installation’. b) Registered it to the local satellite. c) Created a tar-ball of the filesystem with the command below: [sourcecode language=“bash” gutter=“false”] # tar –numeric-owner –exclude=/proc –exclude=/sys –exclude=/mnt –exclude=/var/cache --exclude=/usr/share/doc –exclude=/tmp –exclude=/var/log -zcvf /mnt/rhel6.5-base.tar.gz / [/sourcecode] d) Load the tar.gz image using ‘docker load’ (as per the man page of ‘docker load’) [sourcecode language=“bash” gutter=“false”] # docker load -i rhel6.5-base.tar.gz [/sourcecode] This is where it erred with the message: [sourcecode language=“bash” gutter=“false”] 2014/08/16 20:37:42 Error: open /tmp/docker-import-123456789/repo/bin/json: no such file or directory [/sourcecode] After a bit of searching and testing, I found that ‘docker load -i’ doesn’t work as expected. The workaround is to cat and pipe the tar.gz file, as shown below: [sourcecode language=“bash” gutter=“false”] # cat rhel6.5-base.tar.gz | docker import - rhel6/6.5 [/sourcecode] This ends up with the image showing up in ‘docker images’ [sourcecode language=“bash” gutter=“false”] # docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE rhel6/6.1 latest 32b4b345454a About a minute ago 1.251 GB [/sourcecode] Update: ‘docker load -i ’ would only work if the image is created as a layered docker image. If the is a tar ball created from a root filesystem, you would need to use ‘cat | docker import ’ ","date":"16-08-2014","objectID":"/error-open-tmpdocker/:0:0","tags":null,"title":"\"Error: open /tmp/docker-import-123456789/repo/bin/json: no such file or directory\"","uri":"/error-open-tmpdocker/"},{"categories":["techno"],"content":"**T**he binary ‘/sbin/lsusb’ in a chroot-ed environment have problems running properly. I have not checked this in a manually created chroot environment or using tools like ‘mock’. The scenario is as following : We were trying to check the output of ’lsusb’ in the %post section of a kickstart installation. I had specified ’noreboot’ in the kickstart file so the machine will wait for the user to manually reboot the machine. This helps to check the logs and the situation of the machine just after the installation finishes. After the installation and prior to the reboot, i checked in the second available terminal (Alt + F2) created by anaconda and was astonished to see that the command ’lsusb’ does not give us the required output but an error that ‘/usr/share/hwdata/usb.ids’ is not accessible or found. By default, i think only the ‘installation’ ie.. the %post section starts in a ‘chroot’ mode and the terminal available is not chroot-ed. So we will have to use ‘/mnt/sysimage/sbin/lsusb’. This didn’t work as expected since the ’lsusb’ binary needs to check the file ‘/usr/share/hwdata/usb.ids’ and won’t be able to find it. So I did a chroot from the second terminal and did an /sbin/lsusb, since /sbin in not in the ‘PATH’ by default. That too, didn’t work out. But this time it didn’t even complain anything. Just nothing at all, no output. Last time, at-least it complained it could not find something. So how do we go forward now ??? Here comes ‘strace’ to the rescue !!! strace is of-course a really nice tool to know what system calls are made and lots of internal stuff a binary will do while being executed. But ‘strace’ is not installed by default on a RHEL5 machine, which is the case here. As most of you would know, anaconda creates a virtual file system which consists of most of the folders found under a linux main /. The location where the OS is installed is mounted under /mnt/sysimage. Since we already have an ISO from where we have booted the machine from (DVD/CD), we are free to mount it on the filesystem, which is what we did. : # mkdir /mnt/source # mount -t iso9660 /dev/hdc /mnt/source # cd /mnt/source/Server/ In case you want to know how the DVD/CD drive is detected, all you need to do is execute ‘dmesg’ in the available terminal. ie.. after pressing ‘Alt + Ctrl + F2’. So we went forward and mounted the DVD to /mnt/source and changed the directory to /mnt/source/Server where all the rpm packages reside. Installed the package ‘strace’ using an ‘rpm -ivh’. Please note that we need to use ‘–root /mnt/sysimage’ as an option since we are installing the package to our newly installed file system which is at /mnt/sysimage. If this is not used, the installer will try to install the package to the virtual environment created in the memory. # cd /mnt/source/Server # rpm -ivh strace -fxvttT rpm --root /mnt/sysimage # cd # chroot /mnt/sysimage This will make /mnt/sysimage as the working root, ie.. where our installation was done. OK.. now for the ‘strace’ stuff. # strace -fxvto strace.log -s 1024 /sbin/lsusb The strace output will be saved to ‘strace.log’ which we can open up in a text editor of our choice. Opening it in ‘vi’, shows a lot of stuff such as the command run, the default language, location of libraries loaded, the environment variables etc.. In this case we would only need to be interested in the last parts, ie.. to know where the binary failed : 15:16:17 open(\"/dev/bus/usb\", O\\_RDONLY|O\\_NONBLOCK|O\\_DIRECTORY) = -1 ENOENT (No such file or directory) = 03067 15:16:17 open(\"/proc/bus/usb\", O\\_RDONLY|O\\_NONBLOCK|O\\_DIRECTORY) = 33067 15:16:17 fstat(3, {st\\_dev=makedev(0, 3), st\\_ino=4026532146, st\\_mode=S\\_IFDIR|0555, st\\_nlink=2, st\\_uid=0, st\\_gid=0, st\\_blksize=4096, st\\_blocks=0, st\\_size=0, st\\_atime=2009/09/25-15:16:17, st\\_mtime=2009/09/25-15:16:17, st\\_ctime=2009/09/25-15:16:17}) = 03067 15:16:17 fcntl(3, F\\_SETFD, FD\\_CLOEXEC) = 03067 15:16:17 getdents(3, {{d\\_ino=4026532146, d\\_off=1, d\\_reclen=24, d\\_name=\".\"} {d\\","date":"23-12-2010","objectID":"/lsusb-and-chroot-in-anaconda/:0:0","tags":["anaconda","installation","lsusb","rhel","strace","usb"],"title":"lsusb and chroot in anaconda.. Is usbfs mounted in anaconda %post installation ?","uri":"/lsusb-and-chroot-in-anaconda/"},{"categories":["techno"],"content":"What is device-mapper ? Device mapper is a modular driver for the linux kernel 2.6. It can be said as a framework which helps to create or map logical sectors of a pseudo block device to an underlying physical block device. So what device-mapper do is keep a table of mappings which equate the logical block devices to the physical block devices. Applications such as LVM2, EVMS, software raid aka dmraid, multipathing, block encryption mechanisms such as cryptsetup etc… use device-mapper to work. All these applications excluding EVMS use the libdevmapper library to communicate with device-mapper. The applications communicate with device-mapper’s API to create the mapping. Due to this feature, device-mapper does not need to know what LVM or dmraid is, how it works, what LVM metadata is, etc… It is upto the application to create the pseudo devices pointing to the physical volumes using one of device-mapper’s targets and then update the mapper table. The device-mapper mapping table : The mapping table used by device-mapper doesn’t take too much space and is a list created using a ‘btree’. A btree or a ‘Binary Search Tree’ is a data-structure from which data can be added, removed or queried. In order to know more on what a btree is and the concept behind it, read : http://en.wikipedia.org/wiki/Binary_search_tree http://en.wikipedia.org/wiki/B-tree Types of device-mapper targets : Applications which use device-mapper actually use one or more of its target methods to achieve their purpose. Targets can be said as a method or type of mapping used by device-mapper. The general mapping targets are : a) Linear - Used by linear logical volumes, ie.. the default data layout method used by LVM2. b) Striped - Used by striped logical volumes as well as software RAID0. c) Mirror - Used by software RAID1 and LVM mirroring. d) Crypt - Used by disk encryption utilties. e) Snapshot - Used to take online snapshots of block devices, an example is LVM snapshot. f) Multipath - Used by device-mapper-multipath. g) RAID45 - Software raid using device-mapper, ie.. dmraid h) Error - Sectors of the pseudo device mapped with this target causes the I/O to fail. There are a few more mappings such as ‘flaky’ which is not used much. I’ll write on how device-mapper works in LVM, in the next post… ","date":"22-12-2010","objectID":"/device-mapper-and-applications/:0:0","tags":null,"title":"Device Mapper and applications","uri":"/device-mapper-and-applications/"},{"categories":["techno"],"content":"In case anyone out there gets an error message like “Aborting. Failed to activate new LV to wipe the start of it.” while doing an ’lvcreate’, check (/etc/lvm/lvm.conf) once more. Most probably, a ‘volume_list’ would have been defined in there, which in turns want you to specify the ‘volume_list’ tag specified along with the lvcreate command. Excerpt from /etc/lvm/lvm.conf: # If volume_list is defined, each LV is only activated if there is a # match against the list. # vgname and vgname/lvname are matched exactly. # @tag matches any tag set in the LV or VG. # @* matches if any tag defined on the host is also set in the LV or VG # # volume_list = [ vg1, vg2/lvol1, @tag1, @* ] volume_list = [ VG01, @foo.com ] In this case, you will have to use the ’lvcreate’ command as follows, which will create the logical volume properly. # lvcreate –addtag @foo.com … following-options ","date":"02-11-2009","objectID":"/lvcreate-fails-with-error/:0:0","tags":null,"title":"lvcreate fails with the error \"Aborting. Failed to activate new LV to wipe the start of it.\". Why ??","uri":"/lvcreate-fails-with-error/"},{"categories":["techno"],"content":"From the output of the command ’lspci -n’ (The number after the colon, here ‘1679’ from the below snip) 0a:04.0 0200: 14e4:1679 (rev a3) Subsystem: 103c:703c Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- Status: Cap+ 66MHz+ UDF- FastB2B+ ParErr- DEVSEL=medium Latency: 64 (16000ns min), Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 138 Region 0: Memory at fdef0000 (64-bit, non-prefetchable) [size=64K] Region 2: Memory at fdee0000 (64-bit, non-prefetchable) [size=64K] IMPORTANT: ——————- In the above line “14e4:1679”, ‘14e4’ is the UID of the manufacturer and ‘1679’ is the card model or hardware ID. The actual way to proceed is to open the pci.ids file (’/usr/share/hwdata/pci.ids’ and ‘/lib/modules/`uname -r`/modules.pcimap’) and check for the manufacturer UID, like ‘14e4’ which is the ‘Broadcom Corporation’. The file /lib/modules/`uname -r`/modules.pcimap would be more reliable since it is from the modules of the loaded kernel. Under that, check the card model, like ‘1679’ which is ‘NetXtreme BCM5715S Gigabit Ethernet’. Under that you can also have subdivisions, so in order to pin-point a particular card you will have to use the ‘Subsystem’ value from ’lspci’. In this example, ‘Subsystem’ is 103c:703c, which turns out to be ‘NC326i PCIe Dual Port Gigabit Server Adapter’ ","date":"18-07-2008","objectID":"/map-pci-devices-in-linux/:0:0","tags":null,"title":"How to map PCI devices in Linux ?","uri":"/map-pci-devices-in-linux/"},{"categories":["programming"],"content":"A bash code snippet that helps to check if the installed bootloader is Grub or LILO. [code language=“bash”] #!/bin/bash A=`mount | awk ‘{print $1}’ | grep -n /dev/ | grep “1:” | cut -f2 -d “:” | cut -c 1-8` B=`mount | awk ‘{print $1}’ | grep -n /dev/ | grep “1:” | cut -f2 -d “:”` echo ; echo -e \" / mounted on $B \\n\"; dd if=$A bs=512 count=1 2\u003e\u00261 | grep GRUB \u003e /dev/null; if [ $? = 0 ] ; then echo -e “The installed bootloader is GRUB.\\n” ; fi dd if=$A bs=512 count=1 2\u003e\u00261 | grep LILO \u003e /dev/null; if [ $? = 0 ] ; then echo -e “The installed bootloader is LILO.\\n” ; fi [/code] ","date":"15-02-2008","objectID":"/bootloader-checker/:0:0","tags":["boot-loader-checker","grub","lilo"],"title":"Bootloader checker","uri":"/bootloader-checker/"},{"categories":["techno"],"content":"Adding Swap Space: Sometimes it is necessary to add more swap space after installation. For example, you may upgrade the amount of RAM in your system from 64 MB to 128 MB, but there is only 128 MB of swap space. It might be advantageous to increase the amount of swap space to 256 MB if you perform memory-intense operations or run applications that require a large amount of memory. You have two options: add a swap partition or add a swap file. It is recommended that you add a swap partition, but sometimes that is not easy if you do not have any free space available. To add a swap partition (assuming /dev/hdb2 is the swap partition you want to add): 1) The hard drive can not be in use (partitions can not be mounted, and swap space can not be enabled). The easiest way to achieve this it to boot your system in rescue mode. Refer to Chapter 8 for instructions on booting into rescue mode. When prompted to mount the filesystem, select Skip. Alternately, if the drive does not contain any partitions in use, you can unmount them and turn off all the swap space on the hard drive with the swapoff command. 2) Create the swap partition using parted or fdisk. Using parted is easier than fdisk; thus, only parted will be explained. To create a swap partition with ‘parted:’. At a shell prompt as root, type the command parted /dev/hdb, where /dev/hdb is the device name for the hard drive with free space. At the (parted) prompt, type print to view the existing partitions and the amount of free space. The start and end values are in megabytes. Determine how much free space is on the hard drive and how much you want to allocate for a new swap partition. At the (parted) prompt, type mkpartfs part-type linux-swap start end, where part-type is one of primary, extended, or logical, start is the starting point of the partition, and end is the end point of the partition. Warning Warning _________________________ Changes take place immediately; be careful when you type. Exit parted by typing quit. Now that you have the swap partition, use the command mkswap to setup the swap partition. At a shell prompt as root, type the following: # mkswap /dev/hdb2 To enable the swap partition immediately, type the following command: # swapon /dev/hdb2 To enable it at boot time, edit /etc/fstab to include: /dev/hdb2 swap swap defaults 0 0 The next time the system boots, it will enable the new swap partition. After adding the new swap partition and enabling it, make sure it is enabled by viewing the output of the command cat /proc/swaps or free. To add a swap file: ————————– 1. Determine the size of the new swap file and multiple by 1024 to determine the block size. For example, the block size of a 64 MB swap file is 65536. 2. At a shell prompt as root, type the following command with count being equal to the desired block size: # dd if=/dev/zero of=/swapfile bs=1024 count=65536 3. Setup the swap file with the command: # mkswap /swapfile 4. To enable the swap file immediately but not automatically at boot time: # swapon /swapfile 5. To enable it at boot time, edit /etc/fstab to include: /swapfile swap swap defaults 0 0 The next time the system boots, it will enable the new swap file. 6. After adding the new swap file and enabling it, make sure it is enabled by viewing the output of the command cat /proc/swaps or free. ","date":"15-02-2008","objectID":"/creating-swap-space-in-linux/:0:0","tags":["swap-space"],"title":"Creating a SWAP space in Linux","uri":"/creating-swap-space-in-linux/"},{"categories":["programming"],"content":"Most of the scripts presented in this journal have been created while learning bash and having nothing much to do… I think its usual to get crazy ideas and work trying to implement them, especially while learning any type of coding. This ‘File Counter’ script came as such a crazy idea. It was working at the time of its creation, but have not checked it recently.. should work.. This script counts the entire number of files irrespective the folders under the main directory you specify for this script to work on. ie.. It recursively counts the files under a directory tree. [code language=“bash”] #!/bin/bash # Counts the number of files recursively inside a directory # # echo ; clear echo -e “Please enter the directory location where you want the files to be counted…\\n” ; echo read dir ; echo ; if [ ! -d $dir ] ; then echo -e “The location you specified doesn’t exist.\\n” ; exit 0; else cd $dir ; echo -e “Please wait for the files to be counted…..\\n” ; echo ; fi X=`ls -l | wc -l` Y=`ls -l | grep ^d | awk ‘{print $9}’` B=`ls -l $Y | awk ‘{print $9}’ | grep . | wc -l ` A=`expr $X + $B` echo -e “There are a total of $A files inside the directory $dir…\\n” C=`ls -Rl | grep -v ./ | grep -v total | grep . | awk ‘{print $8}’` echo -e “Do you want to scan the directory for the file types?\\n” echo -e “Y/N\\n” ; read choice; if [ $choice = Y ] ; then cd $dir ; file $C | awk ‘{print $1,\"=======»\", $2}’ \u003e $HOME/Filetype.txt;echo -e “Output saved in file Filetype.txt.\\n” elif [ $choice = N ] ; then echo -e “Thankyou $USER, Take care….\\n” else echo ; echo -e “Invalid choice buddy…\\n” ; echo -e “Exiting…..Bye..\\n” ; fi [/code] ","date":"15-02-2008","objectID":"/file-counter/:0:0","tags":["file-counter"],"title":"Recursive file counter in bash","uri":"/file-counter/"},{"categories":["programming"],"content":"This is an extension or a rebuild of the previous chkrootkit install script, just used functions so its somewhat simplified…. ( Or is it ..? :) ) [code language=“bash”] #!/bin/bash DOWNLOAD_LOCATION=’/root/Downloads’ CHKROOTKIT_WGET=‘ftp://ftp.pangeia.com.br/pub/seg/pac/chkrootkit.tar.gz' RESULT_FILE=’/root/Server-Test.txt’ clear;echo chkrootkit-install () { while true; do echo -e “@@@@@@@@@@@@@@@@@@ CHK-ROOTKIT INSTALL/CHECK SCRIPT @@@@@@@@@@@@@@@@@@@@\\n” echo -e “Do you want to download and compile CHK-ROOTKIT [yes/no] ? : \\c” | tee -a $RESULT_FILE; read answer; echo $answer » $RESULT_FILE; case $answer in yes|YES) echo if [ ! -e $DOWNLOAD_LOCATION ]; then echo -e “$DOWNLOAD_LOCATION does not exist, creating…….\\n”;sleep 1s mkdir -p $DOWNLOAD_LOCATION; fi rm -rf $DOWNLOAD_LOCATION/chkrootkit* \u003e /dev/null; echo -e “Downloading CHK-ROOTKIT….\\n” | tee -a $RESULT_FILE;sleep 1s cd $DOWNLOAD_LOCATION \u0026\u0026 wget –progress=dot $CHKROOTKIT_WGET; if [ $? -eq 0 ] ; then echo -e “Download finished..\\n”; else echo -e “Sorry…Download Failed..!!!\\n”;exit;fi;echo echo -e “Unpacking and compiling CHK-ROOTKIT……….\\n”;sleep 2s cd $DOWNLOAD_LOCATION \u0026\u0026 tar -xvf chkrootkit*; mv $DOWNLOAD_LOCATION/chkrootkit*gz $DOWNLOAD_LOCATION/1-chkrootkit.tar.gz;sleep 2s cd $DOWNLOAD_LOCATION/chkrootki* \u0026\u0026 make sense \u003e /dev/null; if [ $? -eq 0 ] ; then echo -e “CHK-ROOTKIT compiled successfully..\\n”| tee -a $RESULT_FILE; break else echo -e “CHK-ROOTKIT compilation failed, Quiting….\\n” | tee -a $RESULT_FILE; exit fi ;; no|NO) echo echo -e “Ok..As you wish….Aborting.\\n”|tee -a $RESULT_FILE; exit ;; *) echo echo -e “Please enter either ‘yes’ OR ’no’..: \\c” ;; esac done } chkrootkit-run () { if [ -d $DOWNLOAD_LOCATION/chkrootki* ]; then while true; do echo -e “Do you want to run CHK-ROOTKIT now [yes/no] ? : \\c” | tee -a $RESULT_FILE; read reply echo $reply » $RESULT_FILE; case $reply in yes|YES) echo echo -e “Starting CHK-ROOTKIT….\\n” | tee -a $RESULT_FILE;sleep 2s;echo echo -e “—————-CHK-ROOTKIT SCAN RESULT—————–\\n” $DOWNLOAD_LOCATION/chkrootk*/chkrootkit | tee -a $RESULT_FILE;sleep 1s echo;echo -e “CHK-ROOTKIT check finished……\\n”;echo exit ;; no|NO) echo echo -e “DON’T FORGET TO RUN CHK-ROOTKIT PERIODICALLY.\\n” exit ;; *) echo echo -e “Please enter either ‘yes’ OR ’no’..: \\c” ;; esac done else echo -e “Chkrootkit not found in $DOWNLOAD_LOCATION, exiting..\\n” fi } chkrootkit-install \u0026\u0026 chkrootkit-run; echo -e “The result is saved in $RESULT_FILE for reference.\\n” [/code] ","date":"14-02-2008","objectID":"/chkrootkit-installer-with-functions/:0:0","tags":["functions"],"title":"CHKROOTKIT install script (with functions)","uri":"/chkrootkit-installer-with-functions/"},{"categories":["programming"],"content":"This bash script does a sanity check for the DNS domains defined inside /var/named. [code language=“bash”] #!/bin/bash A=`ls -l /var/named/*.db | awk ‘{print $9}’ | cut -f4 -d “/” | sed ’s/.db$//’` #domain names for i in $A; do named-checkzone $i /var/named/$i.db;done [/code] ","date":"14-02-2008","objectID":"/dns-zone-file-sanity-check/:0:0","tags":null,"title":"DNS Zone file sanity check","uri":"/dns-zone-file-sanity-check/"},{"categories":["programming"],"content":"This is a bash script which automates the installation of Nagios. There are more things to do such as setup of service monitoring, but that’s for another time. [code language=“bash”] #!/bin/bash DOWNLOAD_LOCATION=’/root/Downloads/’ NAGIOS_URL=‘http://jaist.dl.sourceforge.net/sourceforge/nagios/nagios-2.9.tar.gz' APACHE_CONF=’/etc/httpd/conf/httpd.conf’ NAGIOS_PLUGIN=‘http://nchc.dl.sourceforge.net/sourceforge/nagiosplug/nagios-plugins-1.4.8.tar.gz' NAGIOSHOME=’/usr/local/nagios’ DATE=`date +%d-%b-%Y` FILE=’/root/Nagios.txt’ ################################# # [1] Installing nagios # ################################# nagios_download () { clear if [ `id -u` -ne 0 ]; then echo -e “You are executing the script as $USER\\n” echo -e “You must be root to execute this script..\\n”; echo -e “Sorry…Exiting..\\n”;exit 111; else if [ ! -e /root/Nagios.txt ]; then touch /root/Nagios.txt; else mv /root/Nagios.txt /root/Nagios-$DATE.txt; touch /root/Nagios.txt; fi echo -e \" [@@@@@@@@@@@@@@@@@@@@@@@@@ NAGIOS INSTALL SCRIPT @@@@@@@@@@@@@@@@@@@@@@@@@]\\n\";sleep 1s echo -e \" …Welcome…\\n\"|tee -a $FILE;sleep 1s echo “[Starting the Nagios Installation Process :-]\"|tee -a $FILE; echo “———————————————\"|tee -a $FILE;echo;sleep 1s fi if [ ! -e $DOWNLOAD_LOCATION ]; then echo “$DOWNLOAD_LOCATION does not exist, creating…….\"|tee -a $FILE;sleep 1s mkdir -pv $DOWNLOAD_LOCATION;echo fi echo “[Downloading the nagios tar-ball to $DOWNLOAD_LOCATION :-]\"|tee -a $FILE; echo “——————————————————–\"|tee -a $FILE;echo;sleep 1s cd $DOWNLOAD_LOCATION \u0026\u0026 wget –progress=dot $NAGIOS_URL;echo echo -e “Extracting the archive….\\n”|tee -a $FILE;sleep 1s cd $DOWNLOAD_LOCATION \u0026\u0026 tar -zxf nagios*gz \u0026\u0026 mv nagios*gz Nagios-$DATE.tar.gz;echo } nagios_usercheck () { echo “[Checking the existence of user/group ’nagios’ :-]\"|tee -a $FILE; echo “————————————————–\"|tee -a $FILE; grep -q nagios /etc/group \u003e /dev/null if [ $? = 0 ];then echo “Group ’nagios’ exist”|tee -a $FILE; else echo “Adding group ’nagios’\"|tee -a $FILE; /usr/sbin/groupadd nagios fi grep -q nagios /etc/passwd \u003e /dev/null if [ $? = 0 ];then echo “User ’nagios’ exists”|tee -a $FILE; else echo “Adding user ’nagios’\"|tee -a $FILE; /usr/sbin/useradd -d $NAGIOSHOME -g nagios -s /bin/false -m nagios fi;echo echo “[Checking the existence of user/group ’nagcmd’ :-]\"|tee -a $FILE; echo “————————————————–\"|tee -a $FILE; grep -q nagcmd /etc/group; if [ $? = 0 ];then echo “Group ’nagcmd’ exists”|tee -a $FILE; else echo “Adding group ’nagcmd’\"|tee -a $FILE; /usr/sbin/groupadd nagcmd; fi grep -q nagcmd /etc/passwd; if [ $? = 0 ];then echo “User ’nagcmd’ exists”|tee -a $FILE; else echo “Adding user ’nagcmd’\"|tee -a $FILE; /usr/sbin/useradd -g nagcmd -s /bin/false -m nagcmd; fi; echo } nagios_previouscheck () { echo “[Checking for previous installations :-]\"|tee -a $FILE echo “—————————————-\"|tee -a $FILE;sleep 1s if [ -d /usr/local/nagios ]; then echo “Installation directory ‘/usr/local/nagios/’ already exist.\"|tee -a $FILE echo “Moving ‘/usr/local/nagios/’ to ‘/usr/local/Nagios-$DATE.back’\"|tee -a $FILE mv -v /usr/local/nagios /usr/local/Nagios-$DATE.back;echo echo “Creating the Installation Directory for Nagios [/usr/local/nagios/]\"|tee -a $FILE mkdir -pv /usr/local/nagios;echo else echo “Nagios installation not found at the default location of $NAGIOSHOME”; echo “Creating the Installation Directory for Nagios [/usr/local/nagios/]\"|tee -a $FILE mkdir -pv /usr/local/nagios;echo fi } nagios_ownership () { echo “[Setting appropriate ownership on the installation directory]\"|tee -a $FILE echo “————————————————————-” chown -v nagios.nagios /usr/local/nagios;echo;sleep 1s echo “[Checking the Web-Server user/group :-]\"|tee -a $FILE echo “—————————————\"|tee -a $FILE;sleep 1s echo “Web-Server User : `grep “^User” $APACHE_CONF|head -n1|awk ‘{print $2}’`\"|tee -a $FILE echo “Web-Server Group : `grep “^Group” $APACHE_CONF|head -n1|awk ‘{print $2}’`\"|tee -a $FILE;echo;sleep 1s echo “[Adding the Web-Server/Nagios user to the ’nagcmd’ group]\"|tee -a ","date":"14-02-2008","objectID":"/nagios-installer-script/:0:0","tags":["nagios-installation"],"title":"Nagios Installation Script","uri":"/nagios-installer-script/"},{"categories":["programming"],"content":"The bash environment variable ‘RANDOM’ is a pseudo-random number generator built in bash, and it can generate random numbers in the range of 0 - 32767. Using the command `echo $RANDOM`, we can generate a random number. Building a random number generator which emits a sequence of random numbers is pretty easy. [code language=“bash”] #!/bin/bash for i in `seq 1 10`: do echo $RANDOM; sleep 1s; done [/code] The ‘seq’ or the ‘sequential’ can be used to generate a sequence of numbers. ","date":"13-02-2008","objectID":"/bash-script-for-generating-random-number/:0:0","tags":["bash","random-number-generator"],"title":"A random number generator in Bash","uri":"/bash-script-for-generating-random-number/"},{"categories":["programming"],"content":"Some time back, I had to implement a password encryption section in one of my bash programs. It seemed easy to use a C snippet rather than doing it in bash. This was something I got after searching a while. [code language=“C”] #include stdlib.h #include unistd.h #include stdio.h #include crack.h #define DICTIONARY /usr/lib/cracklib_dict int main(int argc, char *argv[]) { char *password; char *problem; int status = 0; printf(\\nEnter an empty password or Ctrl-D to quit.\\n); while ((password = getpass(\\nPassword: )) != NULL *password ) { if ((problem = FascistCheck(password, DICTIONARY)) != NULL) { printf(Bad password: %s.\\n, problem); status = 1; } else { printf(Good password!\\n); } } exit(status); } [/code] Compile the code using the GNU C compiler. # gcc filename.c -lcrack -o cracktest' ","date":"13-02-2008","objectID":"/password-encryptor-in-c/:0:0","tags":["password-encrypt"],"title":"Password Encryptor in C","uri":"/password-encryptor-in-c/"},{"categories":["techno"],"content":"Why can’t I kill a process with the signal 9? A process can be sleeping in kernel code. Usually that’s because of faulty hardware or a badly written driver- or maybe a little of both. A device that isn’t set to the interrupt the driver thinks it is can cause this, for example- the driver is waiting for something its never going to get. The process doesn’t ignore your signal- it just never gets it. A zombie process doesn’t react to signals because it’s not really a process at all- it’s just what’s left over after it died. What’s supposed to happen is that its parent process was to issue a “wait()” to collect the information about its exit. If the parent doesn’t (programming error or just bad programming), you get a zombie. The zombie will go away if its parent dies- it will be “adopted” by init which will do the wait()- so if you see one hanging about, check its parent; if it is init, it will be gone soon, if not the only recourse is to kill the parent..which you may or may not want to do. Finally, a process that is being traced (by a debugger, for example) won’t react to the KILL either then you do a ps, processes that have a status of Z are called “zombies”. When people see a zombie process, the first thing they try to do is to kill the zombie, using kill or (horrors!) kill -9. This won’t work, however: you can’t kill a zombie, it’s already dead. When a process has already terminated (“died”) by receiving a signal to do so, it can stick around for a bit to finish up a few last tasks. These include closing open files and shutting down any allocated resources (memory, swap space, that sort of thing). These “housekeeping” tasks are supposed to happen very quickly. Once they’re completed, the final thing that a process has to do before dying is to report its exit status to its parent. This is generally where things go wrong. Each process is assigned a unique Process ID (PID). Each process also has an associated parent process ID (PPID), which identifies the process that spawned it (or PPID of 1, meaning that the process has been inherited bythe init process, if the parent has already terminated). While the parent is still running, it can remember the PID’s of all the children it has spawned. These PID’s can not be re-used by other (new) processes until the parent knows that the child process is done. When a child terminates and has completed its housekeeping tasks, it sends a one-byte status code to its parent. If this status code never gets sent, the PID is kept alive (in “zombie” status) in order to reserve its PID … the parent is waiting for the status code, and until it gets it, it doesn’t want any new processes to try and reuse that PID number for themselves. To get rid of a zombie, you can try killing its parent, which will temporarily orphan the zombie. The init process will inherent the zombie, and this might allow the process to finish terminating since the init process is always in a wait() state (ready to receive exit status reports of children). Generally, though, zombies clean themselves up. Whatever the process was waiting for eventually occurs and the process can report its exit status to its parent and all is well. If a zombie is already owned by init, though, and it’s still sticking around (like zombies are wont to do), then the process is almost certainly stuck in a device driver close routine, and will likely remain that way forever. You can reboot to clear out the zombies, but fixing the device driver is the only permanent solution. Killing the parent (init in this case) is highly unrecommended, since init is an extremely important process to keeping your system running.. ","date":"01-01-2008","objectID":"/zombie-processes/:0:0","tags":["runaway-process","zombie"],"title":"Zombie processes","uri":"/zombie-processes/"},{"categories":null,"content":" Obsidian ","date":"01-01-0001","objectID":"/writing-my-blog-article-in-obsidian/:0:0","tags":["blog","arvimal.github.io"],"title":"","uri":"/writing-my-blog-article-in-obsidian/"},{"categories":["programming","python"],"content":"_M_ethod Resolution Order or ‘MRO’ in short, denotes the way a programming language resolves a method or attribute. This post looks into how Method Resolution Order works, using Python. Python supports classes inheriting from other classes. The class being inherited is called the Parent/Super class, while the class that inherits is called the Child/Sub class. While inheriting from another class, the interpreter needs a way to resolve the methods that are being called via an instance. Hence a method resolution order is needed. Example 0: [code language=“python”] class A(object): def my_func(self): print(“Doing this in class A”) class B(A): def my_func(self): print(“Doing this in class B”) my_instance = B() my_instance.my_func() [/code] ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:0","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"Structure: We’ve two classes, class A and class B. Instantiate class B as my_instance. Call the my_func() method through the my_instance instance. Where is the method fetched from? From class B or class A? ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:1","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"How does the code work? This should be pretty obvious, the answer would be class B. But why is it being called from class B and not from class A? Answer : The Method Resolution Order [MRO]. To understand this in depth, let’s check another example: Example 1: [code language=“python”] class A(object): def my_func(self): print(“Doing this in Class A”) class B(A): pass class C(object): def my_func(self): print(“Doing this in Class C”) class D(B, C): pass my_instance = D() my_instance.my_func() [/code] ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:2","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"Structure: Four classes, class A, B, C, and D. Class D inherits from both B and C Class B inherits from A. Class A and C doesn’t inherit from any super classes, but from the object base class due to being new-style classes. Class A and class C both have a method/function named my_func(). Class D is instantiated through my_instance If we were to call the method my_func() through the my_instance() instance, which class would it be called from? Would it be from class A or class C? ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:3","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"How does the code work? This won’t be as obvious as Example 0. The instance my_instance() is created from class D. Since class Dinherits from both class B and C, the python interpreter searches for the method my_func() in both of these classes. The intrepreter finds that class B inherits from class A, and class C doesn’t have any super classes other than the default object class. Class A and class C both has the method named my_func(), and hence has to be called from one of these. Python follows a depth-first lookup order and hence ends up calling the method from class A. Following the depth-first Method Resolution Order, the lookup would be in the order : Class D -\u003e Class B -\u003e Class C Let’s check another example, which can be a bit more complex. Example 2: [code language=“python”] class A(object): def my_func(self): print(“Doing this in A”) class B(A): pass class C(A): def my_func(self): print(“doing this in C”) class D(B, C): pass my_instance = D() my_instance.my_func() [/code] ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:4","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"Structure: Four classes, class A, B, C, and D Class D inherits from both B and C Class B inherits from class A. Class C inherits from class A. Class A inherits from the default base class object. This sort of inheritance is called the Diamond Inheritance or the Deadly Diamond of death and looks like the following: Image courtsey : Wikipedia ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:5","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"How does the code work? Following the depth-first Method Resolution Order, the lookup would be in the order : Class D -\u003e Class B -\u003e Class A -\u003e Class C -\u003e Class A In order to avoid ambiguity while doing a lookup for a method where multiple classes are inherited and involved, the MRO lookup has changed slightly from Python 2.3 onwards. It still goes for the depth-first order, but if the occurrence of a class happens multiple times in the MRO path, it removes the initial occurrence and keeps the latter. Hence, the look up order in Example 2 becomes: Class D -\u003e Class B -\u003e Class C -\u003e Class A. NOTE: Python provides a method for a class to lookup the Method Resolution Order. Let’s recheck Example 2 using that. [code language=“python”] class A(object): def my_func(self): print(“Calling this from A”) class B(A): pass class C(A): def my_func(self): print(\"\\nCalling this from C\") class D(B, C): pass my_instance = D() my_instance.my_func() print(\"\\nPrint the Method Resolution Order\") print(D.mro()) print(D.__bases__) [/code] This should print: [code language=“python”] # python /tmp/Example-2.py Calling this from C Print the Method Resolution Order class ‘__main__.D’, class ‘__main__.B’, class ‘__main__.C’, class ‘__main__.A’, type ‘object’ (, ) [/code] ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:6","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"Takeaway Python follows a depth-first order for resolving methods and attributes. In case of multiple inheritances where the methods happen to occur more than once, python omits the first occurrence of a class in the Method Resolution Order. The \u003cclass\u003e.mro()methods helps to understand the Medthod Resolution Order. The `__bases__` and `__base__` magic methods help to understand the Base/Parent classes of a Sub/Child class. ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:7","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"},{"categories":["programming","python"],"content":"References https://en.wikipedia.org/wiki/Multiple_inheritance ","date":"01-01-0001","objectID":"/mro-object-oriented-programming/:0:8","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming","uri":"/mro-object-oriented-programming/"}]