[{"body":"","link":"https://arvimal.github.io/","section":"","tags":null,"title":""},{"body":"","link":"https://arvimal.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://arvimal.github.io/categories/personal/","section":"categories","tags":null,"title":"personal"},{"body":"","link":"https://arvimal.github.io/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"","link":"https://arvimal.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://arvimal.github.io/tags/testing/","section":"tags","tags":null,"title":"testing"},{"body":"Testing LoveIt Hugo theme Trying to revamp the blog with a beautiful theme - LoveIt.\n","link":"https://arvimal.github.io/posts/2022/08/testing-loveit/","section":"posts","tags":["testing"],"title":"Testing LoveIt"},{"body":"","link":"https://arvimal.github.io/archives/","section":"","tags":null,"title":""},{"body":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\nhttps://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","link":"https://arvimal.github.io/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://arvimal.github.io/tags/callable/","section":"tags","tags":null,"title":"callable"},{"body":"Introduction A callable is an object in Python that can be called / executed when called with parantheses ( ). Classes and functions are callable.\nCallables can be a class, a function, or an instance of a class. In simple terms, a class/function/instance/builtin is callable if it gets executed when called with parantheses ().\nExample 1: 1In [1]: help() 2Welcome to Python 3.6\u0026#39;s help utility! 3-- content omitted -- -------- 4 5In [2]: int() 6Out[2]: 0 7 8In [3]: 9callable(int) 10Out [3]: True 11 12In [4]: callable(help) 13Out [4]: True 14 15In [5]: def hello(): 16 print(\u0026#34;Howdy!!\u0026#34;) 17 18In [6]: hello() 19Out [6]: Howdy!! 20 21In [7]: callable(hello) 22Out [7]: True In Example 1, we can see that builtins like help(), a pre-defined type such as int(), and a custom function hello() are all callable. These can be executed while being called with parantheses.\nThe call() method The callable() builtin helps to determine if an object is callable or not. Internally, it translates to the magic method __call__().\nIn short:\nmy_object(*args) translates to my_object.__call__(*args)\nAll classes and functions are callable, as well as instances of classes with the __call__ magic method. An instance of a class/function is usually not callable (even though the class/function itself is), unless the class carries a __call__ magic method.\nie. An instance is callable only if the class it is instantiated from contains the __call__ magic method.\nThe inbuilt documentation on callable states: 1In [1]: print(callable.__doc__) 2Return whether the object is callable (i.e., some kind of function). Note that classes are callable, as are instances of classes with a call() method. [/code]\nExample 2: 1In [5]: def hello(): 2 ...: print(\u0026#34;Howdy!!\u0026#34;) 3 4In [6]: hello() 5Out [6]: Howdy!! 6 7In [7]: hello.__call__() 8Out [7]: Howdy!! 9 10In [8]: callable(hello) 11Out [8]: True Example 2 shows that a function when called with the parantheses (including any required arguments) is equivalent to calling the __call__() magic method. ie.. calling a function/class with parantheses translates to calling the __call__() magic method.\nNOTE: Read more on Magic methods in Python\nExample 3: Non-callable Instances [code language=\u0026quot;bash\u0026quot;]\nIn [1]: type(1) Out[1]: int\nIn [2]: callable(1) Out[2]: False\nIn [3]: x = 1\nIn [4]: type(x) Out[4]: int\nIn [5]: callable(int) Out[5]: True\nIn [6]: callable(x) Out[6]: False [/code] Example 3 above shows that even though the int class is callable, the instances created from the int class are not.\nRemember that instances will only be callable if the class from which it was instantiated contains a __call__ method. Inspecting the methods of class int reveals that it does not have a __call__ method.\nNOTE: You can view the methods of the int class using help(int) or dir(int).\nExample 4: Another example with Classes\n[code language=\u0026quot;bash\u0026quot;]\nIn [52]: class tell: ...: def call(self): ...: pass\nIn [53]: telling = tell()\nIn [54]: callable(tell) Out[54]: True\nIn [55]: callable(telling) Out[55]: True\nIn [56]: class test: ...: pass\nIn [57]: testing = test()\nIn [58]: callable(test) Out[58]: True\nIn [59]: callable(testing) Out[59]: False [/code] Since all classes are by default callable, both the classes tell and test in Example 4 are callable. But the instances of these classes necessarily need not be so. Since the class tell has the magic method __call__, the instance telling is callable. But the instance testing instantiated from the class test is not since the class does not have the magic method. Another set of examples.\nExample 5: Non-callable instance of a class [code language=\u0026quot;bash\u0026quot;]\nIn [1]: class new: ...: def foo(self): ...: print(\u0026quot;Hello\u0026quot;)\nIn [2]: n = new()\nIn [3]: n() ------------------ TypeError Traceback (most recent call last) in module() ----\u0026gt; 1 n()\nTypeError: 'new' object is not callable [/code]\nExample 6: Callable instance of the same class [code language=\u0026quot;bash\u0026quot;] In [4]: class new: ...: def call(self): ...: print(\u0026quot;I'm callable!\u0026quot;)\nIn [5]: n = new()\nIn [6]: n Out[6]: main.new at 0x7f7a614b1f98\nIn [7]: n() I'm callable! [/code] Example 5 and Example 6 shows how a class is itself callable, but unless it carries a __call__() method, the instances spawned out of it are not so.\nReferences: http://docs.python.org/3/library/functions.html#callable http://eli.thegreenplace.net/2012/03/23/python-internals-how-callables-work/ https://docs.python.org/3/reference/datamodel.html#object.__call__ ","link":"https://arvimal.github.io/posts/2017/08/callables-in-python/","section":"posts","tags":["callable","python","python-objects"],"title":"Callables in Python"},{"body":"","link":"https://arvimal.github.io/categories/programming/","section":"categories","tags":null,"title":"programming"},{"body":"","link":"https://arvimal.github.io/tags/python/","section":"tags","tags":null,"title":"python"},{"body":"","link":"https://arvimal.github.io/categories/python/","section":"categories","tags":null,"title":"python"},{"body":"","link":"https://arvimal.github.io/tags/python-objects/","section":"tags","tags":null,"title":"python-objects"},{"body":"","link":"https://arvimal.github.io/tags/kernel/","section":"tags","tags":null,"title":"kernel"},{"body":"","link":"https://arvimal.github.io/categories/linux/","section":"categories","tags":null,"title":"linux"},{"body":"","link":"https://arvimal.github.io/tags/module/","section":"tags","tags":null,"title":"module"},{"body":"Introduction _L_oadable Kernel Modules (LKM) are object code that can be loaded into memory, often used for supporting hardware or enable specific features.\nKernel modules enable the core kernel to be minimal and have features to be loaded as required.\nA kernel module is a normal file usually suffixed with .ko denoting it's a kernel object file. It contains compiled code from one or more source files, gets linked to the kernel when loaded, and runs in kernel space. It can dynamically adds functionality to a running kernel, without requiring a reboot.\nLinux kernel modules are written in C, and is compiled for a specific kernel version. This is the ideal practice since kernel data structures may change across versions, and using a module compiled for a specific version may break for another.\nSince kernel modules can be loaded and unloaded at will, it is pretty easy to unload an older version and load a newer one. This helps immensely in testing out new features since it is easy to change the source code, re-compile, unload the older version, load the newer version, and test the functionality.\nStructure Modules are expected to be under /lib/modules/$(uname -r)/ within directories specified according to use case.\n1[root@centos7 3.10.0-514.26.2.el7.x86_64] # ls -l 2total 2940 3lrwxrwxrwx. 1 root root 43 Jul 8 05:10 build -\u0026gt; /usr/src/kernels/3.10.0-514.26.2.el7.x86_64 4drwxr-xr-x. 2 root root 6 Jul 4 11:17 extra 5drwxr-xr-x. 12 root root 128 Jul 8 05:10 kernel 6-rw-r--r--. 1 root root 762886 Jul 8 05:11 modules.alias 7-rw-r--r--. 1 root root 735054 Jul 8 05:11 modules.alias.bin 8-rw-r--r--. 1 root root 1326 Jul 4 11:17 modules.block 9-rw-r--r--. 1 root root 6227 Jul 4 11:15 modules.builtin 10-rw-r--r--. 1 root root 8035 Jul 8 05:11 modules.builtin.bin 11-rw-r--r--. 1 root root 240071 Jul 8 05:11 modules.dep 12-rw-r--r--. 1 root root 343333 Jul 8 05:11 modules.dep.bin 13-rw-r--r--. 1 root root 361 Jul 8 05:11 modules.devname 14-rw-r--r--. 1 root root 132 Jul 4 11:17 modules.drm 15-rw-r--r--. 1 root root 110 Jul 4 11:17 modules.modesetting 16-rw-r--r--. 1 root root 1580 Jul 4 11:17 modules.networking 17-rw-r--r--. 1 root root 90643 Jul 4 11:15 modules.order 18-rw-r--r--. 1 root root 89 Jul 8 05:11 modules.softdep 19-rw-r--r--. 1 root root 350918 Jul 8 05:11 modules.symbols 20-rw-r--r--. 1 root root 432831 Jul 8 05:11 modules.symbols.bin 21lrwxrwxrwx. 1 root root 5 Jul 8 05:10 source -\u0026gt; build 22drwxr-xr-x. 2 root root 6 Jul 4 11:17 updates 23drwxr-xr-x. 2 root root 95 Jul 8 05:10 vdso 24drwxr-xr-x. 2 root root 6 Jul 4 11:17 weak-updates As we can see, there are several files that deals with the inter-dependencies of modules, which is used by modprobe to understand which modules to load before the one being actually requested to load.\nFor example:\nmodules.block lists the modules for block devices modules.networking lists the ones for network devices. modules.builtin lists the path of modules included in the kernel. modules.devname lists the ones that would be loaded automatically if a particular device is created. The kernel folder contains modules divided according to their use cases.\n1[root@centos7 3.10.0-514.26.2.el7.x86_64]# ls -l kernel/ 2total 16 3drwxr-xr-x. 3 root root 17 Jul 8 05:10 arch 4drwxr-xr-x. 3 root root 4096 Jul 8 05:10 crypto 5drwxr-xr-x. 67 root root 4096 Jul 8 05:10 drivers 6drwxr-xr-x. 26 root root 4096 Jul 8 05:10 fs 7drwxr-xr-x. 3 root root 19 Jul 8 05:10 kernel 8drwxr-xr-x. 5 root root 222 Jul 8 05:10 lib 9drwxr-xr-x. 2 root root 32 Jul 8 05:10 mm 10drwxr-xr-x. 33 root root 4096 Jul 8 05:10 net 11drwxr-xr-x. 11 root root 156 Jul 8 05:10 sound 12drwxr-xr-x. 3 root root 17 Jul 8 05:10 virt Each directory within kernel contains modules depending on the area they're used for.\nFor example, kernel/fs/ contains filesystem drivers.\n1[root@centos7 3.10.0-514.26.2.el7.x86_64]# ls -l kernel/fs 2total 48 3-rw-r--r--. 1 root root 21853 Jul 4 11:51 binfmt_misc.ko 4drwxr-xr-x. 2 root root 22 Jul 8 05:10 btrfs 5drwxr-xr-x. 2 root root 27 Jul 8 05:10 cachefiles 6drwxr-xr-x. 2 root root 21 Jul 8 05:10 ceph 7drwxr-xr-x. 2 root root 21 Jul 8 05:10 cifs 8drwxr-xr-x. 2 root root 23 Jul 8 05:10 cramfs 9drwxr-xr-x. 2 root root 20 Jul 8 05:10 dlm 10drwxr-xr-x. 2 root root 23 Jul 8 05:10 exofs 11drwxr-xr-x. 2 root root 21 Jul 8 05:10 ext4 12drwxr-xr-x. 2 root root 51 Jul 8 05:10 fat 13drwxr-xr-x. 2 root root 24 Jul 8 05:10 fscache 14rwxr-xr-x. 2 root root 36 Jul 8 05:10 fuse 15drwxr-xr-x. 2 root root 21 Jul 8 05:10 gfs2 16drwxr-xr-x. 2 root root 22 Jul 8 05:10 isofs 17drwxr-xr-x. 2 root root 21 Jul 8 05:10 jbd2 18drwxr-xr-x. 2 root root 22 Jul 8 05:10 lockd 19-rw-r--r--. 1 root root 19597 Jul 4 11:51 mbcache.ko 20drwxr-xr-x. 6 root root 128 Jul 8 05:10 nfs 21drwxr-xr-x. 2 root root 40 Jul 8 05:10 nfs_common 22drwxr-xr-x. 2 root root 21 Jul 8 05:10 nfsd 23drwxr-xr-x. 2 root root 4096 Jul 8 05:10 nls 24drwxr-xr-x. 2 root root 24 Jul 8 05:10 overlayfs 25drwxr-xr-x. 2 root root 24 Jul 8 05:10 pstore 26drwxr-xr-x. 2 root root 25 Jul 8 05:10 squashfs 27drwxr-xr-x. 2 root root 20 Jul 8 05:10 udf 28drwxr-xr-x. 2 root root 20 Jul 8 05:10 xfs depmod, and related commands Modules can export the features it carry, called symbols which can be used by other modules.\nIf module A depends on a symbol exported by module B, module B should be loaded first followed by module A.\ndepmod creates a list of symbol dependencies each module has, so that modprobe can go ahead and load the modules serving the symbols, prior loading the actual module.\ndepmod works by:\nCreating a list of symbols each module exports. Creating a list of symbol dependencies each module has. Dumping the list of symbols each module exports, to lib/modules/$(uname -r)/modules.symbols.bin and /lib/modules/$(uname -r)/modules.symbols Dumping the module dependency information to /lib/modules/$(uname -r)/modules.dep.bin and /lib/modules/$(uname -r)/modules.dep. Creating /lib/modules/$(uname -r)/modules.devnames which contains the device file information (device type, major:minor number) that gets created at boot for this module to function properly. NOTE:\nmodprobe refers /lib/modules/$(uname -r)/modules.dep.bin to understand the dependencies each module require. A human-readable version of this file is maintained at /lib/modules/$(uname -r)/modules.dep but modprobe does not refer this. The binary file modules.symbols.bin carry the symbols exported (if any) by each module, one symbol per line. A human-readable version of the same is kept at modules.symbols. A sneak peek into modules.symbols and modules.dep:\nmodules.symbols 1[root@centos7 3.10.0-514.26.2.el7.x86_64]# head modules.symbols 2# Aliases for symbols, used by symbol_request(). 3alias symbol:cfg80211_report_obss_beacon cfg80211 4alias symbol:drm_dp_link_train_channel_eq_delay drm_kms_helper 5alias symbol:__twofish_setkey twofish_common 6alias symbol:mlx4_db_free mlx4_core 7alias symbol:nf_send_unreach nf_reject_ipv4 8alias symbol:sdhci_remove_host sdhci 9alias symbol:videobuf_dma_init_kernel videobuf_dma_sg 10alias symbol:ar9003_paprd_is_done ath9k_hw 11alias symbol:cxgbi_ep_disconnect libcxgbi modules.dep 1[root@centos7 3.10.0-514.26.2.el7.x86_64]# head modules.dep 2kernel/arch/x86/kernel/cpu/mcheck/mce-inject.ko: 3kernel/arch/x86/kernel/test_nx.ko: 4kernel/arch/x86/kernel/iosf_mbi.ko: 5kernel/arch/x86/crypto/ablk_helper.ko: 6kernel/crypto/cryptd.ko 7kernel/arch/x86/crypto/glue_helper.ko: 8kernel/arch/x86/crypto/camellia-x86_64.ko: 9kernel/crypto/xts.ko 10kernel/crypto/lrw.ko 11kernel/crypto/gf128mul.ko 12kernel/arch/x86/crypto/glue_helper.ko 13kernel/arch/x86/crypto/blowfish-x86_64.ko: 14kernel/crypto/blowfish_common.ko 15kernel/arch/x86/crypto/twofish-x86_64.ko: 16kernel/crypto/twofish_common.ko 17kernel/arch/x86/crypto/twofish-x86_64-3way.ko: 18kernel/arch/x86/crypto/twofish-x86_64.ko 19kernel/crypto/twofish_common.ko 20kernel/crypto/xts.ko 21kernel/crypto/lrw.ko 22kernel/crypto/gf128mul.ko 23kernel/arch/x86/crypto/glue_helper.ko 24kernel/arch/x86/crypto/salsa20-x86_64.ko: lsmod is a parser that reads through /proc/modules and presents it in an easy-to-read format.\nNote how lsmod parse throug the content of /proc/modules below:\n1[root@centos7 3.10.0-514.26.2.el7.x86_64]# head /proc/modules 2test 12498 0 - Live 0xffffffffa0492000 (POE) 3binfmt_misc 17468 1 - Live 0xffffffffa048c000 4uhid 17369 0 - Live 0xffffffffa0486000 5ipt_MASQUERADE 12678 2 - Live 0xffffffffa0481000 6nf_nat_masquerade_ipv4 13412 1 7ipt_MASQUERADE, Live 0xffffffffa0451000 8xt_addrtype 12676 2 - Live 0xffffffffa044c000 9br_netfilter 22209 0 - Live 0xffffffffa0468000 10dm_thin_pool 65565 1 - Live 0xffffffffa046f000 11dm_persistent_data 67216 1 12dm_thin_pool, Live 0xffffffffa0456000 13dm_bio_prison 15907 1 14dm_thin_pool, Live 0xffffffffa043f000 15 16[root@centos7 3.10.0-514.26.2.el7.x86_64]# lsmod | head 17Module Size Used by 18test 12498 0 19binfmt_misc 17468 1 20uhid 17369 0 21ipt_MASQUERADE 12678 2 22nf_nat_masquerade_ipv4 13412 1 23ipt_MASQUERADE xt_addrtype 12676 2 24br_netfilter 22209 0 25dm_thin_pool 65565 1 26dm_persistent_data 67216 1 dm_thin_pool NOTE:\nThe first field lists the module name. The second field lists the size of the module in memory. The third field lists the number of times the module is in use. `0` means the module is not used despite it being loaded. The fourth field lists the modules which uses this module as their dependency. Creating a dummy module The steps for creating a kernel module includes:\nWriting the module file. Writing the Makefile for the module. Compile the module file using make , which will refer the Makefile to build it. The module file and its corresponding Makefile are put in a separate directory so as to keep the kernel module directory clean. Once the module code and the Makefile are ready, the make command is used to build the module, $(PWD) being the directory with the module code and Makefile.\n1# make -C /lib/modules/$(uname -r)/build M=$PWD modules The make command above does the following:\nChange to the path mentioned after -C, ie.. to the location where the kernel Makefile is present. (/lib/modules/$(uname -r)/build/) Use the kernel Makefile's macro M which denotes the location from which the code should be compiled, ie.. in this case, the PWD where the module code/Makefile is present. Use the target modules which tells make to build the module. make is trying to build a module in the current working directory, using the kernel Makefile at /lib/modules/$(uname -r)/build/Makefile\nIf we have a module file named test.c and its corresponding Makefile in $(PWD), the make command would follow the steps below:\nmake calls the modules target and refers to the kernel Makefile. The kernel Makefile looks for the module Makefile in $PWD. The kernel Makefile read the module's Makefile and gets a list of the objects assigned to the macro obj-m. The make command builds modules for each object assigned to the macro obj-m. Writing a simple module The following is a very simple module, which prints a message while loading, and another one while unloading.\n1int test_module(void) { 2 printk(\u0026#34;Loading the test module!\\\\n\u0026#34;); 3 return 0; } 4 5void unload_test(void) { 6 printk(\u0026#34;Unloading the test module!\\\\n\u0026#34;); 7 } 8 9module_init(test_module) 10module_exit(unload_test) This has two functions, test_module() and unload_test() which simply prints a text banner upon loading and unloading respectively.\nmodule_init() is used to load the module, and can call whatever functions that needs to initialize the module. We load our test_module() function into module_init() so that it gets initialized when the module is loaded.\nmodule_exit() is called whenever the module has to be unloaded, and it can take in whatever functions are required to do a proper cleanup (if required) prior the module being unloaded. We load our unload_test() function in module_exit().\nWriting a Makefile Since the kernel Makefile will look in for the obj-m macro in the module Makefile with the object filename as its argument, add the following in the Makefile:\n1obj-m := test.o make will create an object file test.o from test.c, and then create a kernel object file test.ko.\nCompiling the module with make Let's compile the module\n1[root@centos7 test]# pwd 2/lib/modules/3.10.0-514.26.2.el7.x86_64/test 3 4[root@centos7 test]# ls 5Makefile test.c 6 7[root@centos7 test]# make -C /lib/modules/$(uname -r)/build M=$PWD modules 8make: Entering directory `/usr/src/kernels/3.10.0-514.26.2.el7.x86_64\u0026#39; 9CC \\[M\\] /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.o Building modules, stage 2. MODPOST 1 modules 10CC /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.mod.o 11LD \\[M\\] /lib/modules/3.10.0-514.26.2.el7.x86_64/test/test.ko 12make: Leaving directory \\`/usr/src/kernels/3.10.0-514.26.2.el7.x86_64\u0026#39; Listing the contents show lot of new files, including the module code, the Makefile, the object file test.o created from test.c, the kernel object file test.ko.\ntest.mod.c contains code which should be the one ultimately being built to test.ko, but that should be for another post since much more is yet to be read/learned on what's happening there.\n1[root@centos7 test]# ls -l 2total 292 3-rw-r--r--. 1 root root 16 Jul 27 11:52 Makefile 4-rw-r--r--. 1 root root 60 Jul 27 11:57 modules.order 5-rw-r--r--. 1 root root 0 Jul 27 11:57 Module.symvers 6-rw-r--r--. 1 root root 281 Jul 27 11:53 test.c 7-rw-r--r--. 1 root root 137768 Jul 27 11:57 test.ko 8-rw-r--r--. 1 root root 787 Jul 27 11:57 test.mod.c 9-rw-r--r--. 1 root root 52912 Jul 27 11:57 test.mod.o 10-rw-r--r--. 1 root root 87776 Jul 27 11:57 test.o Loading/Unloading the module Loading and unloading the module should print the messages passed via printk in dmesg.\n1[root@centos7 test]# insmod ./test.ko 2 3[root@centos7 test]# lsmod | grep test 4test 12498 0 5 6[root@centos7 test]# rmmod test Checking dmesg shows the informational messages in the module code:\n1[root@centos7 test]# dmesg | tail 2[35889.187282] test: loading out-of-tree module taints kernel. 3[35889.187288] test: module license \u0026#39;unspecified\u0026#39; taints kernel. 4[35889.187290] Disabling lock debugging due to kernel taint 5[35889.187338] test: module verification failed: signature and/or required key missing - tainting kernel 6[35889.187548] Loading the test module! 7[35899.216954] Unloading the test module! Note the messages about the module test tainting the kernel.\nRead more on how a module can taint the kernel, at https://www.kernel.org/doc/html/latest/admin-guide/tainted-kernels.html.\nMore on customizing the Makefile in another post.\n","link":"https://arvimal.github.io/posts/2017/07/writing-a-minimalistic-kernel-module/","section":"posts","tags":["kernel","module"],"title":"Writing a minimalistic kernel module in Linux - Part 1"},{"body":"","link":"https://arvimal.github.io/tags/algorithms/","section":"tags","tags":null,"title":"algorithms"},{"body":"","link":"https://arvimal.github.io/categories/data-structures-and-algorithms/","section":"categories","tags":null,"title":"data-structures-and-algorithms"},{"body":"","link":"https://arvimal.github.io/tags/programming/","section":"tags","tags":null,"title":"programming"},{"body":"","link":"https://arvimal.github.io/tags/recursion/","section":"tags","tags":null,"title":"recursion"},{"body":"_R_ecursion is a technique by which a function calls itself until a condition is met.\nIntroduction Loops or repetitive execution based on certain conditions are inevitable in programs. Usual loops include if, while and for loops. Recursion is an entirely different way to deal with such situations, and in many cases, easier.\nRecursion is a when a function calls itself in each iteration till a condition is met. Ideally, the data set in each iteration gets smaller until it reach the required condition, after which the recursive function exists.\nA typical example of recursion is a factorial function.\nHow does Recursion work? A recursive function ideally contains a Base case and a Recursive case.\nA Recursive case is when the function calls itself, until the Base case is met. Each level of iteration in the Recursive case moves the control to the next level.\nOnce a specific level finishes execution, the control is passed back to the previous level of execution. A Recursive function can go several layers deep until the Base condition is met. In short, a Recursive case is a loop in which the function calls itself.\nThe Base case is required so that the function doesn't continue running in the Recursive loop forever. Once the Base case is met, the control moves out of the Recursive case, executes the conditions in the Base case (if any), and exits.\nAs mentioned in the Introduction, a factorial function can be seen as an example of recursion.\nNOTE: The Base case for a factorial function is when n == 1\nConsider n!:\nn! can be written as:\nn x (n - 1) x (n - 2) x (n - 3) x .... x 1\nn! can also be represented as:\n[code language=\u0026quot;bash\u0026quot;] n! = n * (n - 1)! ---\u0026gt; [Step 1] (n - 1)! = (n - 1) * (n - 2)! ---\u0026gt; [Step 2] (n - 2)! = (n - 2) * (n - 3)! ---\u0026gt; [Step 3] . .. ... (n - (n - 1)) = 1 ---\u0026gt; [Base case] [/code]\nEach level/step is a product of a value and all the levels below it. Hence, Step 1 will end up moving to Step 2 to get the factorial of elements below it, then to Step 3 and so on.\nie.. the control of execution move as:\n[Step 1] -\u0026gt; [Step 2] -\u0026gt; [Step 3] -\u0026gt; ..... [Step n]\nIn a much easier-to-grasp example, a 5! would be:\n[code language=\u0026quot;bash\u0026quot;] 5! = 5 * 4! ---\u0026gt; [Step 1] 4! = 4 * 3! ---\u0026gt; [Step 2] 3! = 3 * 2! ---\u0026gt; [Step 3] 2! = 2 * 1! ---\u0026gt; [Step 4] 1! = 1 ---\u0026gt; [Step 5] / [Base case] [/code]\nThe order of execution will be :\n[Step 1] -\u0026gt; [Step 2] -\u0026gt; [Step 3] -\u0026gt; [Step 4] -\u0026gt; [Step 5]\nAs we know, in Recursion, each layer pause itself and pass the control to the next level. Once it reach the end or the Base case, it returns the result back to the previous level one by one until it reaches where it started off.\nIn this example, once the control of execution reaches Step 5 / Base case , the control is returned back to its previous level Step 4 . This level returns the result back to Step 3 which completes its execution and returns to Step 2 , so on and so forth until it reach Step 1 .\nThe return control flow would be as:\n[Base case / Step 5] -\u0026gt; [Step 4] -\u0026gt; [Step 3] -\u0026gt; [Step 2] -\u0026gt; [Step 1] -\u0026gt; Result.\nThis can be summed up using an awesome pictorial representation, from the book Grokking Algorithms by Adit. Please check out the References section for the link for more information about this awesome book.\nFigure 1: Recursion, Recursive case and Base case (Copyright Manning Publications, drawn by adit.io)\nCode Example 1: A factorial function in a while loop [code language=\u0026quot;python\u0026quot;] def fact(n): factorial = 1 while n \u0026gt; 1: factorial = factorial * n n = n - 1 return factorial\nprint(\u0026quot;Factorial of {0} is {1}\u0026quot;.format(10, fact(10))) print(\u0026quot;Factorial of {0} is {1}\u0026quot;.format(20, fact(20))) [/code]\nThe same function above, in a recursive loop [code language=\u0026quot;python\u0026quot;] def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1)\nprint(\u0026quot;Factorial of {0} is {1}\u0026quot;.format(10, factorial(10))) print(\u0026quot;Factorial of {0} is {1}\u0026quot;.format(20, factorial(20))) [/code]\nExample 2: A function to sum numbers in a normal for loop. [code language=\u0026quot;python\u0026quot;] def my_sum(my_list): num = 0 for i in my_list: num += i return num\nprint(my_sum([10, 23, 14, 12, 11, 94, 20])) [/code]\nThe same function to add numbers, in a recursive loop [code language=\u0026quot;python\u0026quot;] def my_sum(my_list): if my_list == []: return 0 else: return my_list[0] + my_sum(my_list[1:])\nprint(my_sum([10, 23, 14, 12, 11, 94, 20])) [/code]\nCode explanation Both Example 1 and Example 2 are represented as an iterative function as well as a recursive function.\nThe iterative function calls the next() function on the iterator sum.__iter__() magic method iterate over the entire data set. The recursive function calls itself to reach a base case and return the result.\nObservations: While a recursive function does not necessarily give you an edge on performance, it is much easier to understand and the code is cleaner.\nRecursion has a disadvantage though, for large data sets. Each loop is put on a call stack until it reaches a Base case. Once the Base case is met, the call stack is rewound back to reach where it started, executing each of the previous levels on the way. The examples above showed a sum function and a factorial function. In large data sets, this can lead to a large call stack which in turns take a lot of memory.\nReferences: Grokking Algorithms Data Structures and Algorithms in Python ","link":"https://arvimal.github.io/posts/2017/06/recursion-algorithm-study/","section":"posts","tags":["algorithms","programming","python","recursion"],"title":"Recursion - Algorithm Study"},{"body":"Selection Sort is a sorting algorithm used to sort a data set either in incremental or decremental order.\nIt goes through the entire elements one by one and hence it's not a very efficient algorithm to work on large data sets.\n1. How does Selection sort work? Selection sort starts with an unsorted data set. With each iteration, it builds up a sub dataset with the sorted data.\nBy the end of the sorting process, the sub dataset contains the entire elements in a sorted order.\nIterate through the data set one element at a time. Find the biggest element in the data set (Append it to another if needed) Reduce the sample space by the biggest element just found. The new data set becomes n - 1 compared to the previous iteration. Start over the iteration again, on the reduced sample space. Continue till we have a sorted data set, either incremental or decremental 2. How does the data sample change in each iteration? Consider the data set [10, 4, 9, 3, 6, 19, 8]\nData set - [10, 4, 9, 3, 6, 19, 8]\nAfter Iteration 1 - [10, 4, 9, 3, 6, 8] - [19] After Iteration 2 - [4, 9, 3, 6, 8] - [10, 19] After Iteration 3 - [4, 3, 6, 8] - [9, 10, 19] After Iteration 4 - [4, 3, 6] - [8, 9, 10, 19] After Iteration 5 - [4, 3] - [6, 8, 9, 10, 19] After Iteration 6 - [3] - [4, 6, 8, 9, 10, 19] After Iteration 7 - [3, 4, 6, 8, 9, 10, 19] Let's check what the Selection Sort algorithm has to go through in each iteration.\nIter 1 - [10, 4, 9, 3, 6, 8] Iter 2 - [4, 9, 3, 6, 8] Iter 3 - [4, 3, 6, 8] Iter 4 - [4, 3, 6] Iter 5 - [4, 3] Iter 6 - [3] Sorted Dataset - [3, 4, 6, 8, 9, 10, 19]\n3. Performance / Time Complexity Selection Sort has to go through all the elements in the data set, no matter what. Hence, the Worst case, Best case and Average Time Complexity would be O(n^2). Since Selection Sort takes in n elements while starting, and goes through the data set n times (each step reducing the data set size by 1 member), the iterations would be:\n1n + [ (n - 1) + (n - 2) + (n - 3) + (n - 4) + ... + 2 + 1 ] We are more interested in the worse-case scenario. In a very large data set, an n - 1, n - 2 etc.. won't make a difference.\nHence we can re-write the above iterations as:\n1n + [n + n + n + n ..... n] Or also as:\n1n * n = (n^2) 4. Code 1def find_smallest(my_list): 2 smallest = my_list[0] 3 smallest_index = 0 4 5for i in range(1, len(my_list)): 6 if my_list[i] \u0026lt; smallest: 7 smallest = my_list[i] 8 smallest_index = i 9 return smallest_index 10 11def selection_sort(my_list): 12 new_list = [] 13 for i in range(len(my_list)): 14 smallest = find_smallest(my_list) 15 new_list.append(my_list.pop(smallest)) 16 return new_list[code] 5. Observations Selection Sort is an algorithm to sort a data set, but it is not particularly fast. It takes n iterations in each step to find the biggest element in that iteration. The next iteration has to run on a data set of n - 1 elements compared to the previous iteration. For n elements in a sample space, Selection Sort takes n x n iterations to sort the data set. 6. References https://en.wikipedia.org/wiki/Selection_sort http://bigocheatsheet.com https://github.com/egonschiele/grokking_algorithms ","link":"https://arvimal.github.io/posts/2017/02/selection-sort-algorithm-study/","section":"posts","tags":["algorithms","python"],"title":"Selection Sort - Algorithm Study"},{"body":"Introduction **B**inary Search is a search method used to find an object in a data set. This is much faster compared to the Linear Search algorithm we saw in a previous post.\nThis algorithm works on the Divide and Conquer principle. Binary Search gets its speed by essentially dividing the list/array in half in each iteration, thus reducing the dataset size for the next iteration.\nImagine searching for an element in a rather large dataset. Searching for an element one by one using Linear Search would take n iterations. In a worst case scenario, if the element being searched is not present in the dataset or is at the end of the dataset, the time taken to find the object/element would be proportional to the size of the dataset.\nThe element of interest is returned if it is present in the dataset, else a NULL/None value is.\nNote: Binary search will only work effectively on a Sorted collection. The code implementation will need minor changes depending on how the dataset is sorted, ie.. either in an increasing order or in a decreasing order. Performance 1. Worst-case performance: log(n) As discussed in the post on, Linear Search a worst-case analysis is done with the upper bound of the running time of the algorithm. ie.. the case when the maximum number of operations are needed/executed to find/search the element of interest in the dataset.\nOf course, the worst-case scenario for any search algorithms is when the element of interest is not present in the dataset. The maximum number of searches has to be done in such a case, and it still ends up with no fruitful result. A similar but less worse case is when the element is found in the final (or somewhere near the last) iteration.\nDue to the divide-and-conquer method, the maximum number of iterations needed for a dataset of n elements is, log(n) where the log base is 2.\nHence, for a data set of 10240 elements, Binary Search takes a maximum of 13 iterations.\n[code language=\u0026quot;python\u0026quot;] In [1]: import math\nIn [2]: math.log(10240, 2) Out[2]: 13.321928094887364 [/code] For a data set of 50,000 elements, Binary Search takes 16 iterations in the worst case scenario while a Linear Search may take 50,000 iterations in a similar case.\n[code language=\u0026quot;python\u0026quot;] In [1]: import math\nIn [2]: math.log(50000, 2) Out[2]: 15.609640474436812 [/code] ie.. the Worst case for Binary search takes log(n) iterations to find the element.\n2. Best-case performance: O(1) The best case scenario is when the element of interest is found in the first iteration itself. Hence the best-case would be where the search finishes in one iteration.\nie.. The best-case scenario would be O(1).\nHow does Binary Search work? Imagine a sorted dataset of 100 numbers and we're searching for 98 is in the list. A simple search would start from index 0 , moves to the element at index 1, progresses element by element until the one in interest is found. Since we're searching for 98, it'll take n iterations depending on the number of elements between the first element in the dataset and 98.\nBinary Search uses the following method, provided the dataset is sorted.\nFind the length of the data set. Find the lowest (index 0), highest (index n), and the middle index of the dataset. Find the subsequent elements residing in the first, last, and middle index. Check if the element of interest is the middle element. If not, check if the element-of-interest is higher or lower than the middle element. If it is higher, assuming the dataset is sorted in an increasing order, move the lower index to one above the middle index. if it is lower, move the highest index to one below the middle index. Check if the element of interest is the middle element in the new/shorter dataset. Continue till the element of interest is found. [caption id=\u0026quot;attachment_2310\u0026quot; align=\u0026quot;alignnone\u0026quot; width=\u0026quot;1280\u0026quot;] Binary Search - Source: Wikipedia[/caption]\nThe figure above shows how Binary Search works on a dataset of 16 elements, to find the element 7.\nIndex 0 , Index 16, and the middle index are noted. Subsequent values/elements at these indices are found out as well. Check if the element of interest 7 is equal to, lower, or higher than the middle element 14 at index 8. Since it's lower and the dataset is sorted in an increasing order, the search moves to the left of the middle index, ie.. from index 0 to index 7. ---- The lower index is now 0, the higher index is now 7, and the middle index is now 3, the element in the middle index is 6. Check if the element of interest 7 is lower or higher than the middle element 6 at index 3. Since it's higher and the dataset is sorted in an increasing order, the search moves to the right of the middle index, ie.. from index 4 to index 7. ---- So on and so forth.. till we arrive at the element of interest, ie.. 7. As noted earlier, the data set is divided into half in each iteration. A numeric representation on how Binary search progress can be seen as:\n100 elements -\u0026gt; 50 elements -\u0026gt; 25 elements -\u0026gt; 12 elements -\u0026gt; 6 elements - 3 elements -\u0026gt; 1 element\nCode Example 1 : (Data set sorted in Increasing order) [code language=\u0026quot;python\u0026quot;] def binary_search(my_list, item): low_position = 0 high_position = len(my_list) - 1\nwhile low_position = high_position: mid_position = (low_position + high_position) // 2 mid_element = my_list[mid_position]\nif mid_element == item: print(\u0026quot;\\nYour search item {0} is at index {1}\u0026quot;.format( item, mid_position)) return mid_element\nelif mid_element \u0026lt;= item: high_position = mid_position - 1\nelse: low_position = mid_position + 1 return None\nif __name__ == \u0026quot;__main__\u0026quot;: my_list = [1, 2, 3, 4, 5, 6] binary_search(my_list, 3) [/code]\nExample 2 : (Same as above, with statements on how the search progresses) [code language=\u0026quot;python\u0026quot;] def binary_search(my_list, item):\n# Find and set the low and high positions of the data set # Note that these are not the values, but just positions. low_position = 0 high_position = len(my_list) - 1\n# Calculate the Complexity import math complexity = math.ceil(math.log(len(my_list), 2))\n# Print some info on the dataset print(\u0026quot;\\nDataset size : {0} elements\u0026quot;.format(len(my_list))) print(\u0026quot;Element of interest : {0}\u0026quot;.format(item)) print(\u0026quot;Maximum number of iterations to find {0} : {1}\\n\u0026quot;.format( item, complexity))\nwhile low_position \u0026lt;= high_position:\n# Find the middle position from the low and high positions mid_position = (low_position + high_position) // 2\n# Find the element residing in the middle position of the data set. mid_element = my_list[mid_position]\nprint(\u0026quot;Element at min index : {0}\u0026quot;.format(my_list[low_position])) print(\u0026quot;Element at max index : {1}\u0026quot;.format(high_position, my_list[high_position])) print(\u0026quot;Element at mid index {0} : {1}\u0026quot;.format(mid_position, mid_element))\nif mid_element == item: print(\u0026quot;\\nYour search item {0} is at index {1}\u0026quot;.format( item, mid_position)) return mid_element\nelif mid_element \u0026gt; item: high_position = mid_position - 1 print(\u0026quot;{0} in the left subset, omitting the right subset\\n\u0026quot;.format(item))\nelse: low_position = mid_position + 1 print(\u0026quot;{0} in the right subset, omitting the left subset\\n\u0026quot;.format(item))\nprint(\u0026quot;Element of interest not in dataset\\n\u0026quot;) return None\nif __name__ == \u0026quot;__main__\u0026quot;: my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] binary_search(my_list, 13) [/code]\nObservations: Binary Search needs a Sorted dataset to work, either increasing or decreasing. It finds the element of interest in logarithmic time, hence is also known as, Logarithmic Search. Binary Search searches through a dataset of n elements in log(n) iterations, in the worst case scenario. NOTE: All the examples used in this blog are available at https://github.com/arvimal/DataStructures-and-Algorithms-in-Python, with more detailed notes.\nReferences: https://en.wikipedia.org/wiki/Binary_search_algorithm http://quiz.geeksforgeeks.org/binary-search/ https://www.hackerearth.com/practice/algorithms/searching/binary-search/tutorial/ http://research.cs.queensu.ca/home/cisc121/2006s/webnotes/search.html ","link":"https://arvimal.github.io/posts/2017/01/binary-search-algorithm-study/","section":"posts","tags":null,"title":"Binary Search - Algorithm Study"},{"body":"autoauto- [Introduction](#introduction)auto- [Performance](#performance)auto - [1\\. Worst-case performance: O(n)](#1\\-worst-case-performance-on)auto - [2\\. Best-case performance: O(1)](#2\\-best-case-performance-o1)auto - [3\\. Average performance: O(n/2)](#3\\-average-performance-on2)auto - [Observations:](#observations)auto- [How does Linear Search work?](#how-does-linear-search-work)auto- [Code](#code)auto - [Reference:](#reference)autoauto Introduction _L_inear Search is an way to search a data set for an element of interest. It is one of the many search algorithms available and is also the most direct and simple of the lot.\nLinear search looks for the element of interest in a dataset starting from the first element and moves on to the consecutive elements till it finds the one we're interested in. Due to this behaviour, it's not the fastest search algorithm around.\nIn the worst case, when the element of interest is the last (or near-last) in the data set, linear-search has to sift through till the end. Hence, in a worst-case scenario, the larger the data set is, the more the iterations it take to find the element of interest. Hence, the performance of Linear search takes a toll as the data set grows.\nLinear search works on sorted and unsorted data sets equally, since it has to go through the elements one by one and so doesn't mind if the data is ordered or not.\nPerformance 1. Worst-case performance: O(n) A worst-case analysis is done with the upper bound of the running time of the algorithm. ie.. the case when the maximum number of operations are executed.\nThe worst-case scenario for a linear search happens when the element-of-interest is not present in the dataset. A near worst-case scenario is when the element-of-interest is the last element of the dataset. In the first case, the search has to go through each element only to find that the element is not present in the dataset. In the second scenario, the search has to be done till the last element, which still takes n iterations.\nIn the worst-case, the performance is O(n), where n is the number of elements in the dataset.\n2. Best-case performance: O(1) In the best-case, where the element-of-interest is the first element in the dataset, only one search/lookup is needed. Hence the performance is denoted as O(1), for n elements.\n3. Average performance: O(n/2) On an average, the performance can be denoted as O(n/2).\nObservations: Linear Search iterates through every element in the dataset until it finds the match. In Linear Search, the number of iterations grows linearly if the data set grows in size. This algorithm is called Linear Search due to this linear increase in the complexity depending on the dataset. The best case scenario is when the first iteration finds the element. The Worst case is when the element of interest is not present in the dataset. A very near worse case is when the element of interest is the last one in the dataset. How does Linear Search work? Linear search progresses as following:\n1. Takes in a dataset as well as an element of interest. 2. Checks if the first element is the element of interest. 3. If yes, returns the element. 4. If not, move to the next element in the dataset. 5. Iterate till the dataset is exhausted. 6. Return None if the element of interest is not present in the dataset.\nCode [code language=\u0026quot;python\u0026quot;] def linear_search(my_list, item): \u0026quot;\u0026quot;\u0026quot;Linear search\u0026quot;\u0026quot;\u0026quot;\nlow_position = 0 high_position = len(my_list) - 1\nwhile low_position \u0026lt; high_position:\nif my_list[low_position] == item: print(\u0026quot;Your search item {0} is at position {1}\u0026quot;.format( item, low_position)) return low_position else: low_position += 1\nif __name__ == \u0026quot;__main__\u0026quot;: my_list = [1, 2, 3, 4, 5, 6] linear_search(my_list, 5) [/code]\nReference: http://quiz.geeksforgeeks.org/linear-search/ http://research.cs.queensu.ca/home/cisc121/2006s/webnotes/search.html ","link":"https://arvimal.github.io/posts/2017/01/linear-search-algorithm-study/","section":"posts","tags":null,"title":"Linear Search - Algorithm Study"},{"body":"A method defined within a class can either be an Accessor or a Mutator method.\nAn Accessor method returns the information about the object, but do not change the state or the object.\nA Mutator method, also called an Update method, can change the state of the object.\nConsider the following example:\n[code language=\u0026quot;python\u0026quot;] In [10]: a = [1,2,3,4,5]\nIn [11]: a.count(1) Out[11]: 1\nIn [12]: a.index(2) Out[12]: 1\nIn [13]: a Out[13]: [1, 2, 3, 4, 5]\nIn [14]: a.append(6)\nIn [15]: a Out[15]: [1, 2, 3, 4, 5, 6] [/code]\nThe methods a.count() and a.index() are both Accessor methods since it doesn't alter the object a in any sense, but only pulls the relevant information.\nBut a.append() is a mutator method, since it effectively changes the object (list a) to a new one.\nIn short, knowing the behavior of a method is helpful to understand how it alters the objects it acts upon.\n","link":"https://arvimal.github.io/posts/2016/12/accessor-and-mutator-methods/","section":"posts","tags":["object-oriented-programming","python"],"title":"Accessor and Mutator methods - Python"},{"body":"","link":"https://arvimal.github.io/tags/object-oriented-programming/","section":"tags","tags":null,"title":"object-oriented-programming"},{"body":"","link":"https://arvimal.github.io/tags/namespace/","section":"tags","tags":null,"title":"namespace"},{"body":"","link":"https://arvimal.github.io/tags/objects/","section":"tags","tags":null,"title":"objects"},{"body":"","link":"https://arvimal.github.io/tags/python-namespace/","section":"tags","tags":null,"title":"python-namespace"},{"body":"_E_verything in Python is an object, what does that mean? This post tries to discuss some very basic concepts.\nWhat does the following assignment do?\n[code language=\u0026quot;python\u0026quot;]\na = 1 [/code] Of course, anyone dabbled in code knows this. The statement above creates a container `a` and stores the value `1` in it.\nBut it seem that's not exactly what's happening, at least from Python's view-point.\nWhen a = 1 is entered or executed by the python interpreter, the following happens in the backend, seemingly unknown to the user.\nThe Python interpreter evaluates the literal 1 and tries to understand what data type can be assigned for it. There are several in-built data types such as str, float, bool, list, dict, set etc.. Builtin types are classes implemented in the python core. For a full list of types and explanation, read the python help at python-\u0026gt; help()-\u0026gt; topics -\u0026gt; TYPES Read the help sections for builtin types, eg.. help(int), help(list) etc.. The interpreter finds the appropriate builtin type for the literal. Since the literal 1 fits the type int, the interpreter creates an instance from class int() in memory. This instance is called an object since it's just a blob with some metadata. This object has a memory address, a value, a name in one or more namespace, some metadata etc.. type(a) helps in understanding the instance type. In short, an assignment statement simply creates an instance in memory from a pre-defined class. The interpreter reads the LHS (Left hand side) of the statement a = 1, and creates the name a in the current namespace. The name in the namespace is a reference to the object in memory. Through this reference, we can access the data portion as well as the attributes of that object. A single object can have multiple names (references). The name a created in the current namespace is linked to the corresponding object in memory. When a name that's already defined is entered at the python prompt, the interpreter reads the namespace, finds the name (reference), goes to the memory location it's referring to, and pull the value of the object, and prints it on-screen.\nEvery object has the following features: A single value, available in its data section. [code language=\u0026quot;python\u0026quot;] In [1]: a = 1\nIn [2]: a Out[2]: 1 [/code]\nA single type, since the object is an instance of a pre-defined type class such as int , float etc.. [code language=\u0026quot;python\u0026quot;] In [3]: type(a) Out[3]: int [/code]\nAttributes either inherited from the parent type class or defined by the user. [code language=\u0026quot;python\u0026quot;] In [10]: dir(a) Out[10]: ['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', ...[content omitted] '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes'] [/code]\nOne or more base classes. All new-stlye classes in Python ultimately inherits from the object class. [code language=\u0026quot;python\u0026quot;] In [4]: type(a) Out[4]: int\nIn [5]: int.mro() Out[5]: [int, object] [/code]\nNOTE: a is an instance of the int class, and int inturn inherits from the object class. Read more on Method Resolution Order.\nA unique ID representing the object. [code language=\u0026quot;python\u0026quot;] In [6]: id(a) Out[6]: 140090033476640 [/code]\nZero, One, or more names. Use dir() to check the current namespace. Use dir(\u0026lt;object-name\u0026gt;) to refer the indirect namespace. Several other builtins are available in the default namespace without defining them specifically, possible due to the inclusion of the builtin module available under the reference __builtin__ in the current namespace.\nFor a full list of the pre-defined variables, refer dir(__builtins__), help(__builtin__) or help(builtins) after an import builtins.\nA few questions and observations: Q1. How can an assignment have zero names in the namespace?\nAns: An assignment such as a = 1 creates an object in memory and creates a corresponding name (a in our case) in the namespace. a acts as a reference to the object in memory.\nBut, simply entering 1 at the python prompt creates an object in memory which is an instance of a type class, without creating the reference in the namespace.\nObjects which don't have a reference from the current namespace are usually garbage-collected due to lack of references. Hence, an object which doesn't have a reference (a name), or had multiple references (more than one names) but had them deleted (for example, del() gets garbage-collected by python.\nIf the assignment 1 happens to be at a python prompt, it echoes the literal back after creating the object and reference since the prompt is essentially a REPL (Read Eval Print loop)\nQ2. Can an object have more than one name references?\nAns: It's perfectly fine to have more than one reference to a single object. The example below should explain things very well.\n[code language=\u0026quot;python\u0026quot;] In [1]: a = 5000\nIn [2]: id(a) Out[2]: 140441367080400\nIn [3]: b = a\nIn [4]: b Out[4]: 5000\nIn [5]: id(b) Out[5]: 140441367080400\nIn [6]: c = 5000\nIn [7]: id(c) Out[7]: 140441367080432\nIn [8]: a is b Out[8]: True\nIn [9]: a == b Out[9]: True\nIn [10]: a is c Out[10]: False\nIn [11]: a == c Out[11]: True [/code]\nThe example shown above creates an object with value 5000 and assign it a name a in the current namespace. We checked the identifier of the object using id(a) and found out it to be 140441367080400.\nAs the next step, we created another name in the namespace, ie.. b which takes in whatever a points to. Hence, b would default to 5000 and it will have the same identifier as a.\nThis shows that an object in memory can have multiple references in a namespace.\nAnother object of value 5000 is created with a name c , but we can see that the identifier differs from what id(a) and id(b) is. This shows that c points to an entirely different object in memory.\nTo test if a is exactly the same object as b, use the keyword is. Meanwhile, if you want to test if two objects contain the same value, use the equality == symbol.\n","link":"https://arvimal.github.io/posts/2016/10/python-objects/","section":"posts","tags":["namespace","objects","programming","python-namespace","python-objects"],"title":"Python, Objects, and some more.."},{"body":"","link":"https://arvimal.github.io/tags/dentry/","section":"tags","tags":null,"title":"dentry"},{"body":"","link":"https://arvimal.github.io/tags/directory-entry-structure/","section":"tags","tags":null,"title":"directory-entry-structure"},{"body":"","link":"https://arvimal.github.io/tags/ext4/","section":"tags","tags":null,"title":"ext4"},{"body":"","link":"https://arvimal.github.io/tags/ext4_dir_entry/","section":"tags","tags":null,"title":"ext4_dir_entry"},{"body":"","link":"https://arvimal.github.io/tags/ext4_dir_entry_2/","section":"tags","tags":null,"title":"ext4_dir_entry_2"},{"body":"","link":"https://arvimal.github.io/tags/file-systems/","section":"tags","tags":null,"title":"file-systems"},{"body":"","link":"https://arvimal.github.io/tags/filesystem/","section":"tags","tags":null,"title":"filesystem"},{"body":"A recent discussion at work brought up the question \u0026quot;What can be the length of a file name in EXT4\u0026quot;. Or in other words, what would be the maximum character length of the name for a file in EXT4?\nWikipedia states that it's 255 Bytes, but how does that come to be? Is it 255 Bytes or 255 characters?\nIn the kernel source for the 2.6 kernel series (the question was for a RHEL6/EXT4 combination), in fs/ext4/ext4.h, we'd be able to see the following:\n[code language=\u0026quot;c\u0026quot;]\n#define EXT4_NAME_LEN 255\nstruct ext4_dir_entry { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __le16 name_len; /* Name length */ char name[EXT4_NAME_LEN]; /* File name */ };\n/* * The new version of the directory entry. Since EXT4 structures are * stored in intel byte order, and the name_len field could never be * bigger than 255 chars, it's safe to reclaim the extra byte for the * file_type field. */\nstruct ext4_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[EXT4_NAME_LEN]; /* File name */ }; [/code] This shows that there are two versions of the directory entry structure, ie.. ext4_dir_entry and ext4_dir_entry_2\nA directory entry structure carries the file/folder name and the corresponding inode number under every directory.\nBoth structs use an element named name_len to denote the length of the file/folder name.\nIf the EXT filesystem feature filetype is not set, the directory entry structure falls to the first method ext4_dir_entry, else it's the second, ie.. ext4_dir_entry_2.\nBy default, the file system feature filetype is set, hence the directory entry structure is ext4_dir_entry_2 . As seen above, in this case, the name_len field is set to 8 bits.\n__u8 represents an unsigned 8-bit integer in C, and can store values from 0 to 255.\nie.. 2^8 = 255 (0 t0 255 == 256)\next4_dir_entry has a name_len of __le16, but it seems that the file-name length can only go to a max of 256.\nObservations: The maximum name length is 255 characters on Linux machines. The actual name length of a file/folder is stored in name_len in each directory entry, under its parent folder. So if the file name length is 5 characters, 5 would be the value set for name_len for that particular file. ie.. the actual length. A character will consume a byte of storage, so the number of characters in a file name will map to the respective number bytes. If so, a file with a name_len of 5 will be using 5 bytes of memory to store the name. Hence, name_len denotes the number of characters that a file can have. Since U8 is 8-bits, name_len can store a file name with upto 255 chars.\nNow the actual memory being consumed for storing these characters is not denoted by name_len. Since the size of a character translates to a byte, the maximum size wrt memory that a file name can have is 255 Bytes.\nNOTE: The initial dir entry structure ext4_dir_entry had __le16 for name_len, it was later re-sized to __u8 in ext4_dir_entry_2 , by culling 8 bits from the existing 16 bits of name_len.\nThe remaining free space culled from name_len was assigned to store the file type, in ext4_dir_entry_2. It was named file_type with size __u8.\nfile_type helps to identity the file types such as regular files, sockets, character devices, block devices etc..\nReferences: RHEL6 kernel-2.6.32-573.el6 EXT4 header file (ext4.h) EXT4 Wiki - Disk layout http://unix.stackexchange.com/questions/32795/what-is-the-maximum-allowed-filename-and-folder-size-with-ecryptfs ","link":"https://arvimal.github.io/posts/2016/07/max-file-name-in-ext4/","section":"posts","tags":["dentry","directory-entry-structure","ext4","ext4_dir_entry","ext4_dir_entry_2","file-systems","filesystem","name_len"],"title":"Max file-name length in an EXT4 file system."},{"body":"","link":"https://arvimal.github.io/tags/name_len/","section":"tags","tags":null,"title":"name_len"},{"body":"","link":"https://arvimal.github.io/categories/techno/","section":"categories","tags":null,"title":"techno"},{"body":"","link":"https://arvimal.github.io/tags/inheritance/","section":"tags","tags":null,"title":"inheritance"},{"body":"_s_uper() is a feature through which inherited methods can be accessed, which has been overridden in a class. It can also help with the MRO lookup order in case of multiple inheritance. This may not be obvious first, but a few examples should help to drive the point home.\nInheritance and method overloading was discussed in a previous post, where we saw how inherited methods can be overloaded or enhanced in the child classes.\nIn many scenarios, it's needed to overload an inherited method, but also call the actual method defined in the Parent class.\nLet's start off with a simple example based on Inheritance, and build from there.\nExample 0:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object):\ndef func(self): print(\u0026quot;I'm being called from the Parent class!\u0026quot;)\nclass ChildClass(MyClass): pass\nmy_instance_1 = ChildClass() my_instance_1.func() [/code] This outputs:\n[code language=\u0026quot;python\u0026quot;] In [18]: %run /tmp/super-1.py I'm being called from the Parent class [/code] In Example 0, we have two classes, MyClass and ChildClass. The latter inherits from the former, and the parent class MyClass has a method named func defined.\nSince ChildClass inherits from MyClass, the child class has access to the methods defined in the parent class. An instance is created my_instance_2, for ChildClass.\nCalling my_instance_1.func() will print the statement from the Parent class, due to the inheritance.\nBuilding up on the first example:\nExample 1:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object):\ndef func(self): print(\u0026quot;I'm being called from the Parent class\u0026quot;)\nclass ChildClass(MyClass):\ndef func(self): print(\u0026quot;I'm being called from the Child class\u0026quot;)\nmy_instance_1 = MyClass() my_instance_2 = ChildClass()\nmy_instance_1.func() my_instance_2.func() [/code] This outputs:\n[code language=\u0026quot;python\u0026quot;] In [19]: %run /tmp/super-1.py I'm being called from the Parent class I'm being called from the Child class [/code] This example has a slight difference, both the child class as well as the parent class have the same method defined, ie.. func. In this scenario, the parent class' method is overridden by the child class method.\nie.. if we call the func() method from the instance of ChildClass, it need not go a fetch the method from its Parent class, since it's already defined locally.\nNOTE: This is due to the Method Resolution Order, discussed in an earlier post.\nBut what if there is a scenario that warranties the need for specifically calling methods defined in the Parent class, from the instance of a child class?\nie.. How to call the methods defined in the Parent class, through the instance of the Child class, even if the Parent class method is overloaded in the Child class?\nIn such a case, the inbuilt function super() can be used. Let's add to the previous example.\nExample 2:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object):\ndef func(self): print(\u0026quot;I'm being called from the Parent class\u0026quot;)\nclass ChildClass(MyClass):\ndef func(self): print(\u0026quot;I'm actually being called from the Child class\u0026quot;) print(\u0026quot;But...\u0026quot;) # Calling the `func()` method from the Parent class. super(ChildClass, self).func()\nmy_instance_2 = ChildClass() my_instance_2.func() [/code] This outputs:\n[code language=\u0026quot;python\u0026quot;] In [21]: %run /tmp/super-1.py I'm actually being called from the Child class But... I'm being called from the Parent class [/code]\nHow is the code structured? We have two classes MyClass and ChildClass. The latter is inheriting from the former. Both classes have a method named func The child class ChildClass is instantiated as my_instance_2 The func method is called from the instance. How does the code work? When the func method is called, the interpreter searches it using the Method Resolution Order, and find the method defined in the class ChildClass. Since it finds the method in the child class, it executes it, and prints the string \u0026quot;I'm actually being called from the Child class\u0026quot;, as well \u0026quot;But...\u0026quot; The next statement is super which calls the method func defined in the parent class of ChildClass Since the control is now passed onto the func method in the Parent class via super, the corresponding print() statement is printed to stdout. Example 2 can also be re-written as :\n[code language=\u0026quot;python\u0026quot;] class MyClass(object):\ndef func(self): print(\u0026quot;I'm being called from the Parent class\u0026quot;)\nclass ChildClass(MyClass):\ndef func(self): print(\u0026quot;I'm actually being called from the Child class\u0026quot;) print(\u0026quot;But...\u0026quot;) # Calling the `func()` method from the Parent class. # super(ChildClass, self).func() MyClass.func(self) # Call the method directly via Parent class\nmy_instance_2 = ChildClass() my_instance_2.func() [/code]\nNOTE: The example above uses the Parent class directly to access it's method. Even though it works, it is not the best way to do it since the code is tied to the Parent class name. If the Parent class name changes, the child/sub class code has to be changed as well.\nLet's see another example for super() . This is from our previous article on Inheritance and method overloading.\nExample 3:\n[code language=\u0026quot;python\u0026quot;] import abc\nclass MyClass(object):\n__metaclass__ = abc.ABCMeta\ndef my_set_val(self, value): self.value = value\ndef my_get_val(self): return self.value\n@abc.abstractmethod def print_doc(self): return\nclass MyChildClass(MyClass):\ndef my_set_val(self, value): if not isinstance(value, int): value = 0 super(MyChildClass, self).my_set_val(self)\ndef print_doc(self): print(\u0026quot;Documentation for MyChild Class\u0026quot;)\nmy_instance = MyChildClass() my_instance.my_set_val(100) print(my_instance.my_get_val()) print(my_instance.print_doc()) [/code] The code is already discussed here. The my_set_val method is defined in both the child class as well as the parent class.\nWe overload the my_set_val method defined in the parent class, in the child class. But after enhancing/overloading it, we call the my_set_val method specifically from the Parent class using super() and thus enhance it.\nTakeaway: super() helps to specifically call the Parent class method which has been overridden in the child class, from the child class. The super() in-built function can be used to call/refer the Parent class without explicitly naming them. This helps in situations where the Parent class name may change. Hence, super() helps in avoiding strong ties with class names and increases maintainability. super() helps the most when there are multiple inheritance happening, and the MRO ends up being complex. In case you need to call a method from a specific parent class, use super(). There are multiple ways to call a method from a Parent class. . super(, self). super(). References: https://docs.python.org/2/library/functions.html#super https://rhettinger.wordpress.com/2011/05/26/super-considered-super/ https://stackoverflow.com/questions/222877/how-to-use-super-in-python ","link":"https://arvimal.github.io/posts/2016/07/inheritance-and-super-oop/","section":"posts","tags":["inheritance","object-oriented-programming","python","super"],"title":"Inheritance and super() - Object Oriented Programming"},{"body":"","link":"https://arvimal.github.io/tags/super/","section":"tags","tags":null,"title":"super"},{"body":"_E_xt3 and Ext4 recently have been the most commonly used file system on Linux machines.\nWhat does uninit_bg actually do?\nRead https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout\nsection `Meta Block Groups and Lazy Block Group Initialization`.\nhttps://ext4.wiki.kernel.org/index.php/Frequently_Asked_Questions\nhttps://www.thomas-krenn.com/en/wiki/Ext4_Filesystem\nhttps://access.redhat.com/labs/psb/versions/kernel-3.10.0-327.18.2.el7/Documentation/filesystems/ext4.txt\nhttps://access.redhat.com/labs/psb/versions/kernel-3.10.0-327.18.2.el7/fs/ext4/ext4.h\n","link":"https://arvimal.github.io/posts/2016/06/uninit-bg-and-lazy-block-group-allocation/","section":"posts","tags":["ext3","ext4","file-systems","lazy-block-group-allocation","uninit_bg"],"title":"`uninit_bg` and lazy block group allocation in EXT3/4"},{"body":"","link":"https://arvimal.github.io/tags/ceph/","section":"tags","tags":null,"title":"ceph"},{"body":"","link":"https://arvimal.github.io/categories/ceph/","section":"categories","tags":null,"title":"ceph"},{"body":"","link":"https://arvimal.github.io/tags/ext3/","section":"tags","tags":null,"title":"ext3"},{"body":"","link":"https://arvimal.github.io/tags/lazy-block-group-allocation/","section":"tags","tags":null,"title":"lazy-block-group-allocation"},{"body":"","link":"https://arvimal.github.io/tags/rados/","section":"tags","tags":null,"title":"rados"},{"body":"","link":"https://arvimal.github.io/tags/rados-gateway/","section":"tags","tags":null,"title":"rados-gateway"},{"body":"","link":"https://arvimal.github.io/tags/rgw/","section":"tags","tags":null,"title":"rgw"},{"body":"","link":"https://arvimal.github.io/tags/rgw-index/","section":"tags","tags":null,"title":"rgw-index"},{"body":"","link":"https://arvimal.github.io/tags/sharding/","section":"tags","tags":null,"title":"sharding"},{"body":"_S_harding is the process of breaking down data onto multiple locations so as to increase parallelism, as well as distribute load. This is a common feature used in databases. Read more on this at Wikipedia.\nThe concept of sharding is used in Ceph, for splitting the bucket index in a RADOS Gateway.\nRGW or RADOS Gateway keeps an index for all the objects in its buckets for faster and easier lookup. For each RGW bucket created in a pool, the corresponding index is created in the XX.index pool.\nFor example, for each of the buckets created in .rgw pool, the bucket index is created in .rgw.buckets.index pool. For each bucket, the index is stored in a single RADOS object.\nWhen the number of objects increases, the size of the RADOS object increases as well. Two problems arise due to the increased index size.\nRADOS does not work good with large objects since it's not designed as such. Operations such as recovery, scrubbing etc.. work on a single object. If the object size increases, OSDs may start hitting timeouts because reading a large object may take a long time. This is one of the reason that all RADOS client interfaces such as RBD, RGW, CephFS use a standard 4MB object size. Since the index is stored in a single RADOS object, only a single operation can be done on it at any given time. When the number of objects increases, the index stored in the RADOS object grows. Since a single index is handling a large number of objects, and there is a chance the number of operations also increase, parallelism is not possible which can end up being a bottleneck. Multiple operations will need to wait in a queue since a single operation is possible at a time. In order to work around these problems, the bucket index is sharded into multiple parts. Each shard is kept on a separate RADOS object within the index pool.\nSharding is configured with the tunable bucket_index_max_shards . By default, this tunable is set to 0 which means that there are no shards.\nHow to check if Sharding is set? List the buckets [code language=\u0026quot;bash\u0026quot;] # radosgw-admin metadata bucket list [ \u0026quot;my-new-bucket\u0026quot; ] [/code]\nGet information on the bucket in question[code language=\u0026quot;bash\u0026quot;]\n# radosgw-admin metadata get bucket:my-new-bucket { \u0026quot;key\u0026quot;: \u0026quot;bucket:my-new-bucket\u0026quot;, \u0026quot;ver\u0026quot;: { \u0026quot;tag\u0026quot;: \u0026quot;_bGZAVUgayKVwGNgNvI0328G\u0026quot;, \u0026quot;ver\u0026quot;: 1 }, \u0026quot;mtime\u0026quot;: 1458940225, \u0026quot;data\u0026quot;: { \u0026quot;bucket\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;my-new-bucket\u0026quot;, \u0026quot;pool\u0026quot;: \u0026quot;.rgw.buckets\u0026quot;, \u0026quot;data_extra_pool\u0026quot;: \u0026quot;.rgw.buckets.extra\u0026quot;, \u0026quot;index_pool\u0026quot;: \u0026quot;.rgw.buckets.index\u0026quot;, \u0026quot;marker\u0026quot;: \u0026quot;default.2670570.1\u0026quot;, \u0026quot;bucket_id\u0026quot;: \u0026quot;default.2670570.1\u0026quot; }, \u0026quot;owner\u0026quot;: \u0026quot;rgw_user\u0026quot;, \u0026quot;creation_time\u0026quot;: 1458940225, \u0026quot;linked\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;has_bucket_info\u0026quot;: \u0026quot;false\u0026quot; } }\n[/code]\nUse the bucket ID to get more information, including the number of shards.\n[code language=\u0026quot;bash\u0026quot;] radosgw-admin metadata get bucket.instance:my-new-bucket:default.2670570.1 { \u0026quot;key\u0026quot;: \u0026quot;bucket.instance:my-new-bucket:default.2670570.1\u0026quot;, \u0026quot;ver\u0026quot;: { \u0026quot;tag\u0026quot;: \u0026quot;_xILkVKbfQD7reDFSOB4a5VU\u0026quot;, \u0026quot;ver\u0026quot;: 1 }, \u0026quot;mtime\u0026quot;: 1458940225, \u0026quot;data\u0026quot;: { \u0026quot;bucket_info\u0026quot;: { \u0026quot;bucket\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;my-new-bucket\u0026quot;, \u0026quot;pool\u0026quot;: \u0026quot;.rgw.buckets\u0026quot;, \u0026quot;data_extra_pool\u0026quot;: \u0026quot;.rgw.buckets.extra\u0026quot;, \u0026quot;index_pool\u0026quot;: \u0026quot;.rgw.buckets.index\u0026quot;, \u0026quot;marker\u0026quot;: \u0026quot;default.2670570.1\u0026quot;, \u0026quot;bucket_id\u0026quot;: \u0026quot;default.2670570.1\u0026quot; }, \u0026quot;creation_time\u0026quot;: 1458940225, \u0026quot;owner\u0026quot;: \u0026quot;rgw_user\u0026quot;, \u0026quot;flags\u0026quot;: 0, \u0026quot;region\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;placement_rule\u0026quot;: \u0026quot;default-placement\u0026quot;, \u0026quot;has_instance_obj\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;quota\u0026quot;: { \u0026quot;enabled\u0026quot;: false, \u0026quot;max_size_kb\u0026quot;: -1, \u0026quot;max_objects\u0026quot;: -1 }, \u0026quot;num_shards\u0026quot;: 0, \u0026quot;bi_shard_hash_type\u0026quot;: 0 }, \u0026quot;attrs\u0026quot;: [ { \u0026quot;key\u0026quot;: \u0026quot;user.rgw.acl\u0026quot;, \u0026quot;val\u0026quot;: \u0026quot;AgKPAAAAAgIaAAAACAAAAHJnd191c2VyCgAAAEZpcnN0IFVzZXIDA2kAAAABAQAAAAgAAAByZ3dfdXNlcg8AAAABAAAACAAAAHJnd191c2VyAwM6AAAAAgIEAAAAAAAAAAgAAAByZ3dfdXNlcgAAAAAAAAAAAgIEAAAADwAAAAoAAABGaXJzdCBVc2VyAAAAAAAAAAA=\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;user.rgw.idtag\u0026quot;, \u0026quot;val\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;user.rgw.manifest\u0026quot;, \u0026quot;val\u0026quot;: \u0026quot;\u0026quot; } ] } }\n[/code] Note that `num_shards` is set to 0, which means that sharding is not enabled.\nHow to configure Sharding? To configure sharding, we need to first dump the region info.\nNOTE: By default, RGW has a region named default even if regions are not configured.\n[code language=\u0026quot;bash\u0026quot;] # radosgw-admin region get \u0026gt; /tmp/region.txt\n# cat /tmp/region.txt { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;api_name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;is_master\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;endpoints\u0026quot;: [], \u0026quot;hostnames\u0026quot;: [], \u0026quot;master_zone\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;zones\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;endpoints\u0026quot;: [], \u0026quot;log_meta\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;log_data\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;bucket_index_max_shards\u0026quot;: 0 } ], \u0026quot;placement_targets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default-placement\u0026quot;, \u0026quot;tags\u0026quot;: [] } ], \u0026quot;default_placement\u0026quot;: \u0026quot;default-placement\u0026quot; }\n[/code] Edit the file /tmp/region.txt, change the value for `bucket_index_max_shards` to the needed shard value (we're setting it to 8 in this example), and inject it back to the region.\n[code language=\u0026quot;bash\u0026quot;] # radosgw-admin region set \u0026lt; /tmp/region.txt { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;api_name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;is_master\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;endpoints\u0026quot;: [], \u0026quot;hostnames\u0026quot;: [], \u0026quot;master_zone\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;zones\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;endpoints\u0026quot;: [], \u0026quot;log_meta\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;log_data\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;bucket_index_max_shards\u0026quot;: 8 } ], \u0026quot;placement_targets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default-placement\u0026quot;, \u0026quot;tags\u0026quot;: [] } ], \u0026quot;default_placement\u0026quot;: \u0026quot;default-placement\u0026quot; } [/code] Reference:\nRed Hat Ceph Storage 1.3 Rados Gateway documentation https://en.wikipedia.org/wiki/Shard_(database_architecture) ","link":"https://arvimal.github.io/posts/2016/06/sharding-the-ceph-rgw-bucket-index/","section":"posts","tags":["ceph","rados","rados-gateway","rgw","rgw-index","sharding"],"title":"Sharding the Ceph RADOS Gateway bucket index"},{"body":"","link":"https://arvimal.github.io/tags/uninit_bg/","section":"tags","tags":null,"title":"uninit_bg"},{"body":"","link":"https://arvimal.github.io/tags/abstract-methods/","section":"tags","tags":null,"title":"abstract-methods"},{"body":"","link":"https://arvimal.github.io/tags/abstractmethod/","section":"tags","tags":null,"title":"abstractmethod"},{"body":"","link":"https://arvimal.github.io/tags/builtins/","section":"tags","tags":null,"title":"builtins"},{"body":"_I_nheritance is a usual theme in Object Oriented Programming. Because of Inheritance, the functions/methods defined in parent classes can be called in Child classes which enables code reuse, and several other features. In this article, we try to understand some of those features that come up with Inheritance.\nWe've discussed Abstract Methods in an earlier post, which is a feature part of Inheritance, and can be applied on child classes that inherits from a Parent class.\nE the methods which are inherited can also be seen as another feature or possibility in Inheritance. In many cases, it's required to override or specialize the methods inherited from the Parent class. This is of course possible, and is called as 'Method Overloading'.\nConsider the two classes and its methods defined below:\nExample 0:\n[code language=\u0026quot;python\u0026quot;] import abc\nclass MyClass(object):\n__metaclass__ = abc.ABCMeta\ndef __init__(self): pass\ndef my_set_method(self, value): self.value = value\ndef my_get_method(self): return self.value\n@abc.abstractmethod def printdoc(self): return\nclass MyChildClass(MyClass):\ndef my_set_method(self, value): if not isinstance(value, int): value = 0 super(MyChildClass, self).my_set_method(self)\ndef printdoc(self): print(\u0026quot;\\nDocumentation for MyChildClass()\u0026quot;)\ninstance_1 = MyChildClass() instance_1.my_set_method(10) print(instance_1.my_get_method()) instance_1.printdoc() [/code]\nWe have two classes, the parent class being MyClass and the child class being MyChildClass.\nMyClass has three methods defined.\nmy_set_method() my_get_method() printdoc() The printdoc() method is an Abstract method, and hence should be implemented in the Child class as a mandatory method.\nThe child class MyChildClass inherits from MyClass and has access to all it's methods.\nNormally, we can just go ahead and use the methods defined in MyClass , in MyChildClass. But there can be situations when we want to improve or build upon the methods inherited. As said earlier, this is called Method Overloading.\nMyChildClass extends the parent's my_set_method() function by it's own implementation. In this example, it does an additional check to understand if the input value is an int or not, and then calls the my_set_method() of it's parent class using super. Hence, this method in the child class extends the functionality prior calling method in the parent. A post on super is set for a later time.\nEven though this is a trivial example, it helps to understand how the features inherited from other classes can be extended or improved upon via method overloading.\nThe my_get_method() is not overridden in the child class but still called from the instance, as instance_1.my_get_method(). We're using it as it is available via Inheritance. Since it's defined in the parent class, it works in the child class' instance when called, even if not overridden.\nThe printdoc() method is an abstract method and hence is mandatory to be implemented in the child class, and can be overridden with what we choose to do.\nInheritance is possible from python builtins, and can be overridden as well. Let's check out another example:\nExample 1:\n[code language=\u0026quot;python\u0026quot;] class MyList(list):\ndef __getitem__(self, index): if index == 0: raise IndexError if index \u0026gt; 0: index -= 1 return list.__getitem__(self, index)\ndef __setitem__(self, index, value): if index == 0: raise IndexError if index \u0026gt; 0: index -= 1 list.__setitem__(self, index, value)\nx = MyList(['a', 'b', 'c']) print(x) print(\u0026quot;-\u0026quot; * 10)\nx.append('d') print(x) print(\u0026quot;-\u0026quot; * 10)\nx.__setitem__(4, 'e') print(x) print(\u0026quot;-\u0026quot; * 10)\nprint(x[1]) print(x.__getitem__(1)) print(\u0026quot;-\u0026quot; * 10)\nprint(x[4]) print(x.__getitem__(4)) [/code] This outputs:\n[code language=\u0026quot;python\u0026quot;] ['a', 'b', 'c'] ---------- ['a', 'b', 'c', 'd'] ---------- ['a', 'b', 'c', 'e'] ---------- a a ---------- e e [/code]\nHow does the code work? The class MyList() inherits from the builtin list. Because of the inheritance, we can use list's available magic methods such as __getitem__() , __setitem__() etc..\nNOTE: In order to see the available methods in list, use dir(list).\nWe create two functions/methods named `__getitem__()` and `__setitem__()` to override the inherited methods. Within these functions/methods, we set our own conditions. Wie later call the builtin methods directly within these functions, using list.__getitem__() list.__setitem__() We create an instance named x from MyList(). We understand that x[1] and x.__getitem__(1) are same. x[4, 'e'] and x.__setitem__(4, 'e') are same. x.append(f) is same as x.__setitem__(\u0026lt;n\u0026gt;, f) where is the element to the extreme right which the python interpreter iterates and find on its own. Hence, in Inheritance, child classes can:\nInherit from parent classes and use those methods. Parent classes can either be user-defined classes or buitins like list , dict etc.. Override (or Overload) an inherited method. Extend an inherited method in its own way. Implement an Abstract method the parent class requires. Reference: Python beyond the basics - Object Oriented Programming ","link":"https://arvimal.github.io/posts/2016/06/inheritance-and-method-overloading-oop/","section":"posts","tags":["abstract-methods","abstractmethod","builtins","inheritance","method-overloading"],"title":"Inheritance and Method overloading - Object Oriented Programming"},{"body":"","link":"https://arvimal.github.io/tags/method-overloading/","section":"tags","tags":null,"title":"method-overloading"},{"body":"_A_bstract classes, in short, are classes that are supposed to be inherited or subclassed, rather than instantiated.\nThrough Abstract Classes, we can enforce a blueprint on the subclasses that inherit the Abstract Class. This means that Abstract classes can be used to define a set of methods that must be implemented by it subclasses.\nAbstract classes are used when working on large projects where classes have to be inherited, and need to strictly follow certain blueprints.\nPython supports Abstract Classes via the module abc from version 2.6. Using the abc module, its pretty straight forward to implement an Abstract Class.\nExample 0:\n[code language=\u0026quot;python\u0026quot;] import abc\nclass My_ABC_Class(object): __metaclass__ = abc.ABCMeta\n@abc.abstractmethod def set_val(self, val): return\n@abc.abstractmethod def get_val(self): return\n# Abstract Base Class defined above ^^^\n# Custom class inheriting from the above Abstract Base Class, below\nclass MyClass(My_ABC_Class):\ndef set_val(self, input): self.val = input\ndef get_val(self): print(\u0026quot;\\nCalling the get_val() method\u0026quot;) print(\u0026quot;I'm part of the Abstract Methods defined in My_ABC_Class()\u0026quot;) return self.val\ndef hello(self): print(\u0026quot;\\nCalling the hello() method\u0026quot;) print(\u0026quot;I'm *not* part of the Abstract Methods defined in My_ABC_Class()\u0026quot;)\nmy_class = MyClass()\nmy_class.set_val(10) print(my_class.get_val()) my_class.hello() [/code] In the code above, set_val() and get_val() are both abstract methods defined in the Abstract Class My_ABC_Class(). Hence it should be implemented in the child class inheriting from My_ABC_Class().\nIn the child class MyClass() , we have to strictly define the abstract classes defined in the Parent class. But the child class is free to implement other methods of their own. The hello() method is one such.\nThis will print :\n[code language=\u0026quot;bash\u0026quot;] # python abstractclasses-1.py\nCalling the get_val() method I'm part of the Abstract Methods defined in My_ABC_Class() 10\nCalling the hello() method I'm *not* part of the Abstract Methods defined in My_ABC_Class() [/code] The code gets executed properly even if the hello() method is not an abstract method.\nLet's check what happens if we don't implement a method marked as an abstract method, in the child class.\nExample 1:\n[code language=\u0026quot;python\u0026quot;] import abc\nclass My_ABC_Class(object): __metaclass__ = abc.ABCMeta\n@abc.abstractmethod def set_val(self, val): return\n@abc.abstractmethod def get_val(self): return\n# Abstract Base Class defined above ^^^\n# Custom class inheriting from the above Abstract Base Class, below\nclass MyClass(My_ABC_Class):\ndef set_val(self, input): self.val = input\ndef hello(self): print(\u0026quot;\\nCalling the hello() method\u0026quot;) print(\u0026quot;I'm *not* part of the Abstract Methods defined in My_ABC_Class()\u0026quot;)\nmy_class = MyClass()\nmy_class.set_val(10) print(my_class.get_val()) my_class.hello() [/code] Example 1 is the same as Example 0 except we don't have the get_val() method defined in the child class.\nThis means that we're breaking the rule of abstraction. Let's see what happens:\n[code language=\u0026quot;bash\u0026quot;] # python abstractclasses-2.py Traceback (most recent call last): File \u0026quot;abstractclasses-2.py\u0026quot;, line 50, in my_class = MyClass() TypeError: Can't instantiate abstract class MyClass with abstract methods get_val [/code]\nThe traceback clearly states that the child class MyClass() cannot be instantiated since it does not implement the Abstract methods defined in it's Parent class.\nWe mentioned that an Abstract class is supposed to be inherited rather than instantiated. What happens if we try instantiating an Abstract class?\nLet's use the same example, this time we're instantiating the Abstract class though.\nExample 2:\n[code language=\u0026quot;python\u0026quot;] import abc\nclass My_ABC_Class(object): __metaclass__ = abc.ABCMeta\n@abc.abstractmethod def set_val(self, val): return\n@abc.abstractmethod def get_val(self): return\n# Abstract Base Class defined above ^^^\n# Custom class inheriting from the above Abstract Base Class, below\nclass MyClass(My_ABC_Class):\ndef set_val(self, input): self.val = input\ndef hello(self): print(\u0026quot;\\nCalling the hello() method\u0026quot;) print(\u0026quot;I'm *not* part of the Abstract Methods defined in My_ABC_Class()\u0026quot;)\nmy_class = My_ABC_Class() # \u0026lt;- Instantiating the Abstract Class\nmy_class.set_val(10) print(my_class.get_val()) my_class.hello() [/code] What does this output?\n[code language=\u0026quot;bash\u0026quot;] # python abstractclasses-3.py Traceback (most recent call last): File \u0026quot;abstractclasses-3.py\u0026quot;, line 54, in my_class = My_ABC_Class() TypeError: Can't instantiate abstract class My_ABC_Class with abstract methods get_val, set_val [/code] As expected, the Python interpreter says that it can't instantiate the abstract class My_ABC_Class.\nTakeaway: An Abstract Class is supposed to be inherited, not instantiated. The Abstraction nomenclature is applied on the methods within a Class. The abstraction is enforced on methods which are marked with the decorator @abstractmethod or @abc.abstractmethod, depending on how you imported the module, from abc import abstractmethod or import abc. It is not mandatory to have all the methods defined as abstract methods, in an Abstract Class. Subclasses/Child classes are enforced to define the methods which are marked with @abstractmethod in the Parent class. Subclasses are free to create methods of their own, other than the abstract methods enforced by the Parent class. Reference: https://pymotw.com/2/abc/ Python beyond the basics - Object Oriented Programming ","link":"https://arvimal.github.io/posts/2016/06/abc-oop-python/","section":"posts","tags":["abstract-base-class","abstract-methods","abstractmethod","object-oriented-programming","programming","python"],"title":"Abstract Base Classes/Methods - Object Oriented Programming"},{"body":"","link":"https://arvimal.github.io/tags/abstract-base-class/","section":"tags","tags":null,"title":"abstract-base-class"},{"body":"_T_his article was long overdue and should have been published before many of the articles in this blog. Better late than never.\nself in Python is usually used in an Object Oriented nomenclature, to denote the instance/object created from a Class.\nIn short, self is the instance itself.\nLet's check the following example:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object): def __init__(self, name): self.name = name print(\u0026quot;Initiating the instance!\u0026quot;)\ndef hello(self): print(self.name)\nmyclass = MyClass(\u0026quot;Dan Inosanto\u0026quot;)\n# Calling the `hello` method via the Instance `myclass` myclass.hello()\n# Calling the `hello` method vai the class. MyClass.hello(myclass) [/code]\nThe code snippet above is trivial and stupid, but I think it gets the idea across.\nWe have a class named MyClass() which takes a name value as an argument. It also prints the string \u0026quot;Initiating the instance\u0026quot;. The name value is something that has to be passed while creating an instance.\nThe function hello() just prints the name value that is passed while instantiating the class MyClass().\nWe instantiate the class MyClass() as myclass and pass the string Dan Inosanto as an argument. Read about the great Inosanto here.\nNext, we call the hello() method through the instance. ie..\n[code language=\u0026quot;python\u0026quot;] myclass.hello() [/code]\nThis should print the name we passed while instantiating MyClass() as myclass , which should be pretty obvious.\nThe second and last instruction is doing the same thing, but in a different way.\n[code language=\u0026quot;python\u0026quot;] MyClass.hello(myclass) [/code] Here, we call the class MyClass() directly as well as it's method hello(). Let's check out what both prints:\n[code language=\u0026quot;bash\u0026quot;] # python /tmp/test.py\nInitiating the instance! Dan Inosanto Dan Inosanto [/code]\nAs we can see, both prints the same output. This means that :\nmyclass.hello(self) == MyClass.hello(myclass)\nIn general, we can say that:\n.(self) == .()\nie.. The keyword self actually represents the instance being instantiated from the Class. Hence self can be seen as Syntactic sugar.\n","link":"https://arvimal.github.io/posts/2016/06/self-in-python/","section":"posts","tags":null,"title":"`self` in Python - Object Oriented Programming"},{"body":"_F_unctions defined under a class are also called methods. Most of the methods are accessed through an instance of the class.\nThere are three types of methods:\nInstance methods Static methods Class methods Both Static methods and Class methods can be called using the @staticmethod and @classmethod syntactic sugar respectively.\nInstance methods _I_nstance methods are also called Bound methods since the instance is bound to the class via self. Read a simple explanation on self here.\nAlmost all methods are Instance methods since they are accessed through instances.\nFor example:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object):\ndef set_val(self, val): self.value = val\ndef get_val(self): print(self.value) return self.value\na = MyClass() b = MyClass()\na.set_val(10) b.set_val(100)\na.get_val() b.get_val() [/code] The above code snippet shows manipulating the two methods set_val() and get_val() . These are done through the instances a and b. Hence these methods are called Instance methods.\nNOTE: Instance methods have self as their first argument. self is the instance itself.\nAll methods defined under a class are usually called via the instance instantiated from the class. But there are methods which can work without instantiating an instance.\nClass methods and Static methods don't require an instance, and hence don't need self as their first argument.\nStatic methods Static methods are functions/methods which doesn't need a binding to a class or an instance.\nStatic methods, as well as Class methods, don't require an instance to be called. Static methods doesn't need self or cls as the first argument since it's not bound to an instance or class. Static methods are normal functions, but within a class. Static methods are defined with the keyword @staticmethod above the function/method. Static methods are usually used to work on Class Attributes. ============================= A note on class attributes\nAttributes set explicitly under a class (not under a function) are called Class Attributes.\nFor example:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object): value = 10\ndef my_func(self): pass [/code] In the code snippet above, value = 10 is an attribute defined under the class MyClass() and not under any functions/methods. Hence, it's called a Class attribute. =============================\nLet's check out an example on static methods and class attributes:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object): # A class attribute count = 0\ndef __init__(self, name): print(\u0026quot;An instance is created!\u0026quot;) self.name = name MyClass.count += 1\n# Our class method @staticmethod def status(): print(\u0026quot;The total number of instances are \u0026quot;, MyClass.count)\nprint(MyClass.count)\nmy_func_1 = MyClass(\u0026quot;MyClass 1\u0026quot;) my_func_2 = MyClass(\u0026quot;MyClass 2\u0026quot;) my_func_3 = MyClass(\u0026quot;MyClass 3\u0026quot;)\nMyClass.status() print(MyClass.count) [/code] This prints the following:\n[code language=\u0026quot;bash\u0026quot;] # python statismethod.py\n0 An instance is created! An instance is created! An instance is created!\nThe total number of instances are 3 3 [/code]\nHow does the code work?\nThe example above has a class MyClass() with a class attribute count = 0. An __init__ magic method accepts a name variable. The __init__ method also increments the count in the count counter at each instantiation. We define a staticmethod status() which just prints the number of the instances being created. The work done in this method is not necessarily associated with the class or any functions, hence its defined as a staticmethod. We print the initial value of the counter count via the class, as MyClass.count. This will print 0since the counter is called before any instances are created. We create three instances from the class MyClass We can check the number of instances created through the status() method and the count counter. Another example:\n[code language=\u0026quot;python\u0026quot;] class Car(object):\ndef sound(): print(\u0026quot;vroom!\u0026quot;) [/code]\nThe code above shows a method which is common to all the Car instances, and is not limited to a specific instance of Car. Hence, this can be called as a staticmethod since it's not necessarily bound to a Class or Instance to be called.\n[code language=\u0026quot;python\u0026quot;] class Car(object):\n@staticmethod def sound(): print(\u0026quot;vroom!\u0026quot;) [/code]\nClass methods We can define functions/methods specific to classes. These are called Class methods.\nThe speciality of a class methods is that an instance is not required to access a class method. It can be called directly via the Class name.\nClass methods are used when it's not necessary to instantiate a class to access a method.\nNOTE: A method can be set as a Class method using the decorator @classmethod.\nExample:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object): value = 10\n@classmethod def my_func(cls): print(\u0026quot;Hello\u0026quot;) [/code]\nNOTE: Class methods have cls as their first argument, instead of self.\nExample:\n[code language=\u0026quot;python\u0026quot;] class MyClass(object): count = 0\ndef __init__(self, val): self.val = val MyClass.count += 1\ndef set_val(self, newval): self.val = newval\ndef get_val(self): return self.val\n@classmethod def get_count(cls): return cls.count\nobject_1 = MyClass(10) print(\u0026quot;\\nValue of object : %s\u0026quot; % object_1.get_val()) print(MyClass.get_count())\nobject_2 = MyClass(20) print(\u0026quot;\\nValue of object : %s\u0026quot; % object_2.get_val()) print(MyClass.get_count())\nobject_3 = MyClass(40) print(\u0026quot;\\nValue of object : %s\u0026quot; % object_3.get_val()) print(MyClass.get_count()) [/code] Here, we use a get_count() function to get the number of times the counter was incremented. The counter is incremented each time an instance is created.\nSince the counter is not really tied with the instance but only counts the number of instance, we set it as a classmethod, and calls it each time using MyClass.get_count()when an instance is created. The output looks as following:\n[code language=\u0026quot;bash\u0026quot;] # python classmethod.py\nValue of object : 10 1\nValue of object : 20 2\nValue of object : 40 3 [/code]\nCourtsey: This was written as part of studying class and static methods. Several articles/videos have helped including but not limited to the following:\nhttps://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/ Python beyond the basics - Object Oriented Programming - O'Reilly Learning Paths ","link":"https://arvimal.github.io/posts/2016/06/instance-class-static-method-oop/","section":"posts","tags":null,"title":"Instance, Class, and Static methods - Object Oriented Programming"},{"body":"Magic methods _M_agic methods are special methods which can be defined (or already designed and available) to act on objects.\nMagic methods start and end with underscores \u0026quot;__\u0026quot;, and are not implicitly called by the user even though they can be. Most magic methods are used as syntactic sugar by binding it to more clear/easy_to_understand keywords.\nPython is mostly objects and method calls done on objects. Many available functions in Python are actually tied to magic methods_._ Let's checkout a few examples.\nExample 0:\n[code language=\u0026quot;python\u0026quot;] In [1]: my_var = \u0026quot;Hello!\u0026quot;\nIn [2]: print(my_var) Hello!\nIn [3]: my_var.__repr__() Out[3]: \u0026quot;'Hello!'\u0026quot; [/code] As we can see, the __repr__() magic method can be called to print the object, ie.. it is bound to the print() keyword.\nThis is true for many other builtin keywords/operators as well.\nExample 1:\n[code language=\u0026quot;python\u0026quot;] In [22]: my_var = \u0026quot;Hello, \u0026quot; In [23]: my_var1 = \u0026quot;How are you?\u0026quot;\nIn [24]: my_var + my_var1 Out[24]: 'Hello, How are you?'\nIn [25]: my_var.__add__(my_var1) Out[25]: 'Hello, How are you?' [/code] Here, Python interprets the + sign as a mapping to the magic method __add__(), and calls it on the L-value (Left hand object value) my_var, with the R-value (Right hand object value) as the argument.\nWhen a builtin function is called on an object, in many cases it is mapped to the magic method.\nExample 2:\n[code language=\u0026quot;python\u0026quot;] In [69]: my_list_1 = ['a', 'b', 'c', 'd']\nIn [70]: 'a' in my_list_1 Out[70]: True\nIn [71]: my_list_1.__contains__(\u0026quot;a\u0026quot;) Out[71]: True [/code]\nThe in builtin is mapped to the __contains__()method.\nThe methods available for an object should mostly be dependent on the type of the object.\nExample 3:\n[code language=\u0026quot;python\u0026quot; wraplines=\u0026quot;true\u0026quot;] In [59]: my_num = 1\nIn [60]: type(my_num) Out[60]: int\nIn [61]: my_num.__doc__ Out[61]: Out[61]: \u0026quot;int(x=0) -\u0026gt; int or long\\nint(x, base=10) -\u0026gt; int or long\\n\\nConvert a number or string to an integer, or return 0 if no arguments\\nare given. ....\u0026gt;\u0026gt;\u0026gt;\nIn [62]: help(my_num) class int(object) | int(x=0) -\u0026gt; int or long | int(x, base=10) -\u0026gt; int or long | | Convert a number or string to an integer, or return 0 if no arguments | are given. If x is floating point, the conversion truncates towards zero. | If x is outside the integer range, the function returns a long instead.\n[/code]\nFrom the tests above, we can understand that the help() function is actually mapped to the object.__doc__ magic method. It's the same doc string that __doc__ and help() uses.\nNOTE: Due to the syntax conversion (+ to __add__(),and other conversions), operators like + , in, etc.. are also called Syntactic sugar.\nWhat is Syntactic sugar? _A_ccording to Wikipedia, Syntact sugar is:\nIn computer science, syntactic sugar is syntax within a programming language that is designed to make things easier to read or to express. It makes the language \u0026quot;sweeter\u0026quot; for human use: things can be expressed more clearly, more concisely, or in an alternative style that some may prefer.\nHence, magic methods can be said to be Syntactic sugar. But it's not just magic methods that are mapped to syntactic sugar methods, but higher order features such as Decorators are as well.\nExample 4:\n[code language=\u0026quot;python\u0026quot;] def my_decorator(my_function): def inner_decorator(): print(\u0026quot;This happened before!\u0026quot;) my_function() print(\u0026quot;This happens after \u0026quot;) print(\u0026quot;This happened at the end!\u0026quot;) return inner_decorator\ndef my_decorated(): print(\u0026quot;This happened!\u0026quot;)\nvar = my_decorator(my_decorated)\nif __name__ == '__main__': var() [/code] The example above borrows from one of the examples in the post on Decorators.\nHere, my_decorator() is a decorator and is used to decorate my_decorated(). But rather than calling the decorator function my_decorator() with the argument my_decorated(), the above code can be syntactically sugar-coated as below:\n[code language=\u0026quot;python\u0026quot;] def my_decorator(my_function): def inner_decorator(): print(\u0026quot;This happened before!\u0026quot;) my_function() print(\u0026quot;This happens after \u0026quot;) print(\u0026quot;This happened at the end!\u0026quot;) return inner_decorator\n@my_decorator def my_decorated(): print(\u0026quot;This happened!\u0026quot;)\nif __name__ == '__main__': my_decorated() [/code] Observing both code snippets, the decorator is syntactically sugar coated and called as:\n@my_decorator\ninstead of instantiating the decorator with the function to be decorated as an argument, ie..\nvar = my_decorator(my_decorated)\nA few syntax resolution methods: 'name' in my_list -\u0026gt; my_list.__contains__('name') len(my_list) -\u0026gt; my_list.__len__() print(my_list) -\u0026gt; my_list.__repr__() my_list == \u0026quot;value\u0026quot; -\u0026gt; my_list.__eq__(\u0026quot;value\u0026quot;) my_list[5] -\u0026gt; my_list.__getitem__(5) my_list[5:10] -\u0026gt; my_list.__getslice__(5, 10) NOTE: This article is written from the notes created while learning magic methods. The following articles (along with several others) were referred as part of the process.\nA Guide to Python's Magic Methods, by Rafe Kettler Special method names, The Official Python 3 documentation ","link":"https://arvimal.github.io/posts/2016/06/magic-methods-in-python/","section":"posts","tags":["programming","python"],"title":"Magic methods and Syntactic sugar in Python"},{"body":"","link":"https://arvimal.github.io/tags/classmethod/","section":"tags","tags":null,"title":"classmethod"},{"body":"","link":"https://arvimal.github.io/tags/decorators/","section":"tags","tags":null,"title":"decorators"},{"body":"_D_ecorators are wrapper functions (or classes) that wrap and modify another function (or class), and change it's behavior as required. Decorators help to modify your code without actually modifying the working function/class itself.\nThere are several inbuilt Decorators in Python, such as @classmethod and @staticmethod. Examples on these are due for another post.\nDecorators are called to act upon a function or class, by mentioning the Decorator name just above the function/class.\nDecorators are written such as it returns a function, rather than output something.\nExample 0:\n[code language='python'] @my_decorator def my_func(): print(\u0026quot;Hello\u0026quot;)\nmy_func() [/code]\nIn the above code snippet, when my_func() is called, the python interpreter calls the decorator function my_decorator, executes it, and then passes the result to my_func().\nThe example above doesn't do anything worth, but the following example should help to get a better idea.\nNOTE: The examples below are taken from the excellent talks done by Jillian Munson (in PyGotham 2014) and Mike Burns for ThoughtBot. The URLs are at [1] and [2]. All credit goes to them.\nExample 1:\n[code language=\u0026quot;python\u0026quot;] def my_decorator(my_function): def inner_decorator(): print(\u0026quot;This happened before!\u0026quot;) my_function() print(\u0026quot;This happens after \u0026quot;) print(\u0026quot;This happened at the end!\u0026quot;) return inner_decorator\n@my_decorator def my_decorated(): print(\u0026quot;This happened!\u0026quot;)\nif __name__ == '__main__': my_decorated() [/code]\nComponents: A function named my_decorated(). A decorator function named my_decorator(). The decorator function my_decorator() has a function within itself named inner_decorator(). The decorator function my_decorator(), returns the inner function inner_decorator(). Every function should return a value, if not it defaults to None. my_decorator() decorator should return the inner_decorator() inner function, else the decorator cannot be used with the my_decorated() function. To understand this, test with 'return None' for the decorator function my_decorator(). The inner function inner_decorator() is the one that actually decorates (modifies) the function my_decorated(). The decorator function is called on the function my_decorated() using the format @my_decorator. The decorator function takes an argument, which can be named whatever the developer decides. When the decorator function is executed, the argument is replaced with the function name on which the decorator is executed. In our case, it would be my_decorated() How does the code work? The function my_decorated() is called. The interpreter sees that the decorator @my_decorator is called wrt this function. The interpreter searches for a function named my_decorator()and executes it. Since the decorator function returns the inner function inner_decorator(), the python interpreter executes the inner function. It goes through each steps, reaches my_function() , and gets it executed. Once that function is executed, it goes back and continues with the execution of the decorator my_decorator(). Output: [code language=\u0026quot;bash\u0026quot;] # python decorators-1.py This happened before! # Called from the decorator This happened! # Called from the function This happens after # Called from the decorator This happened at the end! # Called from the decorator [/code]\nExample 2:\n[code language=\u0026quot;python\u0026quot;] def double(my_func): def inner_func(a, b): return 2 * my_func(a, b) return inner_func\n@double def adder(a, b): return a + b\n@double def subtractor(a, b): return a - b\nprint(adder(10, 20)) print(subtractor(6, 1)) [/code]\nComponents: Two functions named adder() and subtractor(). A decorator function named double(). The decorator has an inner function named inner_func() which does the actual intended work. The decorator returns the value of the inner function inner_func() Both the adder() and subtractor()functions are decorated with the decorator double() How does the code work? We call the adder() and subtractor() functions with a print(), since the said functions don't print by default (due to the return statement). The python interpreter sees the decorator @double and calls it. Since the decorator returns the inner function inner_func(), the interpreter executes it. The decorator takes an argument my_func, which is always the function on which the decorator is applied, ie.. in our case my_case == adder()and my_case == subtractor(). The inner function within the decorator takes arguments, which are the arguments passed to the functions that are being decorated. ie.. Any arguments passed to adder() and subtractor()are passed to inner_func(). The statement return 2 * my_func(a, b) returns the value of : 2 x adder(10, 20) 2 x subtractor(6, 1) Output: [code language=\u0026quot;bash\u0026quot;] # python decorators-2.py 60 10 [/code]\nInbuilt decorators such as @staticmethod and @classmethod will be discussed in an upcoming post.\nNOTE: To see how decorators are syntactically sugar coated, read Magic methods and Syntactic sugar in Python\n","link":"https://arvimal.github.io/posts/2016/05/decorators-object-oriented-programming/","section":"posts","tags":["classmethod","decorators","object-oriented-programming","programming","python","staticmethod"],"title":"Decorators - Object Oriented Programming"},{"body":"","link":"https://arvimal.github.io/tags/staticmethod/","section":"tags","tags":null,"title":"staticmethod"},{"body":"Ceph OSD daemons need to ensure that the neighbouring OSDs are functioning properly so that the cluster remains in a healthy state.\nFor this, each Ceph OSD process (ceph-osd) sends a heartbeat signal to the neighbouring OSDs. By default, the heartbeat signal is sent every 6 seconds [1], which is configurable of course.\nIf the heartbeat check from one OSD doesn't hear from the other within the set value for `osd_heartbeat_grace` [2], which is set to 20 seconds by default, the OSD that sends the heartbeat check reports the other OSD (the one that didn't respond within 20 seconds) as down, to the MONs. Once an OSD reports three times that the non-responding OSD is indeed `down`, the MON acknowledges it and mark the OSD as down.\nThe Monitor will update the Cluster map and send it over to the participating nodes in the cluster.\nWhen an OSD can't reach another OSD for a heartbeat, it reports the following in the OSD logs:\nosd.510 1497 heartbeat_check: no reply from osd.11 since back 2016-04-28 20:49:42.088802\nIn Ceph Jewel, the MONs require a minimum of two ceph OSDs report a specific OSD as down from two nodes which are in different CRUSH subtrees, in order to actually mark the OSD as down. These are controlled by the following tunables :\nFrom 'common/config_opts.h':\n[1] OPTION(mon_osd_min_down_reporters, OPT_INT, 2) // number of OSDs from different subtrees who need to report a down OSD for it to count\n[2] OPTION(mon_osd_reporter_subtree_level , OPT_STR, \u0026quot;host\u0026quot;) // in which level of parent bucket the reporters are counted\nImage Courtsey : Red Hat Ceph Storage 1.3.2 Configuration guide\n","link":"https://arvimal.github.io/posts/2016/05/ceph-osd-heartbeats/","section":"posts","tags":["ceph","monitors"],"title":"Ceph OSD heartbeats"},{"body":"","link":"https://arvimal.github.io/tags/monitors/","section":"tags","tags":null,"title":"monitors"},{"body":"Many a user wants to know if a Ceph cluster installation has been done to a specific suggested guideline.\nTechnologies like RAID is better avoided in Ceph due to an additional layer, which Ceph already takes care of.\nI've started writing a tool which can be run from the Admin node, and it aims to check various such points.\nThe code can be seen at https://github.com/arvimal/ceph_check\nThe work is slow, really slow, due to my daily work, procrastination, and what not, even though I intend to finish this fast.\n","link":"https://arvimal.github.io/posts/2016/05/ceph-check/","section":"posts","tags":null,"title":"`ceph-check` - A Ceph installation checker"},{"body":"","link":"https://arvimal.github.io/tags/big-o-notation/","section":"tags","tags":null,"title":"big-o-notation"},{"body":"Efficiency or Complexity is how well you're using your resources to get your code run.\nEfficiency can be calculated on the basis of how much time your code takes to run/execute.\nUnderstanding the efficiency of your code can help to reduce the complexity, thus improving the runtime efficiency further. Getting the same job done in less time and less system resources is always good.\nOnce you find the efficiency of your program, you can start to find ways for:\nReducing the complexity (or increase the efficiency) which will reduce the program run time, and hence free the computer resources in a proportional rate. Try to maintain a constant or reduced run time for a growing data set, which will help your program to fare well when the input size grows. In Computer Science, the `Big O` notation is used to indicate the effieciency or complexity of a program. It is denoted by 'O(n)', where 'n' is a mathematical function to denote the input. This is\nSome examples:\nO(n) O(n³) O(n log(n)) O(√n) O(O(n) + 1) or O(1)\nEfficiency can be measures on the best, average, and worst cases. For example, consider finding a specific alphabet from a list of all the alphabets jumbled up.\nThe worst case is your program running through all 26 iterations and finding the alphabet as the last value. The best case is when your program finds it in the first iteration itself. The average is when your program finds the alphabet somewhere around 12-15 iterations (ie.. half of the worst case scenario). Study of Data structures and Algorithms aim to study program complexities, and to reduce it as much as possible.\nAlgorithms are a series of well-defined steps you follow to solve a problem, whereas Data Structures are specific structures by which how you layout your data. Application of well-known algorithms in a program can help in reducing the run time.\nMore on Time complexity and the Big O notation can be read at:\nhttps://en.wikipedia.org/wiki/Time_complexity https://en.wikipedia.org/wiki/Big_O_notation\n","link":"https://arvimal.github.io/posts/2016/05/big-o-notation/","section":"posts","tags":["algorithms","big-o-notation","code-complexity","data-structures","on"],"title":"Code complexity - The Big O notation [O(n)]"},{"body":"","link":"https://arvimal.github.io/tags/code-complexity/","section":"tags","tags":null,"title":"code-complexity"},{"body":"Arrays are a commonly used data structure, and is one of the first a DS student looks into.\nIt is created as a collection of memory addresses which are contiguous in memory. These memory locations store data of a specific type depending on the array's type.\nAdvantages:\nArrays are easier to create since the size and type is mentioned at the creation time. Arrays have constant access/lookup time since the lookup is done by accessing the memory location as an offset from the base/first element. Hence the complexity will be O(1). Arrays are contiguous in memory, ie.. a 10 cell array can start at perhaps 0x0001 and end at 0x0010. Disadvantages:\nThe size of an array has to be defined at the time of its creation. This make the size static, and hence cannot be resized later. An array can only accomodate a specific data type. The type of data an array can store has to be defined at creation time. Hence, if an array is set to store integers, it can only store integers in each memory location. Since the size of an array is set at the time of creation, allocating an array may fail depending on the size of the array and the available memory on the machine. Inserting an element into an array can be expensive depending on the location. To insert an element at a particular position, for example 'n', the element already has to be moved to 'n + 1', the element at 'n + 1' to 'n + 2' etc.. Hence, if the position to which the element is written to is at the starting of the array, the operation will be expensive. But if the position is at the starting, it won't be. What is the lookup time in an array?\nThe elements in an array are continuguous to each other. The address of an element is looked up as an `offset` of the primary or base element. Hence, the lookup of any element in the array is constant and can be denoted by O(1).\nArrays in Python\nPython doesn't have a direct implementation of an Array. The one that closely resembles an array in python is a `list`.\nThe major differences between an array and a list are:\nThe size of lists are not static. It can be grown or shrinked using the `append` and `remove` methods. Arrays are static in size. lists can accomodate multiple data types, while arrays cannot. [code language=\u0026quot;python\u0026quot;] In [1]: mylist = []\nIn [2]: type(mylist) Out[2]: list\nIn [3]: mylist.append(\u0026quot;string\u0026quot;)\nIn [4]: mylist.append(1000)\nIn [5]: mylist Out[5]: ['string', 1000] [/code]\nTime complexity of Arrays\nIndexing - O(1) Insertion/Deletion at beginning - O(n) (If the array has space to shift the elements) Insertion/Deletion at the end - O(1) (If the array has space at the end) Deletion at the end - O(1) (Since it doesn't have to move any elements and reorder) Insertion at the middle - O(n) (Since it requires to move the elements to the right and reorder) Deletion at the middle - O(n) (Since it requires to delete the element and move the ones from the right to the left) The 'array' module\nPython comes with a module named 'array' which emulates the behavior of arrays.\n[code language=\u0026quot;python\u0026quot;] In [24]: import array\nIn [25]: myarray = array.array('i', [1,2,3,4])\nIn [26]: myarray Out[26]: array('i', [1, 2, 3, 4]) [/code]\n","link":"https://arvimal.github.io/posts/2016/05/data-structures-arrays/","section":"posts","tags":null,"title":"Data Structures - Arrays"},{"body":"","link":"https://arvimal.github.io/tags/data-structures/","section":"tags","tags":null,"title":"data-structures"},{"body":"To get a MON map or an OSD map of a specific epoch, use:\n# ceph osd getmap # ceph mon getmap The map can be forwarded to a file as following:\n# ceph osd getmap -o /tmp/ceph_osd_getmap.bin\nThis would be in a binary format, and hence will need to be dumped to a human-readable form.\n# osdmaptool --print /tmp/ceph-osd-getmap.bin\nThis will print the current OSD map, similar to the output of 'ceph osd dump'.\nWhere this command shines is when you can fetch maps from previous epochs, and pull information on specific placement groups in those epochs.\nFor example, I've had all the OSDs on one of my node down some time back (in a previous epoch). The ability to query a previous epoch gives the administrator the power to understand how exactly the cluster was at a specific time period.\n","link":"https://arvimal.github.io/posts/2016/05/ceph-mon-osd-map-at-epoch/","section":"posts","tags":null,"title":"How to get a Ceph MON/OSD map at a specific epoch?"},{"body":"","link":"https://arvimal.github.io/tags/on/","section":"tags","tags":null,"title":"on"},{"body":"This is a crude bash one-liner I did to get the details of all the RBD images, as well as the information on snapshots and clones created from them.\n[code language=\u0026quot;bash\u0026quot;] # for pool in `rados lspools`; do echo \u0026quot;POOL :\u0026quot; $pool; rbd ls -l $pool; echo \u0026quot;-----\u0026quot;; done [/code]\nThis will print an output similar to the following:\n[code language=\u0026quot;bash\u0026quot;] POOL : rbd NAME SIZE PARENT FMT PROT LOCK test_img 10240M 1 test_img2 1024M 2 test_img2@snap2 1024M 2 yes ----- POOL : .rgw.root ----- POOL : .rgw.control ----- POOL : .rgw ----- POOL : .rgw.gc ----- POOL : .users.uid ----- POOL : .users ----- POOL : .users.swift ----- POOL : .users.email ----- POOL : .rgw.buckets.index ----- POOL : images NAME SIZE PARENT FMT PROT LOCK clone1 1024M rbd/test_img2@snap2 2 ----- [/code]\n","link":"https://arvimal.github.io/posts/2015/10/list-ceph-pools-with-snapshots-and-rbd/","section":"posts","tags":null,"title":"List RBD images, snapshots, and clones in Ceph pools"},{"body":"","link":"https://arvimal.github.io/tags/enumerate/","section":"tags","tags":null,"title":"enumerate"},{"body":"","link":"https://arvimal.github.io/tags/range/","section":"tags","tags":null,"title":"range"},{"body":"The usual way to iterate over a range of numbers or a list in python, is to use range().\nExample 0:\n[code language=\u0026quot;python\u0026quot;] colors = [\u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;]\nfor i in range(len(colors)): print(i, colors[i]) [/code]\nThis should output:\n[code language=\u0026quot;bash\u0026quot;] (0, 'yellow') (1, 'red') (2, 'blue') (3, 'white') (4, 'black') [/code]\nprint(), by default, returns a tuple. If we want to print it in a more presentable way, we’ll need to find the indice at which each value is, and print that as well. Re-write the code a bit, to achieve the desired output:\n[code language=\u0026quot;python\u0026quot;] colors = [\u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;]\nfor i in range(len(colors)): color = colors[i] print(\u0026quot;%d: %s\u0026quot; % (i, color)) [/code]\nThis should print:\n[code language=\u0026quot;bash\u0026quot;] 0: yellow 1: red 2: blue 3: white 4: black [/code]\nWe can see that the above output starts with ‘0’ since python starts counting from ‘0’. To change that to ‘1’, we’ll need to tweak the print() statement.\n[code language=\u0026quot;python\u0026quot;] colors = [\u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;]\nfor i in range(len(colors)): color = colors[i] print(\u0026quot;%d: %s\u0026quot; % (i + 1, color)) [/code]\nThis should print:\n[code language=\u0026quot;bash\u0026quot;] 1: yellow 2: red 3: blue 4: white 5: black [/code]\nEven though the above code snippet isn’t that complex, a much better way exists to do this. This is where the builtin function enumerate() comes in.\nenumerate() returns a tuple when passed an object which supports iteration, for example, a list. It also supports a second argument named 'start' which default to 0, and can be changed depending on where to start the order. We’ll check what 'start' is towards the end of this article.\n[code language=\u0026quot;python\u0026quot;] colors = [\u0026quot;yellow\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;white\u0026quot;, \u0026quot;black\u0026quot;] print(list(enumerate(colors))) [/code]\nThis returns a list of a tuples.\n[code language=\u0026quot;bash\u0026quot;] [(0, 'yellow'), (1, 'red'), (2, 'blue'), (3, 'white'), (4, 'black')] [/code]\nTo get to what we desire, modify it as:\n[code language=\u0026quot;python\u0026quot;] for i, color in enumerate(colors): print('%d: %s' % (i, color)) [/code]\nThis outputs:\n[code language=\u0026quot;bash\u0026quot;] 0: yellow 1: red 2: blue 3: white 4: black [/code]\nRemember that we talked about that enumerate() takes a second value named 'start' which defaults to ‘0’? Let’s check how that’ll help here.\nThe above output starts with ‘0’. 'start' can help to change that.\n[code language=\u0026quot;python\u0026quot;] for i, color in enumerate(colors, start=1): print('%d: %s' % (i, color)) [/code]\nThis should change the output as:\n[code language=\u0026quot;bash\u0026quot;] 1: yellow 2: red 3: blue 4: white 5: black [/code]\n","link":"https://arvimal.github.io/posts/2015/10/range-and-enumerate/","section":"posts","tags":["enumerate","programming","python","range"],"title":"range() and enumerate()"},{"body":"In certain cases, a Ceph cluster may move away from an HEALTHY state due to “unfound” objects.\nA “ceph -s” should show if you have any unfound objects. So, what are unfound objects? How does an object become “unfound”? This article tries to explain why/how “unfound” objects come into existence.\nLet’s look into the life cycle of a write to a pool.\nThe client contacts a Ceph monitor and fetches the CRUSH map, which includes: MON map OSD map PG map CRUSH map MDS map Once the client has the maps, the Ceph client-side algorithm breaks the data being written into objects (the object size depends on the client side configuration). Clients such as RBD and RGW uses a 4MB object size, but RADOS doesn’t actually have such a limitation.\nEach pool has a set of Placement Groups (PG) assigned to it at the time of creation, and the client always writes to a pool. Since the client has the maps which talks about the entire cluster, it knows the placement groups within the pool which it is writing to, and the OSDs assigned for each placement group. The client talks to the OSDs directly without going over any other path, such as a monitor.\nThe PG map will have the ACTING and UP OSD sets for each PG. To understand the ACTING set and UP set for the PGs, as well as a plethora of other information, use :\n[code language=\u0026quot;bash\u0026quot;] # ceph pg dump [/code]\nThe ACTING set is the current active set of OSDs that stores the replica sets for that particular PG. The UP set is the set of OSDs that are currently up and running. Usually, the ACTING set and UP set are the same. When an OSD in the ACTING set is not reachable, other OSDs wait for 5 minutes (which is configurable) for it to come back online (this is checked with a hearbeat).\nThe said OSD is removed out of the UP set when it is not accessible. If it doesn’t come back online within the configured period, the said OSD is marked out of the ACTING set, as well as the UP set. When it comes back, it is added back to the ACTING/UP set and a peering happens where the data is synced back.\nLet’s discuss the scenario where an “unfound” object came come into existence. Imagine a pool with a two replica configuration. A write that goes into the pool is split into objects and stored in the OSDs which are in the ACTIVE set of a PG.\nOne OSD in the ACTING set goes down. The write is done on the second OSD which is UP and ACTING. The first OSD which went down, came back up. The peering process started between the first OSD (that came back), and the second OSD (that serviced the write). Peering refers to the process of arriving at an understanding on the object states between the OSDs in an ACTING set, and sync up the metadata/data between them. Both the OSDs reach an understanding on which objects needs to be synced. The second OSD that had the objects ready to be synced, went down before the sync process starts or is in midway. In this situation, the first OSD knows about the objects that was written to the second OSD, but cannot probe it. The first OSD will try to probe possible locations for copies, provided there are more replicas. If the OSD is able to find other locations, the data will be synced up.\nBut in case there are no other copies, and the OSD with the only copy is not coming up anytime soon (perhaps a disk crash, file system corruption etc..) the only way is to either mark the object as “lost”, or revert it back to the previous version. Reverting to a previous version may not be possible for a new object, and in such cases the only way would be to mark it as “lost” or copy from a backup.\n1. For a new object without a previous version:\n[code language=\u0026quot;bash\u0026quot;] # ceph pg {pg.num} mark_unfound_lost delete [/code]\n2. For an object which is likely to have a previous version:\n[code language=\u0026quot;bash\u0026quot;] # ceph pg {pg.num} mark_unfound_lost revert [/code]\nNOTE: The upstream Ceph documentation has an excellent write-up about “unfound” objects here.\nI suggest reading the documentation prior taking any sort of action in a case where you see “unfound” objects in your cluster.\n","link":"https://arvimal.github.io/posts/2015/10/unfound-objects-in-ceph-cluster/","section":"posts","tags":null,"title":"Ceph and unfound objects"},{"body":"I recently came across a scenario where the objects in a RADOS pool used for an RBD block device doesn’t get removed, even if the files created through the mount point were removed.\nI had an RBD image from an RHCS1.3 cluster mapped to a RHEL7.1 client machine, with an XFS filesystem created on it, and mounted locally. Created a 5GB file, and I could see the objects being created in the rbd pool in the ceph cluster.\n1.RBD block device information\n[code language=\u0026quot;bash\u0026quot;] # rbd info rbd_img rbd image 'rbd_img': size 10240 MB in 2560 objects order 22 (4096 kB objects) block_name_prefix: rb.0.1fcbe.2ae8944a format: 1 [/code]\nAn XFS file system was created on this block device, and mounted at /test.\n2.Write a file onto the RBD mapped mount point. Used ‘dd’ to write a 5GB file.\n[code language=\u0026quot;bash\u0026quot;] # dd if=/dev/zero of=/mnt/rbd_image.img bs=1G count=5 5+0 records in 5+0 records out 5368709120 bytes (5.4 GB) copied, 8.28731 s, 648 MB/s [/code]\n3.Check the objects in the backend RBD pool\n[code language=\u0026quot;bash\u0026quot;] # rados -p rbd ls | wc -l \u0026lt; Total number of objects in the 'rbd' pool\u0026gt; [/code]\n4.Delete the file from the mount point.\n[code language=\u0026quot;bash\u0026quot;] # rm /test/rbd_image.img -f # ls /test/ --NO FILES LISTED-- [/code]\n5.List the objects in the RBD pool\n[code language=\u0026quot;bash\u0026quot;] # rados -p rbd ls | wc -l \u0026lt; Total number of objects in the 'rbd' pool \u0026gt; [/code]\nThe number of objects doesn’t go down as we expect, after the file deletion. It remains the same, wrt to step 3.\nWhy does this happen? This is due to the fact that traditional file systems do not delete the underlying data blocks even if the files are deleted.\nThe process of writing a file onto a file system involves several steps like finding free blocks and allocating them for the new file, creating an entry in the directory entry structure of the parent folder, setting the name and inode number in the directory entry structure, setting pointers from the inode to the data blocks allocated for the file etc..\nWhen data is written to the file, the data blocks are used to store the data. Additional information such as the file size, access times etc.. are updated in the inode after the writes.\nDeleting a file involves removing the pointers from the inode to the corresponding data blocks, and also clearing the name\u0026lt;-\u0026gt;inode mapping from the directory entry structure of the parent folder. But, the underlying data blocks are not cleared off, since that is a high I/O intensive operation. So, the data remains on the disk, even if the file is not present. A new write will make the allocator take these blocks for the new data, since they are marked as not-in-use.\nThis applies for the files created on an RBD device as well. The files created on top of the RBD-mapped mount point will ultimately be mapped to objects in the RADOS cluster. When the file is deleted from the mount point, since the entry is removed, it doesn’t show up in the mount point.\nBut, since the file system doesn’t clear off the underlying block device, the objects remain in the RADOS pool. These would be normally over-written when a new file is created via the mount point.\nBut this has a catch though. Since the pool contains the objects even if the files are deleted, it consumes space in the rados pool (even if they'll be overwritten). An administrator won't be able to get a clear understanding on the space usage, if the pool is used heavily, and multiple writes are coming in.\nIn order to clear up the underlying blocks, or actually remove them, we can rely on the TRIM support most modern disks offer. Read more about TRIM at Wikipedia.\nTRIM is a set of commands supported by HDD/SSDs which allow the operating systems to let the disk know about the locations which are not currently being used. Upon receiving a confirmation from the file system layer, the disk can go ahead and mark the blocks as not used.\nFor the TRIM commands to work, the disks and the file system has to have the support. All the modern file systems have built-in support for TRIM. Mount the file system with the 'discard' option, and you're good to go.\n[code language=\u0026quot;bash\u0026quot;] # mount -o discard /dev/rbd{X}{Y} /{mount-point} [/code]\n","link":"https://arvimal.github.io/posts/2015/10/rbd-object-remain-in-pool-after-deletion/","section":"posts","tags":["ceph","discard","fstrim","objects","rados","rados-block-device","rbd","trim"],"title":"Ceph Rados Block Device (RBD) and TRIM"},{"body":"","link":"https://arvimal.github.io/tags/discard/","section":"tags","tags":null,"title":"discard"},{"body":"","link":"https://arvimal.github.io/tags/fstrim/","section":"tags","tags":null,"title":"fstrim"},{"body":"","link":"https://arvimal.github.io/tags/rados-block-device/","section":"tags","tags":null,"title":"rados-block-device"},{"body":"","link":"https://arvimal.github.io/tags/rbd/","section":"tags","tags":null,"title":"rbd"},{"body":"","link":"https://arvimal.github.io/tags/trim/","section":"tags","tags":null,"title":"trim"},{"body":"Ceph supports custom rulesets via CRUSH, which can be used to sort hardware based on various features such as speed and other factors, set custom weights, and do a lot of other useful things.\nPools, or the buckets were the data is written to, can be created on the custom rulesets, hence positioning the pools on specific hardware as per the administrator's need.\nA large Ceph cluster may have lots of pools and rulesets specific for multiple use-cases. There may be times when we'd like to understand the pool to ruleset mapping.\nThe default CRUSH ruleset is named ‘replicated_ruleset’. The available CRUSH rulesets can be listed with:\n$ ceph osd crush rule ls\nOn a fresh cluster, or one without any custom rulesets, you’d find the following being printed to stdout.\n# ceph osd crush rule ls [ \u0026quot;replicated_ruleset\u0026quot; ]\nI’ve got a couple more on my cluster, and this is how it looks:\n# ceph osd crush rule ls [ \u0026quot;replicated_ruleset\u0026quot;, \u0026quot;replicated_ssd\u0026quot;, \u0026quot;erasure-code\u0026quot;]\nSince this article looks into the mapping of pools to CRUSH rulesets, it’d be good to add in how to list the pools, as a refresher.\n# ceph osd lspools\nOn my Ceph cluster, it turned out to be:\n# ceph osd lspools 0 data,1 metadata,2 rbd,21 .rgw,22 .rgw.root,23 .rgw.control,24 .rgw.gc,25 .users.uid,26 .users,27 .users.swift,28 test_pool,\nSince you have the pool name you’re interested in, let’s see how to map it to the ruleset. The command syntax is:\n# ceph osd pool get \u0026lt;pool_name\u0026gt; crush_ruleset\nI was interested to understand the ruleset on which the pool ‘test_pool’ was created. The command to list this was:\n# ceph osd pool get test_pool crush_ruleset crush_ruleset: 1\nPlease note that the rulesets are numbered from ‘0’, and hence ‘1’ would map to the CRUSH ruleset ‘replicated_ssd’.\nWe'll try to understand how a custom ruleset is created, in another article.\n","link":"https://arvimal.github.io/posts/2015/09/find-crush-ruleset-of-a-pool/","section":"posts","tags":null,"title":"Custom CRUSH rulesets and pools"},{"body":"","link":"https://arvimal.github.io/tags/osd/","section":"tags","tags":null,"title":"osd"},{"body":"In case you are trying to get the OSD ID and the corresponding node IP address mappings in a script-able format, use the following command:\n# ceph osd find This will print the OSD number, the IP address, the host name, and the default root in the CRUSH map, as a python dictionary.\n# ceph osd find 2 { \u0026quot;osd\u0026quot;: 2, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.112:6800\\/5311\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node4\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}}\nThe output is in json format, which has a key:value format. This can be parsed using awk/sed, or any programming languages that support json. All recent ones do.\nFor a listing of all the OSDs and related information, get the number of OSDs in the cluster, and then use that number to probe the OSDs.\n# for i in `seq 0 $(ceph osd stat | awk {'print $3'})`; do\nceph osd find $i; echo; done\nThis should output:\n{ \u0026quot;osd\u0026quot;: 0, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.244:6805\\/2579\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node3\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;ssd\u0026quot;}} { \u0026quot;osd\u0026quot;: 1, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.244:6800\\/955\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node3\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;ssd\u0026quot;}} { \u0026quot;osd\u0026quot;: 2, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.112:6800\\/5311\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node4\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 3, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.112:6805\\/5626\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node4\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 4, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.82:6800\\/4194\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node5\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 5, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.82:6805\\/4521\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node5\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 6, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.73:6801\\/5614\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node2\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;ssd\u0026quot;}} { \u0026quot;osd\u0026quot;: 7, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.73:6800\\/1719\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node2\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;ssd\u0026quot;}} { \u0026quot;osd\u0026quot;: 8, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.10:6805\\/5842\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node6\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 9, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.10:6800\\/4356\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node6\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 10, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.109:6800\\/4517\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node7\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}} { \u0026quot;osd\u0026quot;: 11, \u0026quot;ip\u0026quot;: \u0026quot;192.168.122.109:6805\\/4821\u0026quot;, \u0026quot;crush_location\u0026quot;: { \u0026quot;host\u0026quot;: \u0026quot;node7\u0026quot;, \u0026quot;root\u0026quot;: \u0026quot;default\u0026quot;}}\n","link":"https://arvimal.github.io/posts/2015/09/listing-osd-nodes-scriptable/","section":"posts","tags":["ceph","osd"],"title":"OSD information in a scriptable format"},{"body":"The MON map is used by the monitors in a Ceph cluster, where they keep track of various attributes relevant to the working of the cluster.\nSimilar to the CRUSH map, a monitor map can be pulled out of the cluster, inspected, changed, and injected back to the monitors, manually. A frequent use-case is when the IP address of a monitor changes and the monitors cannot agree on a quorum.\nMonitors use the monitor map (monmap) to get the details of other monitors. So just changing the monitor address in 'ceph.conf' and pushing the configuration to all the nodes won't help to propagate the changes.\nIn most cases, starting the monitor with a wrong monitor map would make the monitors commit suicide, since they would find conflicting information about themself in the mon map due to the IP address change.\nThere are two methods to fix this problem, the first being adding enough new monitors, let them form a quorum, and remove the faulty monitors. This doesn't need any explanation. The second and more crude way, is to edit the monitor map directly, set the new IP address, and upload the monmap back to the monitors.\nThis article discusses the second method, ie.. how to edit the monmap, and re-inject it back. This can be done using the 'monmap' tool.\n1. As the first step, login to one of the monitors, and get the monitor map:\n# ceph mon getmap -o /tmp/monitor_map.bin\n2. Inspect what the monitor map contains:\n# monmaptool --print /tmp/monitor_map.bin\nAn example from my cluster : # monmaptool --print monmap\nmonmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000 0: 192.168.122.73:6789/0 mon.node2\n3. Remove the node which has the wrong IP address, referring it's hostname\n# monmaptool --rm node2 /tmp/monitor_map.bin\n4. Inspect the monitor map to see if the monitor is indeed removed.\n# monmaptool --print /tmp/monitor_map.bin\nmonmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000\n5. Add a new monitor (or the existing monitor with it's new IP)\n# monmaptool --add node3 192.168.122.76:6789 /tmp/monitor_map.bin\nmonmaptool: monmap file monmap monmaptool: writing epoch 1 to monmap (1 monitors)\n6. Check the monitor map to confirm the changes\n# monmaptool --print monmap\nmonmaptool: monmap file monmap epoch 1 fsid d978794d-5835-4ac3-8fe3-3855b18b9572 last_changed 0.000000 created 0.000000 0: 192.168.122.76:6789/0 mon.node3\n7. Make sure the mon processes are not running on the monitor nodes\n# service ceph stop mon\n8. Upload the changes\n# ceph-mon -i monitor_node --inject-monmap /tmp/mon_map.bin\n9. Start the mon process on each monitor\n# service ceph start mon\n10. Check if the cluster has taken in the changes.\n# ceph -s\n","link":"https://arvimal.github.io/posts/2015/09/view-change-inject-monitor-map/","section":"posts","tags":["ceph","monitors","monmaptool"],"title":"Monitor maps, how to edit them?"},{"body":"","link":"https://arvimal.github.io/tags/monmaptool/","section":"tags","tags":null,"title":"monmaptool"},{"body":"Recently, I had an incident where the OSDs were crashing at the time of startup. Obviously, the next step was to enable debug logs for the OSDs and understand where they were crashing.\nEnabled OSD debug logs dynamically by injecting it with:\n# ceph tell osd.* injectargs --debug-osd 20 --debug-ms 1\nNOTE: This command can be run from the MON nodes.\nOnce this was done, the OSDs were started manually (since it were crashing and not running) and watched out for the next crash. It crashed with the following logs :\n*read_log 107487'1 (0'0) modify f6b07b93/rbd_data.hash/head//12 by client.version date, time *osd/PGLog.cc: In function 'static bool PGLog::read_log(ObjectStore*, coll_t, hobject_t, const pg_info_t\u0026amp;, std::mapeversion_t, hobject_t\u0026amp;, PGLog::IndexedLog\u0026amp;, pg_missing_t\u0026amp;, std::ostringstream\u0026amp;, std::setstd::basic_stringchar *)' thread thread time date, time *osd/PGLog.cc: 809: FAILED assert(last_e.version.version e.version.version)ceph version version-details\n1: (PGLog::read_log(ObjectStore*, coll_t, hobject_t, pg_info_t const\u0026amp;, std::mapeversion_t, hobject_t, std::lesseversion_t, std::allocatorstd::paireversion_t const,hobject_t , PGLog::IndexedLog\u0026amp;, pg_missing_t\u0026amp;, std::basic_ostringstreamchar, std::char_traitschar, std::allocatorchar, std::setstd::string, std::lessstd:string, std::allocatorstd::string *)+0x13ee) [0x6efcae] 2: (PG::read_state(ObjectStore*, ceph::buffer::list\u0026amp;)+0x315) [0x7692f5] 3: (OSD::load_pgs()+0xfff) [0x639f8f] 4: (OSD::init()+0x7bd) [0x63c10d] 5: (main()+0x2613) [0x5ecd43] 6: (__libc_start_main()+0xf5) [0x7fdc338f9af5] 7: /usr/bin/ceph-osd() [0x5f0f69]\nThe above is a log snippet at which the OSD process was crashing. The ceph-osd process was reading through the log areas of each PG in the OSD, and once it reached the problematic PG it crashed due to failing an assert condition.\nChecking the source at 'osd/PGLog.cc', we see that this error is logged from 'PGLog::read_log'.\nvoid PGLog::read_log(ObjectStore *store, coll_t pg_coll, coll_t log_coll, ghobject_t log_oid, const pg_info_tinfo, mapeversion_t, hobject_tdivergent_priors, IndexedLoglog, pg_missing_tmissing, ostringstreamoss, setstring *log_keys_debug) { ... if (!log.log.empty()) { pg_log_entry_t last_e(log.log.back()); assert(last_e.version.version e.version.version); == The assert condition at which read_log is failing for a particular PG assert(last_e.version.epoch = e.version.epoch);\nIn order to make the OSD start, we needed to move this PG to a different location using the 'ceph_objectstore_tool' so that the ceph-osd can bypass the problematic PG. To understand the PG where it was crashing, we had to do some calculations based on the logs.\nThe 'read_log' line in the debug logs contain a hex value after the string \u0026quot;modify\u0026quot; and that is the hash of the PG number. The last number in that series is the pool id (12 in our case). The following python code will help to calculate the PG id based on the arguments passed to it.\nThis program accepts three arguments, the first being the hex value we talked about, the second being the pg_num of the pool, and the third one being the pool id.\n[code language=\u0026quot;python\u0026quot;]\n#!/usr/bin/env python # Calculate the PG ID from the object hash # vimal@redhat.com import sys\ndef pg_id_calc(*args): if any([len(args) == 0, len(args) \u0026gt; 3, len(args) \u0026lt; 3]): help() else: hash_hex = args[0] pg_num = int(args[1]) pool_id = int(args[2]) hash_dec = int(hash_hex, 16) id_dec = hash_dec % pg_num id = hex(id_dec) pg_id = str(pool_id) + \u0026quot;.\u0026quot; + str(id)[2:] print(\u0026quot;\\nThe PG ID is %s\\n\u0026quot; % pg_id)\ndef help(): print(\u0026quot;Usage:\u0026quot;) print(\u0026quot;This script expects the hash (in Hex), pg_num of the pool, and the pool id as arguments, in order\u0026quot;) print(\u0026quot;\\nExample:\u0026quot;) print(\u0026quot;./pg_id_calc.py 0x8e2fe5d7 2048 12\u0026quot;) sys.exit()\nif __name__ == '__main__': pg_id_calc(*sys.argv[1:])\n[/code]\nAn example of the program in action:\n# python pg_id_calc.py 0xf6b07b93 2048 12 The PG ID is 12.393\nOnce we get the PG ID, we can proceed using 'ceph_objectstore_tool' to move the PG to a different location altogether. More on how to use 'ceph_objectstore_tool' in an upcoming journal.\n","link":"https://arvimal.github.io/posts/2015/08/calculate-pg-id-from-ceph-osd-debug-logs/","section":"posts","tags":["ceph","pg","python"],"title":"Calculate a PG id from the hex values in Ceph OSD debug logs"},{"body":"","link":"https://arvimal.github.io/tags/pg/","section":"tags","tags":null,"title":"pg"},{"body":"Understanding the mapping of Pools and Placement Groups can be very useful while troubleshooting Ceph problems.\nA direct method is to dump information on the PGs via :\n# ceph pg dump\nThis command should output something like the following:\npg_stat objects mip degr unf bytes log disklog state 5.7a 0 0 0 0 0 0 0 active+clean\nThe output will have more information, and I've omitted it for the sake of explanation.\nThe first field is the PG ID, which are two values separated by a single dot (.). The left side value is the POOL ID, while the right side value is the actual PG number. It means that a specific PG can only be present under a specific pool, ie.. no PGs can be shared across pools. But please note that OSDs can be shared across multiple PGs.\nTo get the pools and associated numbers, use:\n# ceph osd lspools\n0 data,1 metadata,2 rbd,5 ssdtest,6 ec_pool,\nSo, the PG 5.7a belongs to the pool numbered '5', ie.. 'ssdtest', and the PG number is '7a'.\nThe output of 'ceph pg dump' also shows various important informations such as the Acting OSD set, the primary OSD, the last time the PG was reported, the state of the PG, the time at which a normal scrub as well as a deep-scrub was run etc..\n","link":"https://arvimal.github.io/posts/2015/08/map-pg-to-pool/","section":"posts","tags":["ceph","placement-groups","pool"],"title":"Mapping Placement Groups and Pools"},{"body":"","link":"https://arvimal.github.io/tags/placement-groups/","section":"tags","tags":null,"title":"placement-groups"},{"body":"","link":"https://arvimal.github.io/tags/pool/","section":"tags","tags":null,"title":"pool"},{"body":"In many cases, one would like to understand the journal disk a particular OSD is using. There are two methods to understand this:\na) This is the most direct method, and should give you details on the OSD disks and the corresponding journal disks.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph-disk list\n[/sourcecode]\nThis should output something like:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph-disk list /dev/sda : /dev/sda1 other, xfs, mounted on /boot /dev/sda2 other, LVM2_member /dev/sr0 other, unknown /dev/vda : /dev/vda1 ceph data, active, cluster ceph, osd.0, journal /dev/vda2 /dev/vda2 ceph journal, for /dev/vda1 /dev/vdb : /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdc1 /dev/vdc : /dev/vdc1 ceph journal, for /dev/vdb1 [/sourcecode]\nb) The second method is cruder, and involves listing the OSD mount point on the file system.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ls -l /var/lib/ceph/osd/ceph-0/\ntotal 52 -rw-r--r--. 1 root root 191 Aug 3 18:02 activate.monmap -rw-r--r--. 1 root root 3 Aug 3 18:02 active -rw-r--r--. 1 root root 37 Aug 3 18:02 ceph_fsid drwxr-xr-x. 70 root root 4096 Aug 4 00:38 current -rw-r--r--. 1 root root 37 Aug 3 18:02 fsid lrwxrwxrwx. 1 root root 58 Aug 3 18:02 journal -\u0026gt; /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 -rw-r--r--. 1 root root 37 Aug 3 18:02 journal_uuid -rw-------. 1 root root 56 Aug 3 18:02 keyring -rw-r--r--. 1 root root 21 Aug 3 18:02 magic -rw-r--r--. 1 root root 6 Aug 3 18:02 ready -rw-r--r--. 1 root root 4 Aug 3 18:02 store_version -rw-r--r--. 1 root root 42 Aug 3 18:02 superblock -rw-r--r--. 1 root root 0 Aug 5 13:09 sysvinit -rw-r--r--. 1 root root 2 Aug 3 18:02 whoami\n# ls -l /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 lrwxrwxrwx. 1 root root 10 Aug 5 13:08 /dev/disk/by-partuuid/d9ebc4bd-7b5e-4e12-b909-0c72c4f58ee0 -\u0026gt; ../../vda2\n[/sourcecode]\nAs you can see, the file 'journal' is a symlink to the journal disk. The first method is much easier, but its always better to know how things are layered out underneath.\n","link":"https://arvimal.github.io/posts/2015/08/map-journal-disk-of-a-ceph-osd/","section":"posts","tags":null,"title":"How to identify the journal disk for a Ceph OSD?"},{"body":"","link":"https://arvimal.github.io/tags/calamari/","section":"tags","tags":null,"title":"calamari"},{"body":"'Calamari' is the monitoring interface for a Ceph cluster.\nThe Calamari interface password can be reset/changed using the 'calamari-ctl' command.\n# calamari-ctl change_password --password {password} {user-name}\ncalamari-ctl can also be used to add a user, as well as disable, enable, and rename the user account. A '--help' should print out all the available ones.\n# calamari-ctl --help\n","link":"https://arvimal.github.io/posts/2015/07/reset-calamari-interface-password/","section":"posts","tags":["calamari","ceph"],"title":"Resetting Calamari password"},{"body":"The Ceph monitor store growing to a big size is a common occurrence in a busy Ceph cluster.\nIf a 'ceph -s' takes considerable time to return information, one of the possibility is the monitor database being large.\nOther reasons included network lags between the client and the monitor, the monitor not responding properly due to the system load, firewall settings on the client or monitor etc..\nThe best way to deal with a large monitor database is to compact the monitor store. The monitor store is a leveldb store which stores key/value pairs.\nThere are two ways to compact a levelDB store, either on the fly or at the monitor process startup.\nTo compact the store dynamically, use :\n# ceph tell mon.[ID] compact\nTo compact the levelDB store every time the monitor process starts, add the following in /etc/ceph/ceph.conf under the [mon] section:\nmon compact on start = true\nThe second option would compact the levelDB store each and every time the monitor process starts.\nThe monitor database is stored at /var/lib/ceph/mon//store.db/ as files with the extension '.sst', which is the synonym for 'Sorted String Table'\nTo read more on levelDB, please refer:\nhttps://en.wikipedia.org/wiki/LevelDB\nhttp://leveldb.googlecode.com/svn/trunk/doc/impl.html\nhttp://google-opensource.blogspot.in/2011/07/leveldb-fast-persistent-key-value-store.html\n","link":"https://arvimal.github.io/posts/2015/07/compact-a-ceph-monitor-data-store/","section":"posts","tags":["ceph","leveldb","monitors"],"title":"Compacting a Ceph monitor store"},{"body":"","link":"https://arvimal.github.io/tags/leveldb/","section":"tags","tags":null,"title":"leveldb"},{"body":"","link":"https://arvimal.github.io/tags/scrubbing/","section":"tags","tags":null,"title":"scrubbing"},{"body":"Data Scrubbing is an error checking and correction method or routine check to ensure that the data on file systems are in pristine condition, and has no errors. Data integrity is of primary concern in today's conditions, given the humongous amounts of data being read and written daily.\nA simple example for a scrubbing, is a file system check done on file systems with tools like 'e2fsck' in EXT2/3/4, or 'xfs_repair' in XFS. Ceph also includes a daily scrubbing as well as weekly scrubbing, which we will talk about in detail in another article.\nThis feature is available on most hardware RAID controllers, backup tools, as well as softwares that emulate RAID such as MD-RAID.\nBtrfs is one of the file systems that can schedule a internal scrubbing automatically, to ensure that corruptions are detected and preventive measures taken automatically. Since Btrfs can maintain multiple copies of data, once it finds an error in the primary copy, it can check for a good copy (if mirroring is used) and replace it.\nWe will be looking more into scrubbing, especially how it is implemented in Ceph, and the various tunables, in an upcoming post.\n","link":"https://arvimal.github.io/posts/2015/07/what-is-data-scrubbing/","section":"posts","tags":["ceph","osd","scrubbing"],"title":"What is data scrubbing?"},{"body":"In a previous post, we saw how to dynamically change a tunable on a running Ceph cluster dynamically. Unfortunately, such a change is not permanent, and will revert back to the previous setting once ceph is restarted.\nRather than using the command 'ceph tell', I recently came upon another way to change configuration values.\nWe'll try changing the tunable 'mon_osd_full_ratio' once again.\n1. Get the current setting\n# ceph daemon osd.1 config get mon_osd_full_ratio { \u0026quot;mon_osd_full_ratio\u0026quot;: \u0026quot;0.75\u0026quot;}\n2. Change the configuration value using 'ceph daemon'.\n# ceph daemon osd.1 config set mon_osd_full_ratio 0.85 { \u0026quot;success\u0026quot;: \u0026quot;mon_osd_full_ratio = '0.85' \u0026quot;}\n3. Check if the change has been introduced.\n# ceph daemon osd.1 config get mon_osd_full_ratio { \u0026quot;mon_osd_full_ratio\u0026quot;: \u0026quot;0.85\u0026quot;}\n4. Restart the 'ceph' service\n# service ceph restart\n5. Check the status\n# ceph daemon osd.1 config get mon_osd_full_ratio { \u0026quot;mon_osd_full_ratio\u0026quot;: \u0026quot;0.75\u0026quot;}\nNOTE: Please note that the changes introduced with 'ceph tell' as well as 'ceph daemon' is not persistent across process restarts.\n","link":"https://arvimal.github.io/posts/2015/06/dynamically-change-ceph-configuration/","section":"posts","tags":["ceph"],"title":"Another method to dynamically change a Ceph configuration"},{"body":"You may have seen the 'noout' flag set in the output of 'ceph -s'. What does this actually mean?\nThis is a global flag for the cluster, which means that if an OSD is out, the said OSD is not marked out of the cluster and data balancing shouldn't start to maintain the replica count. By default, the monitors mark the OSDs out of the acting set if it is not reachable for 300 seconds, ie.. 5 minutes.\nTo know the default value set in your cluster, use:\n# ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_report_timeout\nWhen an OSD is marked as out, another OSD takes its place and data replication starts to that OSD depending on the number of replica counts each pool has.\nIf this flag (noout) is set, the monitor will not mark the OSDs out from the acting set. The PGs will be reporting an inconsistent state, but the OSD will still be in the acting set.\nThis can be helpful when we want to remove an OSD from the server, but don't want the data objects to be replicated over to another OSD.\nTo set the 'noout' flag, use:\n# ceph osd set noout\nOnce everything you've planned has been done/finished, you can reset it back using:\n# ceph osd unset noout\n","link":"https://arvimal.github.io/posts/2015/05/osds-noout-status/","section":"posts","tags":["ceph","noout","osd"],"title":"'noout' flag in Ceph"},{"body":"","link":"https://arvimal.github.io/tags/admin-socket/","section":"tags","tags":null,"title":"admin-socket"},{"body":"","link":"https://arvimal.github.io/tags/ceph-tell/","section":"tags","tags":null,"title":"ceph-tell"},{"body":"","link":"https://arvimal.github.io/tags/config-show/","section":"tags","tags":null,"title":"config-show"},{"body":"It is possible to change a particular configuration setting in a Ceph cluster dynamically, and I think it is a very neat and useful feature.\nImagine the case where you want to change the replica count of a particular PG from 3 to 4. How would you change this without restarting the Ceph cluster itself? That is where the 'ceph tell' command comes in.\nAs we saw in the previous post, you can get the list of configuration settings using the administrator socket, from either a monitor or an OSD node.\nTo change a configuration use:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph tell mon.* injectargs '--{tunable value_to_be_set}'\n[/sourcecode]\nFor example, to change the timeout value after which an OSD is out and down, can be changed with:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph tell mon.* injectargs '--mon_osd_report_timeout 400'\n[/sourcecode]\nBy default, it is 300 seconds, ie.. 5 minute\n","link":"https://arvimal.github.io/posts/2015/05/change-ceph-configurations-dynamically/","section":"posts","tags":["ceph","ceph-tell"],"title":"How to dynamically change a configuration value in a Ceph cluster?"},{"body":"In many cases we would like to get the active configurations from a Ceph node, either a monitor or an OSD node. A neat feature, I must say, is to probe the administrative socket file to get a listing of all the active configurations, be it on the OSD node or the monitor node.\nThis comes handy when we have changed a setting and wants to confirm if it had indeed changed or not.\nThe admin socket file exists for both the monitors and the OSD nodes. The monitor node will have a single admin socket file, while the OSD nodes will have an admin socket for each of the OSDs present on the node.\nListing of the admin socket on a monitor node [sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ls /var/run/ceph/ -l total 4 srwxr-xr-x. 1 root root 0 May 13 05:13 ceph-mon.hp-m300-2.asok -rw-r--r--. 1 root root 7 May 13 05:13 mon.hp-m300-2.pid [/sourcecode]\nListing of the admin sockets on an OSD node [sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ls -l /var/run/ceph/ total 20 srwxr-xr-x. 1 root root 0 May 8 02:42 ceph-osd.0.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.2.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.3.asok srwxr-xr-x. 1 root root 0 May 8 02:42 ceph-osd.4.asok srwxr-xr-x. 1 root root 0 May 26 11:18 ceph-osd.5.asok -rw-r--r--. 1 root root 8 May 8 02:42 osd.0.pid -rw-r--r--. 1 root root 8 May 26 11:18 osd.2.pid -rw-r--r--. 1 root root 8 May 26 11:18 osd.3.pid -rw-r--r--. 1 root root 8 May 8 02:42 osd.4.pid -rw-r--r--. 1 root root 8 May 26 11:18 osd.5.pid [/sourcecode]\nFor example, consider that we have changed the 'mon_osd_full_ratio' value, and need to confirm that the cluster has picked up the change.\nWe can get a listing of the active configured settings and grep out the setting we are interested in.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon /var/run/ceph/ceph-mon.*.asok config show\n[/sourcecode]\nThe above command prints out a listing of all the active configurations and their current values. We can easily grep out 'mon_osd_full_ratio' from this list.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_full_ratio\n[/sourcecode]\nOn my test cluster, this printed out '0.75' which is the default setting. The cluster should print out 'near full' warnings once any OSD has reached 75% of its size.\nThis can be checked by probing the OSD admin socket as well.\nNOTE: In case you are probing a particular OSD, please make sure to use the OSD admin socket on the node in which the OSD is. In order to locate the OSD and the node it is on, use :\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph osd tree\n[/sourcecode]\nExample: We try probing the OSD admin socket on its node, for 'mon_osd_full_ratio' as we did on the monitor. It should return the same value.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon /var/run/ceph/ceph-osd.5.asok config show | grep mon_osd_full_ratio\n[/sourcecode]\nNOTE: Another command exists which should print the same configuration settings, but only for OSDs.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon osd.5 config show\n[/sourcecode]\nA drawback worth mentioning, this should be executed on the node on which the OSD is present. To find that the OSD to node mapping, use 'ceph osd tree'.\n","link":"https://arvimal.github.io/posts/2015/05/get-a-list-of-all-configurations-ceph-cluster/","section":"posts","tags":["admin-socket","ceph","config-show"],"title":"How to fetch the entire list of tunables along with the values for a Ceph cluster node?"},{"body":"","link":"https://arvimal.github.io/tags/noout/","section":"tags","tags":null,"title":"noout"},{"body":"","link":"https://arvimal.github.io/tags/crush/","section":"tags","tags":null,"title":"crush"},{"body":"","link":"https://arvimal.github.io/tags/crush-map/","section":"tags","tags":null,"title":"crush-map"},{"body":"","link":"https://arvimal.github.io/tags/fill-ratio/","section":"tags","tags":null,"title":"fill-ratio"},{"body":"There could be many scenarios where you'd need to change the percentage of space usage on a Ceph OSD. One such use case would be when your OSD space is about to hit the hard limit, and is constantly sending you warnings.\nFor some reason or other, you may need to extend the threshold limit for some time. In such a case, you don't need to change/add the configuration in ceph.conf and push it across. Rather you can do it while the cluster is online, via command mode.\nThe 'ceph tell' is a very useful command in the sense the administrator don't need to stop/start the OSDs, MONs etc.. after a configuration change. In our case, we are looking to set the 'mon_osd_full_ratio' to 98%. We can do it by using:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph tell mon.* injectargs \u0026quot;--mon_osd_full_ratio .98\u0026quot;\n[/sourcecode]\nIn an earlier post (https://goo.gl/xjXOoI) we had seen how to get all the configurable options from a monitor. If I understand correct, almost all the configuration values can be changed online by injecting the values using 'ceph tell'.\n","link":"https://arvimal.github.io/posts/2015/05/how-to-change-ceph-osd-filling-ratio/","section":"posts","tags":["ceph","fill-ratio","osd"],"title":"How to change the filling ratio for a Ceph OSD?"},{"body":"I'm still studying Ceph, and recently faced a scenario in which one of my Ceph nodes went down due to hardware failure. Even though my data was safe due to the replication factor, I was not able to remove the node from the cluster.\nI could remove the OSDs on the node, but I didn't find a way to remove the node being listed in 'ceph osd tree'. I ended up editing the CRUSH map by hand, to remove the host, and uploaded it back. This worked as expected. Following are the steps I did to achieve this.\na) This was the state just after the node went down:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph osd tree\n# id weight type name up/down reweight -10 .08997 root default -20 .01999 host hp-m300-5 00 .009995 osd.0 up 1 40 .009995 osd.4 up 1 -30 .009995 host hp-m300-9 10 .009995 osd.1 down 0 -40 .05998 host hp-m300-4 20 .04999 osd.2 up 1 30 .009995 osd.3 up 1\n[/sourcecode]\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph -w\ncluster 62a6a880-fb65-490c-bc98-d689b4d1a3cb health HEALTH_WARN 64 pgs degraded; 64 pgs stuck unclean; recovery 261/785 objects degraded (33.248%) monmap e1: 1 mons at {hp-m300-4=10.65.200.88:6789/0}, election epoch 1, quorum 0 hp-m300-4 osdmap e130: 5 osds: 4 up, 4 in pgmap v8465: 196 pgs, 4 pools, 1001 MB data, 262 objects 7672 MB used, 74192 MB / 81865 MB avail 261/785 objects degraded (33.248%) 64 active+degraded 132 active+clean [/sourcecode]\nI started with marking the OSDs on the node out, and removing them. Note that I don't need to stop the OSD (osd.1) since the node carrying osd.1 is down and not accessible.\nb) If not, you would've to stop the OSD using:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # sudo service osd stop osd.1 [/sourcecode]\nc) Mark the OSD out, this is not ideally needed in this case since the node is already out.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph osd out osd.1 [/sourcecode]\nd) Remove the OSD from the CRUSH map, so that it does not receive any data. You can also get the crushmap, de-compile it, remove the OSD, re-compile, and upload it back.\nRemove item id 1 with the name 'osd.1' from the CRUSH map.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph osd crush remove osd.1 [/sourcecode]\ne) Remove the OSD authentication key\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph auth del osd.1 [/sourcecode]\nf) At this stage, I had to remove the OSD host from the listing but was not able to find a way to do so. The 'ceph-deploy' didn't have any tools to do this, other than 'purge', and 'uninstall'. Since the node was not f) accessible, these won't work anyways. A 'ceph-deploy purge' failed with the following errors, which is expected since the node is not accessible.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph-deploy purge hp-m300-9\n[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.22-rc1): /usr/bin/ceph-deploy purge hp-m300-9 [ceph_deploy.install][INFO ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm [ceph_deploy.install][INFO ] like: librbd1 and librados2 [ceph_deploy.install][DEBUG ] Purging from cluster ceph hosts hp-m300-9 [ceph_deploy.install][DEBUG ] Detecting platform for host hp-m300-9 ... ssh: connect to host hp-m300-9 port 22: No route to host [ceph_deploy][ERROR ] RuntimeError: connecting to host: hp-m300-9 resulted in errors: HostNotFound hp-m300-9\n[/sourcecode]\nI ended up fetching the CRUSH map, removing the OSD host from it, and uploading it back.\ng) Get the CRUSH map\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph osd getcrushmap -o /tmp/crushmap [/sourcecode]\nh) De-compile the CRUSH map\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # crushtool -d /tmp/crushmap -o crush_map [/sourcecode]\ni) I had to remove the entries pertaining to the host-to-be-removed from the following sections:\na) devices b) types c) And from the 'root' default section as well.\nj) Once I had the entries removed, I went ahead compiling the map, and inserted it back.\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # crushtool -c crush_map -o /tmp/crushmap # ceph osd setcrushmap -i /tmp/crushmap [/sourcecode]\nk) A 'ceph osd tree' looks much cleaner now :)\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;] # ceph osd tree\n# id weight type name up/down reweight -1 0.07999 root default -2 0.01999 host hp-m300-5 0 0.009995 osd.0 down 0 4 0.009995 osd.4 down 0 -4 0.06 host hp-m300-4 2 0.04999 osd.2 up 1 3 0.009995 osd.3 up 1 [/sourcecode]\nThere may be a more direct method to remove the OSD host from the listing. I'm not aware of anything relevant, based on my limited knowledge. Perhaps I'll come across something as I progress with Ceph. Comments welcome.\n","link":"https://arvimal.github.io/posts/2015/05/remove-a-host-from-ceph-cluster/","section":"posts","tags":["ceph","crush","crush-map","osd"],"title":"How to remove a host from a Ceph cluster?"},{"body":"","link":"https://arvimal.github.io/tags/admin_socket/","section":"tags","tags":null,"title":"admin_socket"},{"body":"","link":"https://arvimal.github.io/tags/ceph-admin-socket/","section":"tags","tags":null,"title":"ceph-admin-socket"},{"body":"","link":"https://arvimal.github.io/tags/ceph-conf/","section":"tags","tags":null,"title":"ceph-conf"},{"body":"It can be really helpful to have a single command to list all the configuration settings in a monitor node, in a Ceph cluster.\nThis is possible by interacting directly with the monitor's unix socket file. This can be found under /var/run/ceph/. By default, the admin socket for the monitor will be in the path /var/run/ceph/ceph-mon..asok.\nThe default location can vary in case you have defined it to be a different one, at the time of the installation. To know the actual socket path, use the following command:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph-conf --name mon.$(hostname -s) --show-config-value admin_socket\n[/sourcecode]\nThis should print the location of the admin socket. In most cases, it should be something like /var/run/ceph/ceph-mon.$(hostname -s).asok\nOnce you have the monitor admin socket, use that location to show the various configuration settings with:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon /var/run/ceph/ceph-mon.*.asok config show\n[/sourcecode]\nThe output would be long, and won't fit in a single screen. You can either pipe it to 'less' or grep for a specific value in case you know what you are looking for.\nFor example, if I need to look at the ratio at which the OSD would be considered full, I'll be using:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# ceph daemon /var/run/ceph/ceph-mon.*.asok config show | grep mon_osd_full_ratio\n[/sourcecode]\n","link":"https://arvimal.github.io/posts/2015/05/list-ceph-cluster-configurations/","section":"posts","tags":["admin_socket","ceph","ceph-admin-socket","ceph-conf","monitors","mons"],"title":"How to list all the configuration settings in a Ceph cluster monitor?"},{"body":"","link":"https://arvimal.github.io/tags/mons/","section":"tags","tags":null,"title":"mons"},{"body":"","link":"https://arvimal.github.io/tags/cachefilesd/","section":"tags","tags":null,"title":"cachefilesd"},{"body":"","link":"https://arvimal.github.io/tags/cachefs/","section":"tags","tags":null,"title":"cachefs"},{"body":"","link":"https://arvimal.github.io/tags/fscache/","section":"tags","tags":null,"title":"fscache"},{"body":"The 'cachefilesd' kernel module will create two directories at the location specified in /etc/cachefilesd.conf. By default it's /var/cache/fscache/.\n[root@montypython ~]# lsmod |grep -i cache cachefiles 40871 1 fscache 62354 3 nfs,cachefiles,nfsv4\nThose are /var/cache/fscache/cache and /var/cache/fscache/graveyard.\nThe cache structure is maintained inside '/var/cache/fscache/cache/', while anything that is retired or culled is moved to 'graveyard'. The 'cachefilesd' daemon monitors 'graveyard' using 'dnotify' and will delete anything that is in there.\nWe'll try an example. Consider an NFS share mounted with fscache support. The share contains the following files, with some random text.\n# ls /vol1 files1.txt files2.txt files3.txt files4.txt\na) Configure 'cachefiles' by editing '/etc/cachefilesd.conf', and start the 'cachefilesd' daemon.\n# systemctl start cachefilesd\nb) Mount the NFS share on the client with the 'fsc' mount option, to enable 'fscache' support.\n# sudo mount localhost:/vol1 /vol1-backup/ -o fsc\nd) Access the data from the mount point, and fscache will create the backed caching index at the location specified in /etc/cachefilesd.conf. By default, its /var/cache/fscache/\ne) Once the files are accessed on the client side, fscache builds an index as following:\nNOTE: The index structure is dependent on the netfs (NFS in our case). The netfs driver can structure the cache index as it seems fit.\nExplanation of the caching structure:\n# tree /var/cache/fscache/ /var/cache/fscache/cache/ └── @4a └── I03nfs ├── @22 │ └── Jo00000008400000000000000000000000400 │ └── @59 │ └── J110000000000000000w080000000000000000000000 │ ├── @53 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @5e │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @61 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @62 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @70 │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ ├── @7c │ │ └── EE0g00sgwB-90600000000ww000000000000000 │ └── @e8 │ └── EE0g00sgwB-90600000000ww0000000000000000 └── @42 └── Jc000000000000EggDj00 └── @0a\na) The 'cache' directory under /var/cache/fscache/ is a special index and can be seen as the root of the entire cache index structure.\nb) Data objects (actual cached files) are represented as files if they have no children, or folders if they have. If represented as a directory, data objects will have a file inside named 'data' which holds the data.\nc) The 'cachefiles' kernel module represents :\ni) 'index' objects as 'directories', starting with either 'I' or 'J'.\nii) Data objects are represented with filenames, beginning with 'D' or 'E'.\niii) Special objects are similar to data objects, and start with 'S' or 'T'.\nIn general, any object would be represented as a folder, if that object has children.\ng) In the directory hierarchy, immediately between the parent object and its child object, are directories named with *hash values* of the immediate child object keys, starting with an '@'.\nThe child objects are placed inside this directory.These child objects would be folders, if it has child objects, or files if its the cached data itself. This can go on till the end of the path and reaches the file where the cached data is.\nRepresentation of the object indexes (For NFS, in this case)\nINDEX INDEX INDEX DATA FILES ========= ========== ================================= ================ cache/@4a/I03nfs/@30/Ji000000000000000--fHg8hi8400 cache/@4a/I03nfs/@30/Ji000000000000000--fHg8hi8400/@75/Es0g000w...DB1ry cache/@4a/I03nfs/@30/Ji000000000000000--fHg8hi8400/@75/Es0g000w...N22ry cache/@4a/I03nfs/@30/Ji000000000000000--fHg8hi8400/@75/Es0g000w...FP1ry\n","link":"https://arvimal.github.io/posts/2014/11/structure-of-cached-content-in-fscache/","section":"posts","tags":["cachefilesd","cachefs","fscache"],"title":"FSCache and the on-disk structure of the cached data"},{"body":"","link":"https://arvimal.github.io/tags/fs-cache/","section":"tags","tags":null,"title":"fs-cache"},{"body":"FS-Cache and CacheFS. Are there any differences between these two? Initially, I thought both were same. But no, it's not.\nCacheFS is the backend implementation which caches the data onto the disk and mainpulates it, while FS-Cache is an interface which talks to CacheFS.\nSo why do we need two levels here?\nFS-Cache was introduced as an API or front-end for CacheFS, which can be used by any file system driver. The file system driver talks with the FS-Cache API which inturn talks with CacheFS in the back-end. Hence, FS-Cache acts as a common interface for the file system drivers without the need to understand the backend CacheFS complexities, and how its implemented.\nThe only drawback is the additional code that needs to go into each file system driver which needs to use FS-Cache. ie.. Every file system driver that needs to talk with FS-Cache, has to be patched with the support to do so. Moreover, the cache structure differs slightly between file systems using it, and thus lacks a standard. This unfortunately, prevents FS-Cache from being used by every network filesystem out there.\nThe data flow would be as:\nVFS \u0026lt;-\u0026gt; File system driver (NFS/CIFS etc..) \u0026lt;-\u0026gt; FS-Cache \u0026lt;-\u0026gt; CacheFS \u0026lt;-\u0026gt; Cached data\nCacheFS need not cache every file in its entirety, it can also cache files partially. This partial caching mechanism is possible since FS-Cache caches 'pages' rather than an entire file. Pages are smaller fixed-size segments of data, and these are cached depending on how much the files are read initially.\nFS-Cache does not require an open file to be loaded in the cache, prior being accessed. This is a nice feature as far as I understand, and the reasons are:\na) Not every open file in the remote file system can be loaded into cache, due to size limits. In such a case, only certain parts (pages) may be loaded. And the rest of the file should be accessed normally over the network.\nb) The cache won't necessarily be large enough to hold all the open files on the remote system.\nc) Even if the cache is not populated properly, the file should be accessible. ie.. the cache should be able to be bypassed totally.\nThis hopefully clears the differences between FS-Cache and CacheFS.\n","link":"https://arvimal.github.io/posts/2014/09/fscache-and-cachefs-differences/","section":"posts","tags":["cachefs","fs-cache"],"title":"FS-Cache and CacheFS, what are the differences?"},{"body":"I would be working on enabling FS-Cache support in the FUSE kernel module, as part of my under graduate project.\nNiels De Vos, from Red Hat Engineering, would act as my mentor and guide throughout this project. He would also be presenting this idea in the 'Linux Plumbers Conference' being held in Germany, October 2014.\nMore details on the the talk can be seen at http://www.linuxplumbersconf.org/2014/ocw/sessions/2247\nThis feature has got quite a few requests from the FOSS world, and I'm glad I could work on this. For now, I'm trying to get a hold on FS-Cache, how it works with other file systems, and trying to build FUSE with some customizations. Ultimately, it would be the FUSE module were the code additions would go, not FS-Cache.\nI'll try to keep this blog updated, so that I have a journal to refer later.\n","link":"https://arvimal.github.io/posts/2014/09/fscache-and-fuse/","section":"posts","tags":["cachefilesd","fs-cache","fuse"],"title":"FS-Cache and FUSE"},{"body":"","link":"https://arvimal.github.io/tags/fuse/","section":"tags","tags":null,"title":"fuse"},{"body":"I've been trying to create a minimal docker image for RHEL versions, for one of my projects. The following were the steps I followed:\na) Installed a RHEL6.5 server with 'Minimal Installation'.\nb) Registered it to the local satellite.\nc) Created a tar-ball of the filesystem with the command below:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# tar --numeric-owner --exclude=/proc --exclude=/sys --exclude=/mnt --exclude=/var/cache\n--exclude=/usr/share/doc --exclude=/tmp --exclude=/var/log -zcvf /mnt/rhel6.5-base.tar.gz /\n[/sourcecode]\nd) Load the tar.gz image using 'docker load' (as per the man page of 'docker load')\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# docker load -i rhel6.5-base.tar.gz\n[/sourcecode]\nThis is where it erred with the message:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n2014/08/16 20:37:42 Error: open /tmp/docker-import-123456789/repo/bin/json: no such file or directory\n[/sourcecode]\nAfter a bit of searching and testing, I found that 'docker load -i' doesn't work as expected. The workaround is to cat and pipe the tar.gz file, as shown below:\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# cat rhel6.5-base.tar.gz | docker import - rhel6/6.5\n[/sourcecode]\nThis ends up with the image showing up in 'docker images'\n[sourcecode language=\u0026quot;bash\u0026quot; gutter=\u0026quot;false\u0026quot;]\n# docker images\nREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE rhel6/6.1 latest 32b4b345454a About a minute ago 1.251 GB\n[/sourcecode]\nUpdate: 'docker load -i ' would only work if the image is created as a layered docker image. If the is a tar ball created from a root filesystem, you would need to use 'cat | docker import '\n","link":"https://arvimal.github.io/posts/2014/08/error-open-tmpdocker/","section":"posts","tags":null,"title":"\"Error: open /tmp/docker-import-123456789/repo/bin/json: no such file or directory\""},{"body":"","link":"https://arvimal.github.io/tags/anaconda/","section":"tags","tags":null,"title":"anaconda"},{"body":"","link":"https://arvimal.github.io/tags/installation/","section":"tags","tags":null,"title":"installation"},{"body":"","link":"https://arvimal.github.io/tags/lsusb/","section":"tags","tags":null,"title":"lsusb"},{"body":"**T**he binary '/sbin/lsusb' in a chroot-ed environment have problems running properly. I have not checked this in a manually created chroot environment or using tools like 'mock'.\nThe scenario is as following :\nWe were trying to check the output of 'lsusb' in the %post section of a kickstart installation. I had specified 'noreboot' in the kickstart file so the machine will wait for the user to manually reboot the machine. This helps to check the logs and the situation of the machine just after the installation finishes.\nAfter the installation and prior to the reboot, i checked in the second available terminal (Alt + F2) created by anaconda and was astonished to see that the command 'lsusb' does not give us the required output but an error that '/usr/share/hwdata/usb.ids' is not accessible or found.\nBy default, i think only the 'installation' ie.. the %post section starts in a 'chroot' mode and the terminal available is not chroot-ed. So we will have to use '/mnt/sysimage/sbin/lsusb'. This didn't work as expected since the 'lsusb' binary needs to check the file '/usr/share/hwdata/usb.ids' and won't be able to find it.\nSo I did a chroot from the second terminal and did an /sbin/lsusb, since /sbin in not in the 'PATH' by default. That too, didn't work out. But this time it didn't even complain anything. Just nothing at all, no output. Last time, at-least it complained it could not find something. So how do we go forward now ??? Here comes 'strace' to the rescue !!!\nstrace is of-course a really nice tool to know what system calls are made and lots of internal stuff a binary will do while being executed. But 'strace' is not installed by default on a RHEL5 machine, which is the case here. As most of you would know, anaconda creates a virtual file system which consists of most of the folders found under a linux main /. The location where the OS is installed is mounted under /mnt/sysimage.\nSince we already have an ISO from where we have booted the machine from (DVD/CD), we are free to mount it on the filesystem, which is what we did. :\n1 # mkdir /mnt/source 2 # mount -t iso9660 /dev/hdc /mnt/source 3 # cd /mnt/source/Server/ In case you want to know how the DVD/CD drive is detected, all you need to do is execute 'dmesg' in the available terminal. ie.. after pressing 'Alt + Ctrl + F2'.\nSo we went forward and mounted the DVD to /mnt/source and changed the directory to /mnt/source/Server where all the rpm packages reside. Installed the package 'strace' using an 'rpm -ivh'. Please note that we need to use '--root /mnt/sysimage' as an option since we are installing the package to our newly installed file system which is at /mnt/sysimage. If this is not used, the installer will try to install the package to the virtual environment created in the memory.\n1# cd /mnt/source/Server 2# rpm -ivh strace -fxvttT rpm --root /mnt/sysimage 3# cd 4# chroot /mnt/sysimage This will make /mnt/sysimage as the working root, ie.. where our installation was done. OK.. now for the 'strace' stuff.\n1# strace -fxvto strace.log -s 1024 /sbin/lsusb The strace output will be saved to 'strace.log' which we can open up in a text editor of our choice. Opening it in 'vi', shows a lot of stuff such as the command run, the default language, location of libraries loaded, the environment variables etc.. In this case we would only need to be interested in the last parts, ie.. to know where the binary failed :\n115:16:17 open(\u0026#34;/dev/bus/usb\u0026#34;, O\\_RDONLY|O\\_NONBLOCK|O\\_DIRECTORY) = -1 ENOENT (No such file or directory) = 03067 215:16:17 open(\u0026#34;/proc/bus/usb\u0026#34;, O\\_RDONLY|O\\_NONBLOCK|O\\_DIRECTORY) = 33067 315:16:17 fstat(3, {st\\_dev=makedev(0, 3), st\\_ino=4026532146, st\\_mode=S\\_IFDIR|0555, st\\_nlink=2, st\\_uid=0, st\\_gid=0, st\\_blksize=4096, st\\_blocks=0, st\\_size=0, st\\_atime=2009/09/25-15:16:17, st\\_mtime=2009/09/25-15:16:17, st\\_ctime=2009/09/25-15:16:17}) = 03067 15:16:17 fcntl(3, F\\_SETFD, FD\\_CLOEXEC) = 03067 15:16:17 getdents(3, {{d\\_ino=4026532146, d\\_off=1, d\\_reclen=24, d\\_name=\u0026#34;.\u0026#34;} {d\\_ino=4026531879, d\\_off=2, d\\_reclen=24, d\\_name=\u0026#34;..\u0026#34;}}, 4096) = 483067 415:16:17 getdents(3, {}, 4096) = 03067 15:16:17 close(3) = 03067 15:16:17 exit\\_group(1) = ? The above trace output shows how the 'lsusb' binary proceeded at its last time and where it failed. We can see that it went to open '/dev/bus/usb', only to find that the said location does not exist. We can understand that it is a directory from the call\n1open(\u0026#34;/dev/bus/usb\u0026#34;, O\\_RDONLY|O\\_NONBLOCK|O\\_DIRECTORY) Ok,, fine.. so what does it do next ?\nAs the next step, it tries to open '/proc/bus/usb' and it is present, which we know since there are no 'No such file or directory' errors. Going further, the binary goes on to do a 'stat' on '/proc/bus/usb'. After doing an 'fstat', it goes to check the file descriptor using 'fcntl' and further goes to list the directory contents using 'getdents'.\nThis is where we find the interesting output :\n1getdents(3, {{d\\_ino=4026532146, d\\_off=1, d\\_reclen=24, d\\_name=\u0026#34;.\u0026#34;} {d\\_ino=4026531879, d\\_off=2, d\\_reclen=24, d\\_name=\u0026#34;..\u0026#34;}}, 4096) = 48 As you can see in the above trace, it returns '.' and '..', which means there are nothing in /proc/bus/usb. So what we do understand is 'lsusb' refers /dev/bus/usb and /proc/bus/usb for its outputs.. If it was not able to find anything, strace would have given us an error which obviously would have made life much easier.\nAnd that's how '/sbin/lsusb' failed silently.. Isn't strace a nice tool ??\nOkay, those who want to know why is this so... 'lsusb' needs either /mnt/sysimage/proc/bus/usb or /mnt/sysimage/dev/bus/usb display contents to work properly. Anaconda is not mounting /mnt/sysimage/proc/bus/usb with the 'usbfs' file system in the limited installation environment and hence 'lsusb' fails...\nAnd we have a fix for that which goes into yuminstall.py in the anaconda source :\n1try: 2 isys.mount(\u0026#34;/proc/bus/usb\u0026#34;, anaconda.rootPath + \u0026#34;/proc/bus/usb\u0026#34;, \u0026#34;usbfs\u0026#34;) 3except Exception, e: 4 log.error(\u0026#34;error mounting usbfs: %s\u0026#34; %(e,)) This piece of python code, tries mounting /proc/bus/usb on /mnt/sysimage/proc/bus/usb as 'usbfs. If its not possible, the code excepts an Exception error and reports \u0026quot;error mounting 'usbfs'.\n","link":"https://arvimal.github.io/posts/2010/12/lsusb-and-chroot-in-anaconda/","section":"posts","tags":["anaconda","installation","lsusb","rhel","strace","usb"],"title":"lsusb and chroot in anaconda.. Is usbfs mounted in anaconda %post installation ?"},{"body":"","link":"https://arvimal.github.io/tags/rhel/","section":"tags","tags":null,"title":"rhel"},{"body":"","link":"https://arvimal.github.io/tags/strace/","section":"tags","tags":null,"title":"strace"},{"body":"","link":"https://arvimal.github.io/tags/usb/","section":"tags","tags":null,"title":"usb"},{"body":"What is device-mapper ?\nDevice mapper is a modular driver for the linux kernel 2.6. It can be said as a framework which helps to create or map logical sectors of a pseudo block device to an underlying physical block device. So what device-mapper do is keep a table of mappings which equate the logical block devices to the physical block devices.\nApplications such as LVM2, EVMS, software raid aka dmraid, multipathing, block encryption mechanisms such as cryptsetup etc... use device-mapper to work. All these applications excluding EVMS use the libdevmapper library to communicate with device-mapper.\nThe applications communicate with device-mapper's API to create the mapping. Due to this feature, device-mapper does not need to know what LVM or dmraid is, how it works, what LVM metadata is, etc... It is upto the application to create the pseudo devices pointing to the physical volumes using one of device-mapper's targets and then update the mapper table.\nThe device-mapper mapping table :\nThe mapping table used by device-mapper doesn't take too much space and is a list created using a 'btree'. A btree or a 'Binary Search Tree' is a data-structure from which data can be added, removed or queried.\nIn order to know more on what a btree is and the concept behind it, read :\nhttp://en.wikipedia.org/wiki/Binary_search_tree\nhttp://en.wikipedia.org/wiki/B-tree\nTypes of device-mapper targets :\nApplications which use device-mapper actually use one or more of its target methods to achieve their purpose. Targets can be said as a method or type of mapping used by device-mapper. The general mapping targets are :\na) Linear - Used by linear logical volumes, ie.. the default data layout method used by LVM2.\nb) Striped - Used by striped logical volumes as well as software RAID0.\nc) Mirror - Used by software RAID1 and LVM mirroring.\nd) Crypt - Used by disk encryption utilties.\ne) Snapshot - Used to take online snapshots of block devices, an example is LVM snapshot.\nf) Multipath - Used by device-mapper-multipath.\ng) RAID45 - Software raid using device-mapper, ie.. dmraid\nh) Error - Sectors of the pseudo device mapped with this target causes the I/O to fail.\nThere are a few more mappings such as 'flaky' which is not used much.\nI'll write on how device-mapper works in LVM, in the next post...\n","link":"https://arvimal.github.io/posts/2010/12/device-mapper-and-applications/","section":"posts","tags":null,"title":"Device Mapper and applications"},{"body":"In case anyone out there gets an error message like \u0026quot;Aborting. Failed to activate new LV to wipe the start of it.\u0026quot; while doing an 'lvcreate', check (/etc/lvm/lvm.conf) once more.\nMost probably, a 'volume_list' would have been defined in there, which in turns want you to specify the 'volume_list' tag specified along with the lvcreate command.\nExcerpt from /etc/lvm/lvm.conf:\n# If volume_list is defined, each LV is only activated if there is a # match against the list. # vgname and vgname/lvname are matched exactly. # @tag matches any tag set in the LV or VG. # @* matches if any tag defined on the host is also set in the LV or VG # # volume_list = [ vg1, vg2/lvol1, @tag1, @* ] volume_list = [ VG01, @foo.com ]\nIn this case, you will have to use the 'lvcreate' command as follows, which will create the logical volume properly.\n# lvcreate --addtag @foo.com ... following-options\n","link":"https://arvimal.github.io/posts/2009/11/lvcreate-fails-with-error/","section":"posts","tags":null,"title":"lvcreate fails with the error \"Aborting. Failed to activate new LV to wipe the start of it.\". Why ??"},{"body":"From the output of the command 'lspci -n' (The number after the colon, here '1679' from the below snip)\n0a:04.0 0200: 14e4:1679 (rev a3) Subsystem: 103c:703c Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- Status: Cap+ 66MHz+ UDF- FastB2B+ ParErr- DEVSEL=medium Latency: 64 (16000ns min), Cache Line Size: 64 bytes Interrupt: pin A routed to IRQ 138 Region 0: Memory at fdef0000 (64-bit, non-prefetchable) [size=64K] Region 2: Memory at fdee0000 (64-bit, non-prefetchable) [size=64K]\nIMPORTANT: -------------------\nIn the above line \u0026quot;14e4:1679\u0026quot;, '14e4' is the UID of the manufacturer and '1679' is the card model or hardware ID.\nThe actual way to proceed is to open the pci.ids file ('/usr/share/hwdata/pci.ids' and '/lib/modules/`uname -r`/modules.pcimap') and check for the manufacturer UID, like '14e4' which is the 'Broadcom Corporation'. The file /lib/modules/`uname -r`/modules.pcimap would be more reliable since it is from the modules of the loaded kernel.\nUnder that, check the card model, like '1679' which is 'NetXtreme BCM5715S Gigabit Ethernet'.\nUnder that you can also have subdivisions, so in order to pin-point a particular card you will have to use the 'Subsystem' value from 'lspci'.\nIn this example, 'Subsystem' is 103c:703c, which turns out to be 'NC326i PCIe Dual Port Gigabit Server Adapter'\n","link":"https://arvimal.github.io/posts/2008/07/map-pci-devices-in-linux/","section":"posts","tags":null,"title":"How to map PCI devices in Linux ?"},{"body":"","link":"https://arvimal.github.io/tags/boot-loader-checker/","section":"tags","tags":null,"title":"boot-loader-checker"},{"body":"A bash code snippet that helps to check if the installed bootloader is Grub or LILO.\n[code language=\u0026quot;bash\u0026quot;] #!/bin/bash\nA=`mount | awk '{print $1}' | grep -n /dev/ | grep \u0026quot;1:\u0026quot; | cut -f2 -d \u0026quot;:\u0026quot; | cut -c 1-8` B=`mount | awk '{print $1}' | grep -n /dev/ | grep \u0026quot;1:\u0026quot; | cut -f2 -d \u0026quot;:\u0026quot;`\necho ; echo -e \u0026quot; / mounted on $B \\n\u0026quot;; dd if=$A bs=512 count=1 2\u0026gt;\u0026amp;1 | grep GRUB \u0026gt; /dev/null; if [ $? = 0 ] ; then echo -e \u0026quot;The installed bootloader is GRUB.\\n\u0026quot; ; fi\ndd if=$A bs=512 count=1 2\u0026gt;\u0026amp;1 | grep LILO \u0026gt; /dev/null; if [ $? = 0 ] ; then echo -e \u0026quot;The installed bootloader is LILO.\\n\u0026quot; ; fi [/code]\n","link":"https://arvimal.github.io/posts/2008/02/bootloader-checker/","section":"posts","tags":["boot-loader-checker","grub","lilo"],"title":"Bootloader checker"},{"body":"Adding Swap Space:\nSometimes it is necessary to add more swap space after installation. For example, you may upgrade the amount of RAM in your system from 64 MB to 128 MB, but there is only 128 MB of swap space. It might be advantageous to increase the amount of swap space to 256 MB if you perform memory-intense operations or run applications that require a large amount of memory.\nYou have two options: add a swap partition or add a swap file. It is recommended that you add a swap partition, but sometimes that is not easy if you do not have any free space available.\nTo add a swap partition (assuming /dev/hdb2 is the swap partition you want to add):\n1) The hard drive can not be in use (partitions can not be mounted, and swap space can not be enabled). The easiest way to achieve this it to boot your system in rescue mode. Refer to Chapter 8 for instructions on booting into rescue mode. When prompted to mount the filesystem, select Skip. Alternately, if the drive does not contain any partitions in use, you can unmount them and turn off all the swap space on the hard drive with the swapoff command.\n2) Create the swap partition using parted or fdisk. Using parted is easier than fdisk; thus, only parted will be explained. To create a swap partition with 'parted:'. At a shell prompt as root, type the command parted /dev/hdb, where /dev/hdb is the device name for the hard drive with free space. At the (parted) prompt, type print to view the existing partitions and the amount of free space. The start and end values are in megabytes. Determine how much free space is on the hard drive and how much you want to allocate for a new swap partition. At the (parted) prompt, type mkpartfs part-type linux-swap start end, where part-type is one of primary, extended, or logical, start is the starting point of the partition, and end is the end point of the partition.\nWarning Warning _________________________\nChanges take place immediately; be careful when you type.\nExit parted by typing quit.\nNow that you have the swap partition, use the command mkswap to setup the swap partition. At a shell prompt as root, type the following: # mkswap /dev/hdb2\nTo enable the swap partition immediately, type the following command: # swapon /dev/hdb2\nTo enable it at boot time, edit /etc/fstab to include: /dev/hdb2 swap swap defaults 0 0\nThe next time the system boots, it will enable the new swap partition.\nAfter adding the new swap partition and enabling it, make sure it is enabled by viewing the output of the command cat /proc/swaps or free. To add a swap file: -------------------------- 1. Determine the size of the new swap file and multiple by 1024 to determine the block size. For example, the block size of a 64 MB swap file is 65536.\n2. At a shell prompt as root, type the following command with count being equal to the desired block size:\n# dd if=/dev/zero of=/swapfile bs=1024 count=65536\n3. Setup the swap file with the command:\n# mkswap /swapfile\n4. To enable the swap file immediately but not automatically at boot time:\n# swapon /swapfile\n5. To enable it at boot time, edit /etc/fstab to include:\n/swapfile swap swap defaults 0 0\nThe next time the system boots, it will enable the new swap file.\n6. After adding the new swap file and enabling it, make sure it is enabled by viewing the output of the command cat /proc/swaps or free.\n","link":"https://arvimal.github.io/posts/2008/02/creating-swap-space-in-linux/","section":"posts","tags":["swap-space"],"title":"Creating a SWAP space in Linux"},{"body":"","link":"https://arvimal.github.io/tags/file-counter/","section":"tags","tags":null,"title":"file-counter"},{"body":"","link":"https://arvimal.github.io/tags/grub/","section":"tags","tags":null,"title":"grub"},{"body":"","link":"https://arvimal.github.io/tags/lilo/","section":"tags","tags":null,"title":"lilo"},{"body":"Most of the scripts presented in this journal have been created while learning bash and having nothing much to do...\nI think its usual to get crazy ideas and work trying to implement them, especially while learning any type of coding. This 'File Counter' script came as such a crazy idea. It was working at the time of its creation, but have not checked it recently.. should work..\nThis script counts the entire number of files irrespective the folders under the main directory you specify for this script to work on. ie.. It recursively counts the files under a directory tree.\n[code language=\u0026quot;bash\u0026quot;] #!/bin/bash\n# Counts the number of files recursively inside a directory # # echo ; clear echo -e \u0026quot;Please enter the directory location where you want the files to be counted...\\n\u0026quot; ; echo\nread dir ; echo ; if [ ! -d $dir ] ; then echo -e \u0026quot;The location you specified doesn't exist.\\n\u0026quot; ; exit 0; else cd $dir ; echo -e \u0026quot;Please wait for the files to be counted.....\\n\u0026quot; ; echo ; fi\nX=`ls -l | wc -l` Y=`ls -l | grep ^d | awk '{print $9}'` B=`ls -l $Y | awk '{print $9}' | grep . | wc -l ` A=`expr $X + $B`\necho -e \u0026quot;There are a total of $A files inside the directory $dir...\\n\u0026quot;\nC=`ls -Rl | grep -v ./ | grep -v total | grep . | awk '{print $8}'`\necho -e \u0026quot;Do you want to scan the directory for the file types?\\n\u0026quot; echo -e \u0026quot;Y/N\\n\u0026quot; ; read choice; if [ $choice = Y ] ; then cd $dir ; file $C | awk '{print $1,\u0026quot;=======\u0026gt;\u0026gt;\u0026quot;, $2}' \u0026gt; $HOME/Filetype.txt;echo -e \u0026quot;Output saved in file Filetype.txt.\\n\u0026quot; elif [ $choice = N ] ; then echo -e \u0026quot;Thankyou $USER, Take care....\\n\u0026quot; else echo ; echo -e \u0026quot;Invalid choice buddy...\\n\u0026quot; ; echo -e \u0026quot;Exiting.....Bye..\\n\u0026quot; ; fi [/code]\n","link":"https://arvimal.github.io/posts/2008/02/file-counter/","section":"posts","tags":["file-counter"],"title":"Recursive file counter in bash"},{"body":"","link":"https://arvimal.github.io/tags/swap-space/","section":"tags","tags":null,"title":"swap-space"},{"body":"This is an extension or a rebuild of the previous chkrootkit install script, just used functions so its somewhat simplified.... ( Or is it ..? :) )\n[code language=\u0026quot;bash\u0026quot;]\n#!/bin/bash\nDOWNLOAD_LOCATION='/root/Downloads' CHKROOTKIT_WGET='ftp://ftp.pangeia.com.br/pub/seg/pac/chkrootkit.tar.gz' RESULT_FILE='/root/Server-Test.txt'\nclear;echo chkrootkit-install () {\nwhile true; do echo -e \u0026quot;@@@@@@@@@@@@@@@@@@ CHK-ROOTKIT INSTALL/CHECK SCRIPT @@@@@@@@@@@@@@@@@@@@\\n\u0026quot; echo -e \u0026quot;Do you want to download and compile CHK-ROOTKIT [yes/no] ? : \\c\u0026quot; | tee -a $RESULT_FILE; read answer; echo $answer \u0026gt;\u0026gt; $RESULT_FILE;\ncase $answer in yes|YES) echo if [ ! -e $DOWNLOAD_LOCATION ]; then echo -e \u0026quot;$DOWNLOAD_LOCATION does not exist, creating.......\\n\u0026quot;;sleep 1s mkdir -p $DOWNLOAD_LOCATION; fi rm -rf $DOWNLOAD_LOCATION/chkrootkit* \u0026gt; /dev/null; echo -e \u0026quot;Downloading CHK-ROOTKIT....\\n\u0026quot; | tee -a $RESULT_FILE;sleep 1s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; wget --progress=dot $CHKROOTKIT_WGET; if [ $? -eq 0 ] ; then echo -e \u0026quot;Download finished..\\n\u0026quot;; else echo -e \u0026quot;Sorry...Download Failed..!!!\\n\u0026quot;;exit;fi;echo echo -e \u0026quot;Unpacking and compiling CHK-ROOTKIT..........\\n\u0026quot;;sleep 2s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; tar -xvf chkrootkit*; mv $DOWNLOAD_LOCATION/chkrootkit*gz $DOWNLOAD_LOCATION/1-chkrootkit.tar.gz;sleep 2s cd $DOWNLOAD_LOCATION/chkrootki* \u0026amp;\u0026amp; make sense \u0026gt; /dev/null; if [ $? -eq 0 ] ; then echo -e \u0026quot;CHK-ROOTKIT compiled successfully..\\n\u0026quot;| tee -a $RESULT_FILE; break else echo -e \u0026quot;CHK-ROOTKIT compilation failed, Quiting....\\n\u0026quot; | tee -a $RESULT_FILE; exit fi ;; no|NO) echo echo -e \u0026quot;Ok..As you wish....Aborting.\\n\u0026quot;|tee -a $RESULT_FILE; exit ;; *) echo echo -e \u0026quot;Please enter either 'yes' OR 'no'..: \\c\u0026quot; ;; esac done }\nchkrootkit-run () { if [ -d $DOWNLOAD_LOCATION/chkrootki* ]; then while true; do echo -e \u0026quot;Do you want to run CHK-ROOTKIT now [yes/no] ? : \\c\u0026quot; | tee -a $RESULT_FILE; read reply echo $reply \u0026gt;\u0026gt; $RESULT_FILE;\ncase $reply in yes|YES) echo echo -e \u0026quot;Starting CHK-ROOTKIT....\\n\u0026quot; | tee -a $RESULT_FILE;sleep 2s;echo echo -e \u0026quot;----------------CHK-ROOTKIT SCAN RESULT-----------------\\n\u0026quot; $DOWNLOAD_LOCATION/chkrootk*/chkrootkit | tee -a $RESULT_FILE;sleep 1s echo;echo -e \u0026quot;CHK-ROOTKIT check finished......\\n\u0026quot;;echo exit ;; no|NO) echo echo -e \u0026quot;DON'T FORGET TO RUN CHK-ROOTKIT PERIODICALLY.\\n\u0026quot; exit ;; *) echo echo -e \u0026quot;Please enter either 'yes' OR 'no'..: \\c\u0026quot; ;; esac done\nelse echo -e \u0026quot;Chkrootkit not found in $DOWNLOAD_LOCATION, exiting..\\n\u0026quot; fi\n}\nchkrootkit-install \u0026amp;\u0026amp; chkrootkit-run; echo -e \u0026quot;The result is saved in $RESULT_FILE for reference.\\n\u0026quot; [/code]\n","link":"https://arvimal.github.io/posts/2008/02/chkrootkit-installer-with-functions/","section":"posts","tags":["functions"],"title":"CHKROOTKIT install script (with functions)"},{"body":"This bash script does a sanity check for the DNS domains defined inside /var/named.\n[code language=\u0026quot;bash\u0026quot;] #!/bin/bash A=`ls -l /var/named/*.db | awk '{print $9}' | cut -f4 -d \u0026quot;/\u0026quot; | sed 's/.db$//'` #domain names\nfor i in $A; do named-checkzone $i /var/named/$i.db;done [/code]\n","link":"https://arvimal.github.io/posts/2008/02/dns-zone-file-sanity-check/","section":"posts","tags":null,"title":"DNS Zone file sanity check"},{"body":"","link":"https://arvimal.github.io/tags/functions/","section":"tags","tags":null,"title":"functions"},{"body":"This is a bash script which automates the installation of Nagios. There are more things to do such as setup of service monitoring, but that's for another time.\n[code language=\u0026quot;bash\u0026quot;] #!/bin/bash DOWNLOAD_LOCATION='/root/Downloads/' NAGIOS_URL='http://jaist.dl.sourceforge.net/sourceforge/nagios/nagios-2.9.tar.gz' APACHE_CONF='/etc/httpd/conf/httpd.conf' NAGIOS_PLUGIN='http://nchc.dl.sourceforge.net/sourceforge/nagiosplug/nagios-plugins-1.4.8.tar.gz' NAGIOSHOME='/usr/local/nagios' DATE=`date +%d-%b-%Y` FILE='/root/Nagios.txt'\n################################# # [1] Installing nagios # ################################# nagios_download () { clear\nif [ `id -u` -ne 0 ]; then echo -e \u0026quot;You are executing the script as $USER\\n\u0026quot; echo -e \u0026quot;You must be root to execute this script..\\n\u0026quot;; echo -e \u0026quot;Sorry...Exiting..\\n\u0026quot;;exit 111; else if [ ! -e /root/Nagios.txt ]; then touch /root/Nagios.txt; else mv /root/Nagios.txt /root/Nagios-$DATE.txt; touch /root/Nagios.txt; fi\necho -e \u0026quot; [@@@@@@@@@@@@@@@@@@@@@@@@@ NAGIOS INSTALL SCRIPT @@@@@@@@@@@@@@@@@@@@@@@@@]\\n\u0026quot;;sleep 1s echo -e \u0026quot; ...Welcome...\\n\u0026quot;|tee -a $FILE;sleep 1s echo \u0026quot;[Starting the Nagios Installation Process :-]\u0026quot;|tee -a $FILE; echo \u0026quot;---------------------------------------------\u0026quot;|tee -a $FILE;echo;sleep 1s fi\nif [ ! -e $DOWNLOAD_LOCATION ]; then echo \u0026quot;$DOWNLOAD_LOCATION does not exist, creating.......\u0026quot;|tee -a $FILE;sleep 1s mkdir -pv $DOWNLOAD_LOCATION;echo fi\necho \u0026quot;[Downloading the nagios tar-ball to $DOWNLOAD_LOCATION :-]\u0026quot;|tee -a $FILE; echo \u0026quot;--------------------------------------------------------\u0026quot;|tee -a $FILE;echo;sleep 1s\ncd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; wget --progress=dot $NAGIOS_URL;echo echo -e \u0026quot;Extracting the archive....\\n\u0026quot;|tee -a $FILE;sleep 1s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; tar -zxf nagios*gz \u0026amp;\u0026amp; mv nagios*gz Nagios-$DATE.tar.gz;echo }\nnagios_usercheck () { echo \u0026quot;[Checking the existence of user/group 'nagios' :-]\u0026quot;|tee -a $FILE; echo \u0026quot;--------------------------------------------------\u0026quot;|tee -a $FILE;\ngrep -q nagios /etc/group \u0026gt; /dev/null if [ $? = 0 ];then echo \u0026quot;Group 'nagios' exist\u0026quot;|tee -a $FILE; else echo \u0026quot;Adding group 'nagios'\u0026quot;|tee -a $FILE; /usr/sbin/groupadd nagios fi\ngrep -q nagios /etc/passwd \u0026gt; /dev/null if [ $? = 0 ];then echo \u0026quot;User 'nagios' exists\u0026quot;|tee -a $FILE; else echo \u0026quot;Adding user 'nagios'\u0026quot;|tee -a $FILE; /usr/sbin/useradd -d $NAGIOSHOME -g nagios -s /bin/false -m nagios fi;echo\necho \u0026quot;[Checking the existence of user/group 'nagcmd' :-]\u0026quot;|tee -a $FILE; echo \u0026quot;--------------------------------------------------\u0026quot;|tee -a $FILE;\ngrep -q nagcmd /etc/group; if [ $? = 0 ];then echo \u0026quot;Group 'nagcmd' exists\u0026quot;|tee -a $FILE; else echo \u0026quot;Adding group 'nagcmd'\u0026quot;|tee -a $FILE; /usr/sbin/groupadd nagcmd; fi\ngrep -q nagcmd /etc/passwd; if [ $? = 0 ];then echo \u0026quot;User 'nagcmd' exists\u0026quot;|tee -a $FILE; else echo \u0026quot;Adding user 'nagcmd'\u0026quot;|tee -a $FILE; /usr/sbin/useradd -g nagcmd -s /bin/false -m nagcmd; fi; echo }\nnagios_previouscheck () { echo \u0026quot;[Checking for previous installations :-]\u0026quot;|tee -a $FILE echo \u0026quot;----------------------------------------\u0026quot;|tee -a $FILE;sleep 1s\nif [ -d /usr/local/nagios ]; then echo \u0026quot;Installation directory '/usr/local/nagios/' already exist.\u0026quot;|tee -a $FILE echo \u0026quot;Moving '/usr/local/nagios/' to '/usr/local/Nagios-$DATE.back'\u0026quot;|tee -a $FILE mv -v /usr/local/nagios /usr/local/Nagios-$DATE.back;echo echo \u0026quot;Creating the Installation Directory for Nagios [/usr/local/nagios/]\u0026quot;|tee -a $FILE mkdir -pv /usr/local/nagios;echo else echo \u0026quot;Nagios installation not found at the default location of $NAGIOSHOME\u0026quot;; echo \u0026quot;Creating the Installation Directory for Nagios [/usr/local/nagios/]\u0026quot;|tee -a $FILE mkdir -pv /usr/local/nagios;echo fi }\nnagios_ownership () { echo \u0026quot;[Setting appropriate ownership on the installation directory]\u0026quot;|tee -a $FILE echo \u0026quot;-------------------------------------------------------------\u0026quot; chown -v nagios.nagios /usr/local/nagios;echo;sleep 1s\necho \u0026quot;[Checking the Web-Server user/group :-]\u0026quot;|tee -a $FILE echo \u0026quot;---------------------------------------\u0026quot;|tee -a $FILE;sleep 1s\necho \u0026quot;Web-Server User : `grep \u0026quot;^User\u0026quot; $APACHE_CONF|head -n1|awk '{print $2}'`\u0026quot;|tee -a $FILE echo \u0026quot;Web-Server Group : `grep \u0026quot;^Group\u0026quot; $APACHE_CONF|head -n1|awk '{print $2}'`\u0026quot;|tee -a $FILE;echo;sleep 1s\necho \u0026quot;[Adding the Web-Server/Nagios user to the 'nagcmd' group]\u0026quot;|tee -a $FILE; echo \u0026quot;---------------------------------------------------------\u0026quot; /usr/sbin/usermod -G nagcmd `grep \u0026quot;^User\u0026quot; $APACHE_CONF|head -n1|awk '{print $2}'` \u0026amp;\u0026amp; \\ echo \u0026quot;Added the user `grep \u0026quot;^User\u0026quot; $APACHE_CONF|head -n1|awk '{print $2}'` to the 'nagcmd' group.\u0026quot;|tee -a $FILE sleep 1s /usr/sbin/usermod -G nagcmd nagios \u0026amp;\u0026amp; echo -e \u0026quot;Added the user 'nagios' to the 'nagcmd' group.\\n\u0026quot;|tee -a $FILE;sleep 1s echo }\nnagios_configure () { echo \u0026quot;[Starting the Nagios 'configure' script :-]\u0026quot;|tee -a $FILE; echo \u0026quot;-------------------------------------------\u0026quot;|tee -a $FILE;sleep 4s\ncd $DOWNLOAD_LOCATION/nagios* \u0026amp;\u0026amp; ./configure --with-command-group=nagcmd \u0026amp;\u0026amp; make all \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; make install-config \u0026amp;\u0026amp; make install-init \u0026amp;\u0026amp; make install-commandmode echo }\n################################# # [2] Installing Nagios Plugins # #################################\nnagios_plugins () { sleep 4s echo -e \u0026quot; [@@@@@@@@@@@@@@@@@@@@@@@@@ NAGIOS PLUGIN SETUP @@@@@@@@@@@@@@@@@@@@@@@@@]\\n\u0026quot;|tee -a $FILE;sleep 2s\necho \u0026quot;[Downloading the 'nagios-plugins' tarball :-]\u0026quot;|tee -a $FILE; echo \u0026quot;---------------------------------------------\u0026quot;;sleep 3s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; wget --progress=dot $NAGIOS_PLUGIN;echo echo \u0026quot;[Extracting the plugins archive :-]\u0026quot;|tee -a $FILE; echo \u0026quot;-----------------------------------\u0026quot;;sleep 1s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; tar -zxf nagios-plugins*gz \u0026amp;\u0026amp; mv nagios-plugins*gz Nagios-plugins-$DATE.tar.gz;echo echo \u0026quot;[Configuring and compiling nagios-plugins :-]\u0026quot;|tee -a $FILE; echo \u0026quot;---------------------------------------------\u0026quot;|tee -a $FILE;sleep 1s cd $DOWNLOAD_LOCATION \u0026amp;\u0026amp; cd nagios-plugins* \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; echo -e \u0026quot;[Nagios Plugin Setup Finished.]\\n\u0026quot; } echo;echo;sleep 3s\n################################# # [3] Configuring Nagios # ################################# nagios_conf_files () { echo \u0026quot;[Creating the minimal configuration files :-]\u0026quot;; echo \u0026quot;---------------------------------------------\u0026quot;;sleep 2s cp -apv $NAGIOSHOME/etc/nagios.cfg-sample $NAGIOSHOME/etc/nagios.cfg cp -apv $NAGIOSHOME/etc/commands.cfg-sample $NAGIOSHOME/etc/commands.cfg cp -apv $NAGIOSHOME/etc/resource.cfg-sample $NAGIOSHOME/etc/resource.cfg cp -apv $NAGIOSHOME/etc/localhost.cfg-sample $NAGIOSHOME/etc/localhost.cfg cp -apv $NAGIOSHOME/etc/cgi.cfg-sample $NAGIOSHOME/etc/cgi.cfg;echo\necho \u0026quot;[Setting administrative rights for 'nagiosadmin']\u0026quot; echo \u0026quot;-------------------------------------------------\u0026quot;;sleep 2s;echo echo \u0026quot;\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/cgi.cfg echo -e \u0026quot;#Setting administrative rights for 'nagiosadmin'\\n\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/cgi.cfg\necho \u0026quot;authorized_for_system_information=nagiosadmin authorized_for_configuration_information=nagiosadmin authorized_for_system_commands=nagiosadmin authorized_for_all_services=nagiosadmin authorized_for_all_hosts=nagiosadmin authorized_for_all_service_commands=nagiosadmin authorized_for_all_host_commands=nagiosadmin\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/cgi.cfg\necho \u0026quot;[Creating additional configuration files :-]\u0026quot;; echo \u0026quot;--------------------------------------------\u0026quot;;sleep 2s touch $NAGIOSHOME/etc/hosts.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/hosts.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/hosts.cfg\u0026quot;;fi touch $NAGIOSHOME/etc/hostgroups.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/hostgroups.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/hostgroups.cfg\u0026quot;;fi touch $NAGIOSHOME/etc/contacts.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/contacts.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/contacts.cfg\u0026quot;;fi touch $NAGIOSHOME/etc/contactgroups.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/contactgroups.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/contactgroups.cfg\u0026quot;;fi touch $NAGIOSHOME/etc/services.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/services.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/services.cfg\u0026quot;;fi touch $NAGIOSHOME/etc/timeperiods.cfg; if [ $? -eq 0 ];then echo \u0026quot;Created $NAGIOSHOME/etc/timeperiods.cfg\u0026quot;;else echo \u0026quot;Failed creating $NAGIOSHOME/etc/timeperiods.cfg\u0026quot;;fi; echo\necho \u0026quot;[Changing the ownership of newly created files :-]\u0026quot;; echo \u0026quot;--------------------------------------------------\u0026quot;;sleep 2s chown -Rv nagios.nagios $NAGIOSHOME/etc/* echo\necho \u0026quot;\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/nagios.cfg echo \u0026quot;[Setting config: file paths in $NAGIOSHOME/etc/nagios.cfg :-]\u0026quot;; echo \u0026quot;------------------------------------------------------------------\u0026quot;;echo;sleep 2s echo -e \u0026quot;#Setting configuration file paths.\\n\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/nagios.cfg echo \u0026quot;cfg_file=/usr/local/nagios/etc/hosts.cfg cfg_file=/usr/local/nagios/etc/hostgroups.cfg cfg_file=/usr/local/nagios/etc/services.cfg cfg_file=/usr/local/nagios/etc/contacts.cfg cfg_file=/usr/local/nagios/etc/contactgroups.cfg cfg_file=/usr/local/nagios/etc/timeperiods.cfg\u0026quot; \u0026gt;\u0026gt; $NAGIOSHOME/etc/nagios.cfg\necho echo \u0026quot;[Running the Nagios Syntax Check :-]\u0026quot;; echo \u0026quot;------------------------------------\u0026quot;;sleep 1s $NAGIOSHOME/bin/nagios -v $NAGIOSHOME/etc/nagios.cfg;echo }\n################################# # [4] Setting Up Apache # #################################\nnagios_apache () { echo \u0026quot;[Setting up Apache Web-Interface :-]\u0026quot; echo \u0026quot;------------------------------------\u0026quot;\ngrep -q \u0026quot;### Nagios Script Alias ###\u0026quot; $APACHE_CONF;\nif [ $? -eq 0 ];then echo -e \u0026quot;ScriptAlias for nagios already exists in $APACHE_CONF\\n\u0026quot; /etc/init.d/httpd restart \u0026gt; /dev/null else\necho \u0026quot;\u0026quot; \u0026gt;\u0026gt; $APACHE_CONF echo -e \u0026quot;### Nagios Script Alias ###\\n\u0026quot; \u0026gt;\u0026gt; $APACHE_CONF;\necho -e \u0026quot;ScriptAlias /nagios/cgi-bin /usr/local/nagios/sbin \\n\nOptions ExecCGI AllowOverride None Order allow,deny Allow from all AuthName \\\u0026quot;Nagios Access\\\u0026quot; AuthType Basic AuthUserFile /usr/local/nagios/etc/htpasswd.users Require valid-user \\n\u0026quot; \u0026gt;\u0026gt; $APACHE_CONF\necho -e \u0026quot;Alias /nagios /usr/local/nagios/share \\n\nOptions None AllowOverride None Order allow,deny Allow from all AuthName \\\u0026quot;Nagios Access\\\u0026quot; AuthType Basic AuthUserFile /usr/local/nagios/etc/htpasswd.users Require valid-user \\n\u0026quot; \u0026gt;\u0026gt; $APACHE_CONF\necho \u0026quot;Added the needed Alias configurations in $APACHE_CONF\u0026quot;\necho -e \u0026quot;Restarting the Web-Server...please wait..\\n\u0026quot; /etc/init.d/httpd restart; fi }\nnagios_htpasswd () { echo \u0026quot;[Creating the login credentials for the nagios URL :-]\u0026quot; echo \u0026quot;------------------------------------------------------\u0026quot;; echo \u0026quot;Username : nagiosadmin\u0026quot; htpasswd -c $NAGIOSHOME/etc/htpasswd.users nagiosadmin;echo echo -e \u0026quot;Login to the Nagios Interface is now restricted to user 'nagiosadmin'.\\n\u0026quot; }\nnagios_download \u0026amp;\u0026amp; nagios_usercheck \u0026amp;\u0026amp; nagios_previouscheck \u0026amp;\u0026amp; nagios_ownership \u0026amp;\u0026amp; nagios_configure \u0026amp;\u0026amp; nagios_plugins \u0026amp;\u0026amp; nagios_conf_files \u0026amp;\u0026amp; nagios_apache \u0026amp;\u0026amp; nagios_htpasswd [/code]\n","link":"https://arvimal.github.io/posts/2008/02/nagios-installer-script/","section":"posts","tags":["nagios-installation"],"title":"Nagios Installation Script"},{"body":"","link":"https://arvimal.github.io/tags/nagios-installation/","section":"tags","tags":null,"title":"nagios-installation"},{"body":"The bash environment variable 'RANDOM' is a pseudo-random number generator built in bash, and it can generate random numbers in the range of 0 - 32767.\nUsing the command `echo $RANDOM`, we can generate a random number. Building a random number generator which emits a sequence of random numbers is pretty easy.\n[code language=\u0026quot;bash\u0026quot;] #!/bin/bash for i in `seq 1 10`: do echo $RANDOM; sleep 1s; done [/code]\nThe 'seq' or the 'sequential' can be used to generate a sequence of numbers.\n","link":"https://arvimal.github.io/posts/2008/02/bash-script-for-generating-random-number/","section":"posts","tags":["bash","random-number-generator"],"title":"A random number generator in Bash"},{"body":"","link":"https://arvimal.github.io/tags/bash/","section":"tags","tags":null,"title":"bash"},{"body":"Some time back, I had to implement a password encryption section in one of my bash programs. It seemed easy to use a C snippet rather than doing it in bash. This was something I got after searching a while.\n[code language=\u0026quot;C\u0026quot;]\n#include stdlib.h #include unistd.h #include stdio.h #include crack.h #define DICTIONARY /usr/lib/cracklib_dict\nint main(int argc, char *argv[]) {\nchar *password; char *problem;\nint status = 0; printf(\\nEnter an empty password or Ctrl-D to quit.\\n); while ((password = getpass(\\nPassword: )) != NULL *password ) { if ((problem = FascistCheck(password, DICTIONARY)) != NULL) { printf(Bad password: %s.\\n, problem); status = 1; } else { printf(Good password!\\n); } } exit(status); } [/code]\nCompile the code using the GNU C compiler.\n# gcc filename.c -lcrack -o cracktest'\n","link":"https://arvimal.github.io/posts/2008/02/password-encryptor-in-c/","section":"posts","tags":["password-encrypt"],"title":"Password Encryptor in C"},{"body":"","link":"https://arvimal.github.io/tags/password-encrypt/","section":"tags","tags":null,"title":"password-encrypt"},{"body":"","link":"https://arvimal.github.io/tags/random-number-generator/","section":"tags","tags":null,"title":"random-number-generator"},{"body":"","link":"https://arvimal.github.io/tags/runaway-process/","section":"tags","tags":null,"title":"runaway-process"},{"body":"","link":"https://arvimal.github.io/tags/zombie/","section":"tags","tags":null,"title":"zombie"},{"body":"Why can't I kill a process with the signal 9?\nA process can be sleeping in kernel code. Usually that's because of faulty hardware or a badly written driver- or maybe a little of both. A device that isn't set to the interrupt the driver thinks it is can cause this, for example- the driver is waiting for something its never going to get. The process doesn't ignore your signal- it just never gets it.\nA zombie process doesn't react to signals because it's not really a process at all- it's just what's left over after it died. What's supposed to happen is that its parent process was to issue a \u0026quot;wait()\u0026quot; to collect the information about its exit. If the parent doesn't (programming error or just bad programming), you get a zombie. The zombie will go away if its parent dies- it will be \u0026quot;adopted\u0026quot; by init which will do the wait()- so if you see one hanging about, check its parent; if it is init, it will be gone soon, if not the only recourse is to kill the parent..which you may or may not want to do.\nFinally, a process that is being traced (by a debugger, for example) won't react to the KILL either then you do a ps, processes that have a status of Z are called \u0026quot;zombies\u0026quot;. When people see a zombie process, the first thing they try to do is to kill the zombie, using kill or (horrors!) kill -9. This won't work, however: you can't kill a zombie, it's already dead.\nWhen a process has already terminated (\u0026quot;died\u0026quot;) by receiving a signal to do so, it can stick around for a bit to finish up a few last tasks. These include closing open files and shutting down any allocated resources (memory, swap space, that sort of thing). These \u0026quot;housekeeping\u0026quot; tasks are supposed to happen very quickly. Once they're completed, the final thing that a process has to do before dying is to report its exit status to its parent. This is generally where things go wrong.\nEach process is assigned a unique Process ID (PID). Each process also has an associated parent process ID (PPID), which identifies the process that spawned it (or PPID of 1, meaning that the process has been inherited bythe init process, if the parent has already terminated). While the parent is still running, it can remember the PID's of all the children it has spawned. These PID's can not be re-used by other (new) processes until the parent knows that the child process is done.\nWhen a child terminates and has completed its housekeeping tasks, it sends a one-byte status code to its parent. If this status code never gets sent, the PID is kept alive (in \u0026quot;zombie\u0026quot; status) in order to reserve its PID ... the parent is waiting for the status code, and until it gets it, it doesn't want any new processes to try and reuse that PID number for themselves.\nTo get rid of a zombie, you can try killing its parent, which will temporarily orphan the zombie. The init process will inherent the zombie, and this might allow the process to finish terminating since the init process is always in a wait() state (ready to receive exit status reports of children).\nGenerally, though, zombies clean themselves up. Whatever the process was waiting for eventually occurs and the process can report its exit status to its parent and all is well.\nIf a zombie is already owned by init, though, and it's still sticking around (like zombies are wont to do), then the process is almost certainly stuck in a device driver close routine, and will likely remain that way forever. You can reboot to clear out the zombies, but fixing the device driver is the only permanent solution. Killing the parent (init in this case) is highly unrecommended, since init is an extremely important process to keeping your system running..\n","link":"https://arvimal.github.io/posts/2008/01/zombie-processes/","section":"posts","tags":["runaway-process","zombie"],"title":"Zombie processes"},{"body":"Obsidian ","link":"https://arvimal.github.io/posts/2022/12/writing-my-blog-article-in-obsidian/","section":"posts","tags":["blog","arvimal.github.io"],"title":""},{"body":"","link":"https://arvimal.github.io/tags/__base__/","section":"tags","tags":null,"title":"__base__"},{"body":"","link":"https://arvimal.github.io/tags/__bases__/","section":"tags","tags":null,"title":"__bases__"},{"body":"","link":"https://arvimal.github.io/tags/arvimal.github.io/","section":"tags","tags":null,"title":"arvimal.github.io"},{"body":"","link":"https://arvimal.github.io/tags/blog/","section":"tags","tags":null,"title":"blog"},{"body":"_M_ethod Resolution Order or 'MRO' in short, denotes the way a programming language resolves a method or attribute. This post looks into how Method Resolution Order works, using Python.\nPython supports classes inheriting from other classes. The class being inherited is called the Parent/Super class, while the class that inherits is called the Child/Sub class.\nWhile inheriting from another class, the interpreter needs a way to resolve the methods that are being called via an instance. Hence a method resolution order is needed.\nExample 0:\n[code language=\u0026quot;python\u0026quot;]\nclass A(object): def my_func(self): print(\u0026quot;Doing this in class A\u0026quot;)\nclass B(A): def my_func(self): print(\u0026quot;Doing this in class B\u0026quot;)\nmy_instance = B() my_instance.my_func() [/code]\nStructure: We've two classes, class A and class B. Instantiate class B as my_instance. Call the my_func() method through the my_instance instance. Where is the method fetched from? From class B or class A?\nHow does the code work? This should be pretty obvious, the answer would be class B. But why is it being called from class B and not from class A?\nAnswer : The Method Resolution Order [MRO].\nTo understand this in depth, let's check another example:\nExample 1:\n[code language=\u0026quot;python\u0026quot;] class A(object): def my_func(self): print(\u0026quot;Doing this in Class A\u0026quot;)\nclass B(A): pass\nclass C(object): def my_func(self): print(\u0026quot;Doing this in Class C\u0026quot;)\nclass D(B, C): pass\nmy_instance = D() my_instance.my_func() [/code]\nStructure: Four classes, class A, B, C, and D. Class D inherits from both B and C Class B inherits from A. Class A and C doesn't inherit from any super classes, but from the object base class due to being new-style classes. Class A and class C both have a method/function named my_func(). Class D is instantiated through my_instance If we were to call the method my_func() through the my_instance() instance, which class would it be called from? Would it be from class A or class C?\nHow does the code work? This won't be as obvious as Example 0.\nThe instance my_instance() is created from class D. Since class Dinherits from both class B and C, the python interpreter searches for the method my_func() in both of these classes. The intrepreter finds that class B inherits from class A, and class C doesn't have any super classes other than the default object class. Class A and class C both has the method named my_func(), and hence has to be called from one of these. Python follows a depth-first lookup order and hence ends up calling the method from class A. Following the depth-first Method Resolution Order, the lookup would be in the order :\nClass D -\u0026gt; Class B -\u0026gt; Class C\nLet's check another example, which can be a bit more complex.\nExample 2:\n[code language=\u0026quot;python\u0026quot;] class A(object): def my_func(self): print(\u0026quot;Doing this in A\u0026quot;)\nclass B(A): pass\nclass C(A): def my_func(self): print(\u0026quot;doing this in C\u0026quot;)\nclass D(B, C): pass\nmy_instance = D() my_instance.my_func() [/code]\nStructure: Four classes, class A, B, C, and D Class D inherits from both B and C Class B inherits from class A. Class C inherits from class A. Class A inherits from the default base class object. This sort of inheritance is called the Diamond Inheritance or the Deadly Diamond of death and looks like the following:\nImage courtsey : Wikipedia\nHow does the code work? Following the depth-first Method Resolution Order, the lookup would be in the order :\nClass D -\u0026gt; Class B -\u0026gt; Class A -\u0026gt; Class C -\u0026gt; Class A\nIn order to avoid ambiguity while doing a lookup for a method where multiple classes are inherited and involved, the MRO lookup has changed slightly from Python 2.3 onwards.\nIt still goes for the depth-first order, but if the occurrence of a class happens multiple times in the MRO path, it removes the initial occurrence and keeps the latter.\nHence, the look up order in Example 2 becomes:\nClass D -\u0026gt; Class B -\u0026gt; Class C -\u0026gt; Class A.\nNOTE: Python provides a method for a class to lookup the Method Resolution Order. Let's recheck Example 2 using that.\n[code language=\u0026quot;python\u0026quot;] class A(object): def my_func(self): print(\u0026quot;Calling this from A\u0026quot;)\nclass B(A): pass\nclass C(A): def my_func(self): print(\u0026quot;\\nCalling this from C\u0026quot;)\nclass D(B, C): pass\nmy_instance = D() my_instance.my_func()\nprint(\u0026quot;\\nPrint the Method Resolution Order\u0026quot;) print(D.mro()) print(D.__bases__) [/code] This should print:\n[code language=\u0026quot;python\u0026quot;] # python /tmp/Example-2.py\nCalling this from C\nPrint the Method Resolution Order class '__main__.D', class '__main__.B', class '__main__.C', class '__main__.A', type 'object'\n(, ) [/code]\nTakeaway Python follows a depth-first order for resolving methods and attributes. In case of multiple inheritances where the methods happen to occur more than once, python omits the first occurrence of a class in the Method Resolution Order. The \u0026lt;class\u0026gt;.mro()methods helps to understand the Medthod Resolution Order. The `__bases__` and `__base__` magic methods help to understand the Base/Parent classes of a Sub/Child class. References https://en.wikipedia.org/wiki/Multiple_inheritance ","link":"https://arvimal.github.io/posts/2016/05/mro-object-oriented-programming/","section":"posts","tags":["inheritance","method-resolution-order","mro","python","__bases__","__base__"],"title":"Method Resolution Order - Object Oriented Programming"},{"body":"","link":"https://arvimal.github.io/tags/method-resolution-order/","section":"tags","tags":null,"title":"method-resolution-order"},{"body":"","link":"https://arvimal.github.io/tags/mro/","section":"tags","tags":null,"title":"mro"},{"body":"","link":"https://arvimal.github.io/series/","section":"series","tags":null,"title":"Series"}]